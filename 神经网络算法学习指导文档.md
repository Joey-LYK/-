# ç¥ç»ç½‘ç»œç®—æ³•å­¦ä¹ æŒ‡å¯¼æ–‡æ¡£ï¼ˆåµŒå…¥å¼å·¥ç¨‹å¸ˆç‰ˆï¼‰

## ğŸ“‹ ç›®å½•
- [å­¦ä¹ ç›®æ ‡](#å­¦ä¹ ç›®æ ‡)
- [å­¦ä¹ è®¡åˆ’æ€»è§ˆ](#å­¦ä¹ è®¡åˆ’æ€»è§ˆ)
- [é˜¶æ®µä¸€ï¼šç¥ç»ç½‘ç»œåŸºç¡€ç†è®º](#é˜¶æ®µä¸€ç¥ç»ç½‘ç»œåŸºç¡€ç†è®º)
- [é˜¶æ®µäºŒï¼šç®—æ³•åŸç†ä¸æ¨å¯¼](#é˜¶æ®µäºŒç®—æ³•åŸç†ä¸æ¨å¯¼)
- [é˜¶æ®µä¸‰ï¼šä»é›¶å®ç°ç®—æ³•](#é˜¶æ®µä¸‰ä»é›¶å®ç°ç®—æ³•)
- [é˜¶æ®µå››ï¼šæ¨¡å‹è®­ç»ƒä¸ä¼˜åŒ–](#é˜¶æ®µå››æ¨¡å‹è®­ç»ƒä¸ä¼˜åŒ–)
- [é˜¶æ®µäº”ï¼šæ¨¡å‹éƒ¨ç½²ä¸éªŒè¯](#é˜¶æ®µäº”æ¨¡å‹éƒ¨ç½²ä¸éªŒè¯)
- [ç†è®ºçŸ¥è¯†ä½“ç³»](#ç†è®ºçŸ¥è¯†ä½“ç³»)
- [å‚è€ƒèµ„æº](#å‚è€ƒèµ„æº)
- [å­¦ä¹ å»ºè®®](#å­¦ä¹ å»ºè®®)

---

## ğŸ¯ å­¦ä¹ ç›®æ ‡

ä½œä¸ºåµŒå…¥å¼è½¯ä»¶å·¥ç¨‹å¸ˆï¼ŒæŒæ¡ç¥ç»ç½‘ç»œç®—æ³•çš„æ ¸å¿ƒåŸç†ï¼Œèƒ½å¤Ÿï¼š
- **ç†è§£ç¥ç»ç½‘ç»œå·¥ä½œåŸç†**ï¼šä»æ•°å­¦åŸç†åˆ°ç®—æ³•å®ç°
- **æŒæ¡è®­ç»ƒè¿‡ç¨‹**ï¼šæ•°æ®å‡†å¤‡ã€æ¨¡å‹è®­ç»ƒã€å‚æ•°ä¼˜åŒ–
- **å­¦ä¼šæ¨¡å‹ç®¡ç†**ï¼šä¿å­˜ã€åŠ è½½ã€è°ƒç”¨è®­ç»ƒå¥½çš„æ¨¡å‹
- **éªŒè¯å­¦ä¹ æˆæœ**ï¼šåœ¨ESP32ä¸Šéƒ¨ç½²éªŒè¯

**æ ¸å¿ƒæŠ€èƒ½ï¼š**
- ç¥ç»ç½‘ç»œç®—æ³•åŸç†
- åå‘ä¼ æ’­ç®—æ³•æ¨å¯¼
- æ¨¡å‹è®­ç»ƒå’Œä¼˜åŒ–æŠ€æœ¯
- æ¨¡å‹åºåˆ—åŒ–å’Œéƒ¨ç½²

---

## ğŸ“… å­¦ä¹ è®¡åˆ’æ€»è§ˆ

| é˜¶æ®µ | æ—¶é—´ | ä¸»è¦å†…å®¹ | ç†è®ºé‡ç‚¹ | å®è·µç›®æ ‡ |
|------|------|----------|----------|----------|
| é˜¶æ®µä¸€ | 3å‘¨ | ç¥ç»ç½‘ç»œåŸºç¡€ç†è®º | ç¥ç»å…ƒæ¨¡å‹ã€å‰å‘ä¼ æ’­ | ç†è§£ç®—æ³•åŸç† |
| é˜¶æ®µäºŒ | 3å‘¨ | ç®—æ³•åŸç†ä¸æ¨å¯¼ | åå‘ä¼ æ’­ã€æ¢¯åº¦ä¸‹é™ | æŒæ¡æ•°å­¦æ¨å¯¼ |
| é˜¶æ®µä¸‰ | 3å‘¨ | ä»é›¶å®ç°ç®—æ³• | ä»£ç å®ç°ã€æ•°æ®ç»“æ„ | ç‹¬ç«‹å®ç°ç®—æ³• |
| é˜¶æ®µå›› | 2å‘¨ | æ¨¡å‹è®­ç»ƒä¸ä¼˜åŒ– | è®­ç»ƒç­–ç•¥ã€ä¼˜åŒ–æŠ€æœ¯ | è®­ç»ƒå®é™…æ¨¡å‹ |
| é˜¶æ®µäº” | 1å‘¨ | æ¨¡å‹éƒ¨ç½²ä¸éªŒè¯ | æ¨¡å‹åºåˆ—åŒ–ã€éƒ¨ç½² | ESP32éªŒè¯ |

**æ€»å­¦ä¹ æ—¶é—´ï¼š12å‘¨**

---

## ğŸ“– é˜¶æ®µä¸€ï¼šç¥ç»ç½‘ç»œåŸºç¡€ç†è®ºï¼ˆ3å‘¨ï¼‰

### ç¬¬1å‘¨ï¼šç¥ç»å…ƒä¸æ¿€æ´»å‡½æ•°

#### å­¦ä¹ å†…å®¹
- **ç¥ç»å…ƒæ•°å­¦æ¨¡å‹**
- **æ¿€æ´»å‡½æ•°åŸç†**
- **å‰å‘ä¼ æ’­æœºåˆ¶**

#### ç†è®ºçŸ¥è¯†

**1. ç¥ç»å…ƒæ¨¡å‹**

**ç”Ÿç‰©å­¦åŸºç¡€**ï¼š
ç¥ç»å…ƒæ˜¯å¤§è„‘çš„åŸºæœ¬è®¡ç®—å•å…ƒï¼Œç”±æ ‘çªï¼ˆè¾“å…¥ï¼‰ã€ç»†èƒä½“ï¼ˆå¤„ç†ï¼‰ã€è½´çªï¼ˆè¾“å‡ºï¼‰ç»„æˆã€‚äººå·¥ç¥ç»å…ƒæ¨¡æ‹Ÿäº†è¿™ä¸€ç»“æ„ã€‚

**æ•°å­¦æ¨¡å‹**ï¼š
```
è¾“å…¥: x = [xâ‚, xâ‚‚, ..., xâ‚™]
æƒé‡: w = [wâ‚, wâ‚‚, ..., wâ‚™]
åç½®: b
è¾“å‡º: y = f(Î£wáµ¢xáµ¢ + b)
```

**æ•°å­¦è§£é‡Š**ï¼š
- **çº¿æ€§ç»„åˆ**ï¼šz = Î£wáµ¢xáµ¢ + b è¡¨ç¤ºè¾“å…¥çš„åŠ æƒå’Œ
- **æ¿€æ´»å‡½æ•°**ï¼šf(z) å¼•å…¥éçº¿æ€§å˜æ¢
- **åç½®é¡¹**ï¼šb å…è®¸ç¥ç»å…ƒåœ¨æ²¡æœ‰è¾“å…¥æ—¶ä¹Ÿèƒ½äº§ç”Ÿè¾“å‡º

**ç‰©ç†æ„ä¹‰**ï¼š
- æƒé‡wáµ¢ï¼šè¡¨ç¤ºç¬¬iä¸ªè¾“å…¥çš„é‡è¦ç¨‹åº¦
- åç½®bï¼šè°ƒæ•´ç¥ç»å…ƒçš„æ¿€æ´»é˜ˆå€¼
- æ¿€æ´»å‡½æ•°fï¼šæ¨¡æ‹Ÿç”Ÿç‰©ç¥ç»å…ƒçš„"å…¨æˆ–æ— "ç‰¹æ€§

**2. æ¿€æ´»å‡½æ•°ä½œç”¨**

**éçº¿æ€§å˜æ¢**ï¼š
- å¦‚æœæ²¡æœ‰æ¿€æ´»å‡½æ•°ï¼Œå¤šå±‚ç½‘ç»œç­‰ä»·äºå•å±‚ç½‘ç»œ
- éçº¿æ€§æ¿€æ´»å‡½æ•°ä½¿ç½‘ç»œèƒ½å¤Ÿå­¦ä¹ å¤æ‚çš„éçº¿æ€§æ˜ å°„
- ä¾‹å¦‚ï¼šf(x) = xÂ² å¯ä»¥å­¦ä¹ å¹³æ–¹å…³ç³»

**æ¢¯åº¦æ§åˆ¶**ï¼š
- æ¿€æ´»å‡½æ•°çš„å¯¼æ•°å½±å“æ¢¯åº¦ä¼ æ’­
- Sigmoidå¯¼æ•°åœ¨|x|>4æ—¶æ¥è¿‘0ï¼Œå¯¼è‡´æ¢¯åº¦æ¶ˆå¤±
- ReLUåœ¨x>0æ—¶å¯¼æ•°ä¸º1ï¼Œç¼“è§£æ¢¯åº¦æ¶ˆå¤±é—®é¢˜

**è¾“å‡ºèŒƒå›´**ï¼š
- Sigmoidï¼šè¾“å‡ºèŒƒå›´[0,1]ï¼Œé€‚åˆæ¦‚ç‡è¾“å‡º
- Tanhï¼šè¾“å‡ºèŒƒå›´[-1,1]ï¼Œé›¶ä¸­å¿ƒåŒ–
- ReLUï¼šè¾“å‡ºèŒƒå›´[0,âˆ)ï¼Œç¨€ç–æ¿€æ´»

**3. å¸¸è§æ¿€æ´»å‡½æ•°è¯¦è§£**

**Sigmoidå‡½æ•°**ï¼š
```
f(x) = 1 / (1 + e^(-x))
f'(x) = f(x) * (1 - f(x))
```

**æ•°å­¦ç‰¹æ€§**ï¼š
- **è¾“å‡ºèŒƒå›´**ï¼š[0,1]ï¼Œé€‚åˆæ¦‚ç‡è¾“å‡º
- **å•è°ƒæ€§**ï¼šä¸¥æ ¼å•è°ƒé€’å¢
- **å¯¹ç§°æ€§**ï¼šå…³äºç‚¹(0, 0.5)å¯¹ç§°
- **é¥±å’Œæ€§**ï¼šå½“|x|>4æ—¶ï¼Œå¯¼æ•°æ¥è¿‘0

**ä¼˜ç¼ºç‚¹åˆ†æ**ï¼š
- **ä¼˜ç‚¹**ï¼šè¾“å‡ºèŒƒå›´å›ºå®šï¼Œå¯¼æ•°è®¡ç®—ç®€å•
- **ç¼ºç‚¹**ï¼šæ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œè¾“å‡ºéé›¶ä¸­å¿ƒåŒ–

**åº”ç”¨åœºæ™¯**ï¼š
- äºŒåˆ†ç±»é—®é¢˜çš„è¾“å‡ºå±‚
- éœ€è¦æ¦‚ç‡è¾“å‡ºçš„åœºæ™¯
- æ—©æœŸç¥ç»ç½‘ç»œçš„æ ‡å‡†é€‰æ‹©

**ReLUå‡½æ•°**ï¼š
```
f(x) = max(0, x)
f'(x) = {1 if x > 0, 0 if x â‰¤ 0}
```

**æ•°å­¦ç‰¹æ€§**ï¼š
- **è¾“å‡ºèŒƒå›´**ï¼š[0,âˆ)
- **ç¨€ç–æ€§**ï¼šè´Ÿè¾“å…¥äº§ç”Ÿé›¶è¾“å‡º
- **çº¿æ€§æ€§**ï¼šæ­£è¾“å…¥ä¿æŒçº¿æ€§å…³ç³»
- **è®¡ç®—æ•ˆç‡**ï¼šè®¡ç®—ç®€å•ï¼Œå¯¼æ•°ç®€å•

**ä¼˜ç¼ºç‚¹åˆ†æ**ï¼š
- **ä¼˜ç‚¹**ï¼šç¼“è§£æ¢¯åº¦æ¶ˆå¤±ï¼Œè®¡ç®—é«˜æ•ˆï¼Œç¨€ç–æ¿€æ´»
- **ç¼ºç‚¹**ï¼šæ­»äº¡ReLUé—®é¢˜ï¼Œè¾“å‡ºéé›¶ä¸­å¿ƒåŒ–

**åº”ç”¨åœºæ™¯**ï¼š
- æ·±åº¦ç¥ç»ç½‘ç»œçš„éšè—å±‚
- éœ€è¦ç¨€ç–æ¿€æ´»çš„åœºæ™¯
- è®¡ç®—èµ„æºå—é™çš„ç¯å¢ƒ

**Tanhå‡½æ•°**ï¼š
```
f(x) = (e^x - e^(-x)) / (e^x + e^(-x))
f'(x) = 1 - f(x)Â²
```

**æ•°å­¦ç‰¹æ€§**ï¼š
- **è¾“å‡ºèŒƒå›´**ï¼š[-1,1]ï¼Œé›¶ä¸­å¿ƒåŒ–
- **å•è°ƒæ€§**ï¼šä¸¥æ ¼å•è°ƒé€’å¢
- **å¯¹ç§°æ€§**ï¼šå…³äºåŸç‚¹å¯¹ç§°
- **é¥±å’Œæ€§**ï¼šå½“|x|>2æ—¶ï¼Œå¯¼æ•°æ¥è¿‘0

**ä¼˜ç¼ºç‚¹åˆ†æ**ï¼š
- **ä¼˜ç‚¹**ï¼šé›¶ä¸­å¿ƒåŒ–ï¼Œæ¢¯åº¦åœ¨åŸç‚¹é™„è¿‘è¾ƒå¤§
- **ç¼ºç‚¹**ï¼šä»ç„¶å­˜åœ¨æ¢¯åº¦æ¶ˆå¤±é—®é¢˜

**åº”ç”¨åœºæ™¯**ï¼š
- éœ€è¦é›¶ä¸­å¿ƒåŒ–è¾“å‡ºçš„åœºæ™¯
- RNNå’ŒLSTMçš„éšè—å±‚
- éœ€è¦å¯¹ç§°è¾“å‡ºçš„æƒ…å†µ

**Leaky ReLUå‡½æ•°**ï¼š
```
f(x) = {x if x > 0, Î±x if x â‰¤ 0}
f'(x) = {1 if x > 0, Î± if x â‰¤ 0}
```

**æ•°å­¦ç‰¹æ€§**ï¼š
- **è¾“å‡ºèŒƒå›´**ï¼š(-âˆ,âˆ)
- **ç¨€ç–æ€§**ï¼šéƒ¨åˆ†ä¿ç•™è´Ÿè¾“å…¥ä¿¡æ¯
- **å‚æ•°åŒ–**ï¼šÎ±æ§åˆ¶è´Ÿè¾“å…¥çš„æ–œç‡

**ä¼˜ç¼ºç‚¹åˆ†æ**ï¼š
- **ä¼˜ç‚¹**ï¼šç¼“è§£æ­»äº¡ReLUé—®é¢˜ï¼Œä¿æŒç¨€ç–æ€§
- **ç¼ºç‚¹**ï¼šéœ€è¦è°ƒæ•´è¶…å‚æ•°Î±

**4. æ¿€æ´»å‡½æ•°é€‰æ‹©ç­–ç•¥**

**è¾“å‡ºå±‚é€‰æ‹©**ï¼š
- **å›å½’é—®é¢˜**ï¼šçº¿æ€§æ¿€æ´»å‡½æ•°æˆ–ReLU
- **äºŒåˆ†ç±»é—®é¢˜**ï¼šSigmoidæ¿€æ´»å‡½æ•°
- **å¤šåˆ†ç±»é—®é¢˜**ï¼šSoftmaxæ¿€æ´»å‡½æ•°
- **æ¦‚ç‡è¾“å‡º**ï¼šSigmoidæˆ–Tanh

**éšè—å±‚é€‰æ‹©**ï¼š
- **æµ…å±‚ç½‘ç»œ**ï¼šSigmoidæˆ–Tanh
- **æ·±å±‚ç½‘ç»œ**ï¼šReLUæˆ–Leaky ReLU
- **è®¡ç®—å—é™**ï¼šReLUï¼ˆè®¡ç®—ç®€å•ï¼‰
- **æ¢¯åº¦ç¨³å®š**ï¼šLeaky ReLUæˆ–ELU

**é€‰æ‹©è€ƒè™‘å› ç´ **ï¼š
- **ç½‘ç»œæ·±åº¦**ï¼šæ·±å±‚ç½‘ç»œé¿å…é¥±å’Œæ¿€æ´»å‡½æ•°
- **è®¡ç®—å¤æ‚åº¦**ï¼šå®æ—¶åº”ç”¨é€‰æ‹©è®¡ç®—ç®€å•çš„å‡½æ•°
- **æ¢¯åº¦ä¼ æ’­**ï¼šé€‰æ‹©å¯¼æ•°ä¸ä¸ºé›¶çš„å‡½æ•°
- **è¾“å‡ºè¦æ±‚**ï¼šæ ¹æ®ä»»åŠ¡éœ€æ±‚é€‰æ‹©è¾“å‡ºèŒƒå›´

**5. æ•°å€¼ç¨³å®šæ€§è€ƒè™‘**

**æ¢¯åº¦æ¶ˆå¤±é—®é¢˜**ï¼š
- **åŸå› **ï¼šæ¿€æ´»å‡½æ•°å¯¼æ•°åœ¨é¥±å’ŒåŒºåŸŸæ¥è¿‘0
- **å½±å“**ï¼šæ·±å±‚ç½‘ç»œæ¢¯åº¦æ— æ³•æœ‰æ•ˆä¼ æ’­
- **è§£å†³æ–¹æ¡ˆ**ï¼šä½¿ç”¨ReLUã€Leaky ReLUæˆ–ELU

**æ¢¯åº¦çˆ†ç‚¸é—®é¢˜**ï¼š
- **åŸå› **ï¼šæƒé‡åˆå§‹åŒ–ä¸å½“æˆ–å­¦ä¹ ç‡è¿‡å¤§
- **å½±å“**ï¼šæ¢¯åº¦å€¼è¿‡å¤§å¯¼è‡´å‚æ•°æ›´æ–°ä¸ç¨³å®š
- **è§£å†³æ–¹æ¡ˆ**ï¼šæ¢¯åº¦è£å‰ªã€æƒé‡æ­£åˆ™åŒ–

**æ•°å€¼æº¢å‡º**ï¼š
- **åŸå› **ï¼šæŒ‡æ•°å‡½æ•°åœ¨è¾“å…¥è¿‡å¤§æ—¶æº¢å‡º
- **å½±å“**ï¼šè®¡ç®—é”™è¯¯æˆ–ç¨‹åºå´©æºƒ
- **è§£å†³æ–¹æ¡ˆ**ï¼šä½¿ç”¨æ•°å€¼ç¨³å®šçš„å®ç°

**6. æ¿€æ´»å‡½æ•°çš„ç”Ÿç‰©å­¦æ„ä¹‰**

**ç”Ÿç‰©ç¥ç»å…ƒç‰¹æ€§**ï¼š
- **é˜ˆå€¼ç‰¹æ€§**ï¼šåªæœ‰è¾“å…¥è¶…è¿‡é˜ˆå€¼æ‰æ¿€æ´»
- **å…¨æˆ–æ— ç‰¹æ€§**ï¼šæ¿€æ´»åäº§ç”Ÿå›ºå®šå¹…åº¦çš„è„‰å†²
- **ä¸åº”æœŸ**ï¼šæ¿€æ´»åçŸ­æš‚æ—¶é—´å†…ä¸èƒ½å†æ¬¡æ¿€æ´»

**äººå·¥ç¥ç»å…ƒæ¨¡æ‹Ÿ**ï¼š
- **é˜ˆå€¼**ï¼šé€šè¿‡åç½®é¡¹bå®ç°
- **éçº¿æ€§**ï¼šé€šè¿‡æ¿€æ´»å‡½æ•°å®ç°
- **å¯å¡‘æ€§**ï¼šé€šè¿‡æƒé‡æ›´æ–°å®ç°å­¦ä¹ 

**ç”Ÿç‰©å­¦æ„ä¹‰çš„å…·ä½“ä¾‹å­**ï¼š

**Sigmoidå‡½æ•°çš„ç”Ÿç‰©å­¦æ„ä¹‰**ï¼š
- **ç”Ÿç‰©ç±»æ¯”**ï¼šæ¨¡æ‹Ÿç¥ç»å…ƒçš„æ¿€æ´»æ¦‚ç‡
  - å½“è¾“å…¥å¾ˆå°æ—¶ï¼šç¥ç»å…ƒå‡ ä¹ä¸æ¿€æ´»ï¼ˆè¾“å‡ºæ¥è¿‘0ï¼‰
  - å½“è¾“å…¥å¾ˆå¤§æ—¶ï¼šç¥ç»å…ƒå‡ ä¹æ€»æ˜¯æ¿€æ´»ï¼ˆè¾“å‡ºæ¥è¿‘1ï¼‰
  - å½“è¾“å…¥é€‚ä¸­æ—¶ï¼šç¥ç»å…ƒæœ‰50%æ¦‚ç‡æ¿€æ´»ï¼ˆè¾“å‡ºæ¥è¿‘0.5ï¼‰
- **å®é™…åº”ç”¨**ï¼šåœ¨åŒ»å­¦è¯Šæ–­ä¸­ï¼ŒSigmoidè¾“å‡ºå¯ä»¥è¡¨ç¤ºæ‚£ç—…çš„æ¦‚ç‡
  - è¾“å…¥ï¼šå„ç§ç—‡çŠ¶æŒ‡æ ‡
  - è¾“å‡ºï¼šæ‚£ç—…çš„æ¦‚ç‡ï¼ˆ0-1ä¹‹é—´ï¼‰
  - å†³ç­–ï¼šå¦‚æœæ¦‚ç‡>0.5ï¼Œè¯Šæ–­ä¸ºæ‚£ç—…

**ReLUå‡½æ•°çš„ç”Ÿç‰©å­¦æ„ä¹‰**ï¼š
- **ç”Ÿç‰©ç±»æ¯”**ï¼šæ¨¡æ‹Ÿç¥ç»å…ƒçš„ç¨€ç–æ¿€æ´»
  - è´Ÿè¾“å…¥ï¼šç¥ç»å…ƒä¸æ¿€æ´»ï¼ˆè¾“å‡ºä¸º0ï¼‰ï¼ŒèŠ‚çœèƒ½é‡
  - æ­£è¾“å…¥ï¼šç¥ç»å…ƒçº¿æ€§æ¿€æ´»ï¼Œä¿¡æ¯ä¼ é€’
- **å®é™…åº”ç”¨**ï¼šåœ¨å›¾åƒè¯†åˆ«ä¸­ï¼ŒReLUå¸®åŠ©ç½‘ç»œå­¦ä¹ ç¨€ç–ç‰¹å¾
  - è¾“å…¥ï¼šå›¾åƒåƒç´ å€¼
  - è¾“å‡ºï¼šç‰¹å¾æ£€æµ‹ç»“æœ
  - æ•ˆæœï¼šåªæœ‰é‡è¦çš„ç‰¹å¾è¢«æ¿€æ´»ï¼Œå…¶ä»–ä¿æŒé™é»˜

**Tanhå‡½æ•°çš„ç”Ÿç‰©å­¦æ„ä¹‰**ï¼š
- **ç”Ÿç‰©ç±»æ¯”**ï¼šæ¨¡æ‹Ÿç¥ç»å…ƒçš„å…´å¥‹å’ŒæŠ‘åˆ¶
  - è´Ÿè¾“å…¥ï¼šç¥ç»å…ƒæŠ‘åˆ¶ï¼ˆè¾“å‡ºä¸ºè´Ÿï¼‰
  - æ­£è¾“å…¥ï¼šç¥ç»å…ƒå…´å¥‹ï¼ˆè¾“å‡ºä¸ºæ­£ï¼‰
  - é›¶è¾“å…¥ï¼šç¥ç»å…ƒå¹³è¡¡çŠ¶æ€ï¼ˆè¾“å‡ºä¸º0ï¼‰
- **å®é™…åº”ç”¨**ï¼šåœ¨æƒ…æ„Ÿåˆ†æä¸­ï¼ŒTanhå¯ä»¥è¡¨ç¤ºæƒ…æ„Ÿå¼ºåº¦
  - è¾“å…¥ï¼šæ–‡æœ¬ç‰¹å¾
  - è¾“å‡ºï¼šæƒ…æ„Ÿå¼ºåº¦ï¼ˆ-1åˆ°+1ï¼‰
  - è§£é‡Šï¼šè´Ÿå€¼è¡¨ç¤ºæ¶ˆææƒ…æ„Ÿï¼Œæ­£å€¼è¡¨ç¤ºç§¯ææƒ…æ„Ÿ

**æ¿€æ´»å‡½æ•°åœ¨ä¿¡æ¯å¤„ç†ä¸­çš„æ„ä¹‰**ï¼š
- **ä¿¡æ¯å‹ç¼©**ï¼šå°†æ— é™èŒƒå›´çš„è¾“å…¥å‹ç¼©åˆ°æœ‰é™èŒƒå›´
  - ä¾‹å­ï¼šSigmoidå°†(-âˆ,+âˆ)å‹ç¼©åˆ°(0,1)
  - æ„ä¹‰ï¼šé˜²æ­¢ä¿¡æ¯çˆ†ç‚¸ï¼Œä¿æŒæ•°å€¼ç¨³å®š
- **ç‰¹å¾é€‰æ‹©**ï¼šæŸäº›æ¿€æ´»å‡½æ•°å…·æœ‰ç‰¹å¾é€‰æ‹©èƒ½åŠ›
  - ä¾‹å­ï¼šReLUçš„ç¨€ç–æ¿€æ´»
  - æ„ä¹‰ï¼šè‡ªåŠ¨é€‰æ‹©é‡è¦ç‰¹å¾ï¼Œå¿½ç•¥æ— å…³ç‰¹å¾
- **éçº¿æ€§å˜æ¢**ï¼šå¼•å…¥éçº¿æ€§ï¼Œå¢å¼ºç½‘ç»œè¡¨è¾¾èƒ½åŠ›
  - ä¾‹å­ï¼šå¤šå±‚ç½‘ç»œéœ€è¦éçº¿æ€§æ¿€æ´»å‡½æ•°
  - æ„ä¹‰ï¼šå¦åˆ™å¤šå±‚ç½‘ç»œç­‰ä»·äºå•å±‚ç½‘ç»œ

**7. æ¿€æ´»å‡½æ•°çš„å‘å±•å†ç¨‹**

**æ—©æœŸå‘å±•**ï¼š
- **1943å¹´**ï¼šMcCulloch-Pittsç¥ç»å…ƒæ¨¡å‹
- **1957å¹´**ï¼šæ„ŸçŸ¥æœºä½¿ç”¨é˜¶è·ƒå‡½æ•°
- **1960å¹´ä»£**ï¼šSigmoidå‡½æ•°å¼•å…¥

**ç°ä»£å‘å±•**ï¼š
- **2000å¹´ä»£**ï¼šReLUå‡½æ•°å¼•å…¥
- **2010å¹´ä»£**ï¼šLeaky ReLUã€ELUç­‰å˜ä½“
- **2015å¹´å**ï¼šSwishã€GELUç­‰è‡ªé€‚åº”å‡½æ•°

**æœªæ¥è¶‹åŠ¿**ï¼š
- **è‡ªé€‚åº”æ¿€æ´»**ï¼šæ ¹æ®æ•°æ®è‡ªåŠ¨è°ƒæ•´
- **å¯å­¦ä¹ æ¿€æ´»**ï¼šå°†æ¿€æ´»å‡½æ•°å‚æ•°åŒ–
- **ç¨€ç–æ¿€æ´»**ï¼šæé«˜è®¡ç®—æ•ˆç‡
```
Ïƒ(x) = 1/(1 + eâ»Ë£)
å¯¼æ•°ï¼šÏƒ'(x) = Ïƒ(x)(1-Ïƒ(x))
```
- **ä¼˜ç‚¹**ï¼šè¾“å‡ºèŒƒå›´[0,1]ï¼Œé€‚åˆæ¦‚ç‡è¾“å‡º
- **ç¼ºç‚¹**ï¼šæ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œéé›¶ä¸­å¿ƒåŒ–
- **åº”ç”¨**ï¼šäºŒåˆ†ç±»é—®é¢˜çš„è¾“å‡ºå±‚

**ReLUå‡½æ•°**ï¼š
```
f(x) = max(0, x)
å¯¼æ•°ï¼šf'(x) = 1 if x > 0, else 0
```
- **ä¼˜ç‚¹**ï¼šè®¡ç®—ç®€å•ï¼Œç¼“è§£æ¢¯åº¦æ¶ˆå¤±ï¼Œäº§ç”Ÿç¨€ç–æ¿€æ´»
- **ç¼ºç‚¹**ï¼šæ­»äº¡ReLUé—®é¢˜ï¼ˆç¥ç»å…ƒæ°¸ä¹…å¤±æ´»ï¼‰
- **åº”ç”¨**ï¼šéšè—å±‚çš„é¦–é€‰æ¿€æ´»å‡½æ•°

**Tanhå‡½æ•°**ï¼š
```
f(x) = (eË£ - eâ»Ë£)/(eË£ + eâ»Ë£)
å¯¼æ•°ï¼šf'(x) = 1 - f(x)Â²
```
- **ä¼˜ç‚¹**ï¼šé›¶ä¸­å¿ƒåŒ–ï¼Œè¾“å‡ºèŒƒå›´[-1,1]
- **ç¼ºç‚¹**ï¼šä»ç„¶å­˜åœ¨æ¢¯åº¦æ¶ˆå¤±é—®é¢˜
- **åº”ç”¨**ï¼šRNNçš„éšè—å±‚

**Leaky ReLU**ï¼š
```
f(x) = max(Î±x, x), Î± = 0.01
```
- **ä¼˜ç‚¹**ï¼šè§£å†³æ­»äº¡ReLUé—®é¢˜
- **ç¼ºç‚¹**ï¼šéœ€è¦è°ƒå‚Î±
- **åº”ç”¨**ï¼šå¯¹ReLUçš„æ”¹è¿›ç‰ˆæœ¬

**4. æ¿€æ´»å‡½æ•°é€‰æ‹©ç­–ç•¥**

**è¾“å‡ºå±‚é€‰æ‹©**ï¼š
- **å›å½’é—®é¢˜**ï¼šçº¿æ€§æ¿€æ´»å‡½æ•°æˆ–æ— æ¿€æ´»å‡½æ•°
- **äºŒåˆ†ç±»**ï¼šSigmoidæ¿€æ´»å‡½æ•°
- **å¤šåˆ†ç±»**ï¼šSoftmaxæ¿€æ´»å‡½æ•°

**éšè—å±‚é€‰æ‹©**ï¼š
- **é¦–é€‰**ï¼šReLUåŠå…¶å˜ä½“ï¼ˆLeaky ReLUã€PReLUï¼‰
- **RNN**ï¼šTanhæˆ–Sigmoid
- **ç‰¹æ®Šä»»åŠ¡**ï¼šæ ¹æ®å…·ä½“éœ€æ±‚é€‰æ‹©

**5. æ¿€æ´»å‡½æ•°çš„æ•°å€¼ç¨³å®šæ€§**

**Sigmoidæ•°å€¼ç¨³å®šæ€§**ï¼š
```c
// é¿å…æº¢å‡ºçš„å®ç°
float sigmoid_stable(float x) {
    if (x >= 0) {
        float z = expf(-x);
        return 1.0f / (1.0f + z);
    } else {
        float z = expf(x);
        return z / (1.0f + z);
    }
}
```

**ReLUæ•°å€¼ç¨³å®šæ€§**ï¼š
```c
// é¿å…NaNçš„å®ç°
float relu_stable(float x) {
    return x > 0 ? x : 0.0f;
}
```

**6. æ¿€æ´»å‡½æ•°çš„Cè¯­è¨€å®ç°**

**å®Œæ•´æ¿€æ´»å‡½æ•°åº“**ï¼š
```c
// æ¿€æ´»å‡½æ•°ç±»å‹å®šä¹‰
typedef float (*activation_func_t)(float);

// Sigmoidæ¿€æ´»å‡½æ•°
float sigmoid(float x) {
    return 1.0f / (1.0f + expf(-x));
}

// Sigmoidå¯¼æ•°
float sigmoid_derivative(float x) {
    float s = sigmoid(x);
    return s * (1.0f - s);
}

// ReLUæ¿€æ´»å‡½æ•°
float relu(float x) {
    return x > 0 ? x : 0.0f;
}

// ReLUå¯¼æ•°
float relu_derivative(float x) {
    return x > 0 ? 1.0f : 0.0f;
}

// Tanhæ¿€æ´»å‡½æ•°
float tanh_custom(float x) {
    return tanhf(x);
}

// Tanhå¯¼æ•°
float tanh_derivative(float x) {
    float t = tanhf(x);
    return 1.0f - t * t;
}

// Leaky ReLUæ¿€æ´»å‡½æ•°
float leaky_relu(float x) {
    return x > 0 ? x : 0.01f * x;
}

// Leaky ReLUå¯¼æ•°
float leaky_relu_derivative(float x) {
    return x > 0 ? 1.0f : 0.01f;
}

// æ¿€æ´»å‡½æ•°æŸ¥æ‰¾è¡¨
activation_func_t activation_functions[] = {
    sigmoid,
    relu,
    tanh_custom,
    leaky_relu
};

activation_func_t activation_derivatives[] = {
    sigmoid_derivative,
    relu_derivative,
    tanh_derivative,
    leaky_relu_derivative
};
```

**7. ç¥ç»å…ƒå‰å‘ä¼ æ’­å®ç°**

**å•ç¥ç»å…ƒç»“æ„**ï¼š
```c
typedef struct {
    float* weights;
    float bias;
    int input_size;
    activation_func_t activation;
} Neuron;

// ç¥ç»å…ƒåˆå§‹åŒ–
Neuron* neuron_create(int input_size, activation_func_t activation) {
    Neuron* neuron = malloc(sizeof(Neuron));
    neuron->input_size = input_size;
    neuron->activation = activation;
    neuron->weights = malloc(input_size * sizeof(float));
    neuron->bias = 0.0f;
    
    // éšæœºåˆå§‹åŒ–æƒé‡
    for (int i = 0; i < input_size; i++) {
        neuron->weights[i] = ((float)rand() / RAND_MAX - 0.5f) * 2.0f;
    }
    
    return neuron;
}

// ç¥ç»å…ƒå‰å‘ä¼ æ’­
float neuron_forward(Neuron* neuron, float* inputs) {
    float sum = neuron->bias;
    
    for (int i = 0; i < neuron->input_size; i++) {
        sum += neuron->weights[i] * inputs[i];
    }
    
    return neuron->activation(sum);
}

// ç¥ç»å…ƒé‡Šæ”¾
void neuron_free(Neuron* neuron) {
    free(neuron->weights);
    free(neuron);
}
```

**8. ç¥ç»å…ƒæ¢¯åº¦è®¡ç®—**

**æ¢¯åº¦è®¡ç®—ç»“æ„**ï¼š
```c
typedef struct {
    float* weight_gradients;
    float bias_gradient;
    float input_gradients;
} NeuronGradients;

// è®¡ç®—ç¥ç»å…ƒæ¢¯åº¦
NeuronGradients* neuron_compute_gradients(Neuron* neuron, float* inputs, float output_gradient) {
    NeuronGradients* gradients = malloc(sizeof(NeuronGradients));
    gradients->weight_gradients = malloc(neuron->input_size * sizeof(float));
    gradients->bias_gradient = output_gradient;
    
    // è®¡ç®—çº¿æ€§ç»„åˆ
    float linear_sum = neuron->bias;
    for (int i = 0; i < neuron->input_size; i++) {
        linear_sum += neuron->weights[i] * inputs[i];
    }
    
    // è®¡ç®—æ¿€æ´»å‡½æ•°å¯¼æ•°
    float activation_derivative = 0.0f;
    if (neuron->activation == sigmoid) {
        activation_derivative = sigmoid_derivative(linear_sum);
    } else if (neuron->activation == relu) {
        activation_derivative = relu_derivative(linear_sum);
    }
    
    // è®¡ç®—æƒé‡æ¢¯åº¦
    for (int i = 0; i < neuron->input_size; i++) {
        gradients->weight_gradients[i] = output_gradient * activation_derivative * inputs[i];
    }
    
    // è®¡ç®—è¾“å…¥æ¢¯åº¦ï¼ˆç”¨äºåå‘ä¼ æ’­ï¼‰
    gradients->input_gradients = output_gradient * activation_derivative;
    
    return gradients;
}
```
```
Ïƒ(x) = 1/(1 + eâ»Ë£)
å¯¼æ•°ï¼šÏƒ'(x) = Ïƒ(x)(1-Ïƒ(x))
```
- **ä¼˜ç‚¹**ï¼šè¾“å‡ºèŒƒå›´[0,1]ï¼Œé€‚åˆæ¦‚ç‡è¾“å‡º
- **ç¼ºç‚¹**ï¼šæ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œéé›¶ä¸­å¿ƒåŒ–
- **åº”ç”¨**ï¼šäºŒåˆ†ç±»é—®é¢˜çš„è¾“å‡ºå±‚

**ReLUå‡½æ•°**ï¼š
```
f(x) = max(0, x)
å¯¼æ•°ï¼šf'(x) = 1 if x > 0, else 0
```
- **ä¼˜ç‚¹**ï¼šè®¡ç®—ç®€å•ï¼Œç¼“è§£æ¢¯åº¦æ¶ˆå¤±ï¼Œäº§ç”Ÿç¨€ç–æ¿€æ´»
- **ç¼ºç‚¹**ï¼šæ­»äº¡ReLUé—®é¢˜ï¼ˆç¥ç»å…ƒæ°¸ä¹…å¤±æ´»ï¼‰
- **åº”ç”¨**ï¼šéšè—å±‚çš„é¦–é€‰æ¿€æ´»å‡½æ•°

**Tanhå‡½æ•°**ï¼š
```
f(x) = (eË£ - eâ»Ë£)/(eË£ + eâ»Ë£)
å¯¼æ•°ï¼šf'(x) = 1 - f(x)Â²
```
- **ä¼˜ç‚¹**ï¼šé›¶ä¸­å¿ƒåŒ–ï¼Œè¾“å‡ºèŒƒå›´[-1,1]
- **ç¼ºç‚¹**ï¼šä»ç„¶å­˜åœ¨æ¢¯åº¦æ¶ˆå¤±é—®é¢˜
- **åº”ç”¨**ï¼šRNNçš„éšè—å±‚

**Leaky ReLU**ï¼š
```
f(x) = max(Î±x, x), Î± = 0.01
```
- **ä¼˜ç‚¹**ï¼šè§£å†³æ­»äº¡ReLUé—®é¢˜
- **ç¼ºç‚¹**ï¼šéœ€è¦è°ƒå‚Î±
- **åº”ç”¨**ï¼šå¯¹ReLUçš„æ”¹è¿›ç‰ˆæœ¬

**4. æ¿€æ´»å‡½æ•°é€‰æ‹©ç­–ç•¥**

**è¾“å‡ºå±‚é€‰æ‹©**ï¼š
- **å›å½’é—®é¢˜**ï¼šçº¿æ€§æ¿€æ´»å‡½æ•°æˆ–æ— æ¿€æ´»å‡½æ•°
- **äºŒåˆ†ç±»**ï¼šSigmoidæ¿€æ´»å‡½æ•°
- **å¤šåˆ†ç±»**ï¼šSoftmaxæ¿€æ´»å‡½æ•°

**éšè—å±‚é€‰æ‹©**ï¼š
- **é¦–é€‰**ï¼šReLUåŠå…¶å˜ä½“ï¼ˆLeaky ReLUã€PReLUï¼‰
- **RNN**ï¼šTanhæˆ–Sigmoid
- **ç‰¹æ®Šä»»åŠ¡**ï¼šæ ¹æ®å…·ä½“éœ€æ±‚é€‰æ‹©

**5. æ¿€æ´»å‡½æ•°çš„æ•°å€¼ç¨³å®šæ€§**

**Sigmoidæ•°å€¼ç¨³å®šæ€§**ï¼š
```c
// é¿å…æº¢å‡ºçš„å®ç°
float sigmoid_stable(float x) {
    if (x >= 0) {
        float z = expf(-x);
        return 1.0f / (1.0f + z);
    } else {
        float z = expf(x);
        return z / (1.0f + z);
    }
}
```

**ReLUæ•°å€¼ç¨³å®šæ€§**ï¼š
```c
// é¿å…NaNçš„å®ç°
float relu_stable(float x) {
    return x > 0 ? x : 0.0f;
}
```


#### å®è·µç»ƒä¹ 

**ç»ƒä¹ 1ï¼šæ¿€æ´»å‡½æ•°æµ‹è¯•**
```c
// æ¿€æ´»å‡½æ•°æµ‹è¯•ç¨‹åº
void test_activation_functions() {
    printf("=== æ¿€æ´»å‡½æ•°æµ‹è¯• ===\n");
    
    float test_values[] = {-2.0f, -1.0f, 0.0f, 1.0f, 2.0f};
    int num_tests = sizeof(test_values) / sizeof(test_values[0]);
    
    printf("è¾“å…¥å€¼\tSigmoid\tReLU\tTanh\tLeakyReLU\n");
    for (int i = 0; i < num_tests; i++) {
        float x = test_values[i];
        printf("%.1f\t%.3f\t%.3f\t%.3f\t%.3f\n", 
               x, sigmoid(x), relu(x), tanh_custom(x), leaky_relu(x));
    }
    printf("\n");
}

// æ¿€æ´»å‡½æ•°å¯¼æ•°æµ‹è¯•
void test_activation_derivatives() {
    printf("=== æ¿€æ´»å‡½æ•°å¯¼æ•°æµ‹è¯• ===\n");
    
    float test_values[] = {-1.0f, 0.0f, 1.0f};
    int num_tests = sizeof(test_values) / sizeof(test_values[0]);
    
    printf("è¾“å…¥å€¼\tSigmoid'\tReLU'\tTanh'\tLeakyReLU'\n");
    for (int i = 0; i < num_tests; i++) {
        float x = test_values[i];
        printf("%.1f\t%.3f\t%.3f\t%.3f\t%.3f\n", 
               x, sigmoid_derivative(x), relu_derivative(x), 
               tanh_derivative(x), leaky_relu_derivative(x));
    }
    printf("\n");
}
```

**ç»ƒä¹ 2ï¼šç¥ç»å…ƒå®ç°ä¸æµ‹è¯•**
```c
// ç¥ç»å…ƒæµ‹è¯•ç¨‹åº
void test_neuron() {
    printf("=== ç¥ç»å…ƒæµ‹è¯• ===\n");
    
    // åˆ›å»ºç¥ç»å…ƒ
    Neuron* neuron = neuron_create(3, sigmoid);
    
    // è®¾ç½®æƒé‡å’Œåç½®
    neuron->weights[0] = 0.5f;
    neuron->weights[1] = -0.3f;
    neuron->weights[2] = 0.8f;
    neuron->bias = 0.1f;
    
    // æµ‹è¯•è¾“å…¥
    float inputs[] = {1.0f, 0.5f, -0.2f};
    
    // å‰å‘ä¼ æ’­
    float output = neuron_forward(neuron, inputs);
    
    printf("è¾“å…¥: [%.1f, %.1f, %.1f]\n", inputs[0], inputs[1], inputs[2]);
    printf("æƒé‡: [%.1f, %.1f, %.1f]\n", neuron->weights[0], neuron->weights[1], neuron->weights[2]);
    printf("åç½®: %.1f\n", neuron->bias);
    printf("è¾“å‡º: %.3f\n", output);
    
    // è®¡ç®—æ¢¯åº¦
    NeuronGradients* gradients = neuron_compute_gradients(neuron, inputs, 1.0f);
    
    printf("æƒé‡æ¢¯åº¦: [%.3f, %.3f, %.3f]\n", 
           gradients->weight_gradients[0], 
           gradients->weight_gradients[1], 
           gradients->weight_gradients[2]);
    printf("åç½®æ¢¯åº¦: %.3f\n", gradients->bias_gradient);
    printf("è¾“å…¥æ¢¯åº¦: %.3f\n", gradients->input_gradients);
    
    // æ¸…ç†
    free(gradients->weight_gradients);
    free(gradients);
    neuron_free(neuron);
}
```

**ç»ƒä¹ 3ï¼šæ•°å€¼ç¨³å®šæ€§æµ‹è¯•**
```c
// æ•°å€¼ç¨³å®šæ€§æµ‹è¯•
void test_numerical_stability() {
    printf("=== æ•°å€¼ç¨³å®šæ€§æµ‹è¯• ===\n");
    
    // æµ‹è¯•å¤§æ•°å€¼
    float large_values[] = {100.0f, 500.0f, 1000.0f};
    int num_tests = sizeof(large_values) / sizeof(large_values[0]);
    
    printf("å¤§æ•°å€¼æµ‹è¯•:\n");
    for (int i = 0; i < num_tests; i++) {
        float x = large_values[i];
        printf("x=%.0f: sigmoid=%.6f, sigmoid_stable=%.6f\n", 
               x, sigmoid(x), sigmoid_stable(x));
    }
    
    // æµ‹è¯•å°æ•°å€¼
    float small_values[] = {-100.0f, -500.0f, -1000.0f};
    printf("\nå°æ•°å€¼æµ‹è¯•:\n");
    for (int i = 0; i < num_tests; i++) {
        float x = small_values[i];
        printf("x=%.0f: sigmoid=%.6f, sigmoid_stable=%.6f\n", 
               x, sigmoid(x), sigmoid_stable(x));
    }
}
```

**ç»ƒä¹ 4ï¼šæ€§èƒ½æµ‹è¯•**
```c
// æ€§èƒ½æµ‹è¯•
void performance_test() {
    printf("=== æ€§èƒ½æµ‹è¯• ===\n");
    
    const int num_iterations = 1000000;
    float test_input = 0.5f;
    
    // æµ‹è¯•Sigmoidæ€§èƒ½
    clock_t start = clock();
    for (int i = 0; i < num_iterations; i++) {
        float result = sigmoid(test_input);
    }
    clock_t end = clock();
    double sigmoid_time = ((double)(end - start)) / CLOCKS_PER_SEC;
    
    // æµ‹è¯•ReLUæ€§èƒ½
    start = clock();
    for (int i = 0; i < num_iterations; i++) {
        float result = relu(test_input);
    }
    end = clock();
    double relu_time = ((double)(end - start)) / CLOCKS_PER_SEC;
    
    printf("Sigmoid: %.6f ç§’ (%d æ¬¡è°ƒç”¨)\n", sigmoid_time, num_iterations);
    printf("ReLU: %.6f ç§’ (%d æ¬¡è°ƒç”¨)\n", relu_time, num_iterations);
    printf("ReLUæ¯”Sigmoidå¿« %.2f å€\n", sigmoid_time / relu_time);
}
```

**ç»ƒä¹ 5ï¼šå®Œæ•´æµ‹è¯•ç¨‹åº**
```c
// ä¸»æµ‹è¯•ç¨‹åº
int main() {
    printf("ç¥ç»ç½‘ç»œåŸºç¡€æµ‹è¯•ç¨‹åº\n");
    printf("==================\n\n");
    
    // è®¾ç½®éšæœºç§å­
    srand(time(NULL));
    
    // è¿è¡Œæ‰€æœ‰æµ‹è¯•
    test_activation_functions();
    test_activation_derivatives();
    test_neuron();
    test_numerical_stability();
    performance_test();
    
    printf("æ‰€æœ‰æµ‹è¯•å®Œæˆï¼\n");
    return 0;
}
```

**ç»ƒä¹ 6ï¼šåµŒå…¥å¼ä¼˜åŒ–ç‰ˆæœ¬**
```c
// åµŒå…¥å¼ä¼˜åŒ–ç‰ˆæœ¬ - ä½¿ç”¨æŸ¥æ‰¾è¡¨
#define LUT_SIZE 1000
#define LUT_MIN -5.0f
#define LUT_MAX 5.0f
#define LUT_STEP ((LUT_MAX - LUT_MIN) / (LUT_SIZE - 1))

static float sigmoid_lut[LUT_SIZE];

// åˆå§‹åŒ–æŸ¥æ‰¾è¡¨
void init_sigmoid_lut() {
    for (int i = 0; i < LUT_SIZE; i++) {
        float x = LUT_MIN + i * LUT_STEP;
        sigmoid_lut[i] = 1.0f / (1.0f + expf(-x));
    }
}

// ä½¿ç”¨æŸ¥æ‰¾è¡¨çš„Sigmoid
float sigmoid_lut_fast(float x) {
    if (x <= LUT_MIN) return 0.0f;
    if (x >= LUT_MAX) return 1.0f;
    
    int index = (int)((x - LUT_MIN) / LUT_STEP);
    if (index >= LUT_SIZE) index = LUT_SIZE - 1;
    
    return sigmoid_lut[index];
}

// æ€§èƒ½å¯¹æ¯”æµ‹è¯•
void lut_performance_test() {
    printf("=== æŸ¥æ‰¾è¡¨æ€§èƒ½æµ‹è¯• ===\n");
    
    init_sigmoid_lut();
    
    const int num_iterations = 1000000;
    float test_inputs[] = {-2.0f, 0.0f, 2.0f};
    
    for (int t = 0; t < 3; t++) {
        float x = test_inputs[t];
        
        // æµ‹è¯•æ ‡å‡†Sigmoid
        clock_t start = clock();
        for (int i = 0; i < num_iterations; i++) {
            float result = sigmoid(x);
        }
        clock_t end = clock();
        double std_time = ((double)(end - start)) / CLOCKS_PER_SEC;
        
        // æµ‹è¯•æŸ¥æ‰¾è¡¨Sigmoid
        start = clock();
        for (int i = 0; i < num_iterations; i++) {
            float result = sigmoid_lut_fast(x);
        }
        end = clock();
        double lut_time = ((double)(end - start)) / CLOCKS_PER_SEC;
        
        printf("x=%.1f: æ ‡å‡†=%.6fç§’, æŸ¥æ‰¾è¡¨=%.6fç§’, åŠ é€Ÿæ¯”=%.2f\n", 
               x, std_time, lut_time, std_time / lut_time);
    }
}
```

#### æ¯æ—¥ä»»åŠ¡
- **Day 1-2**ï¼šç†è§£ç¥ç»å…ƒæ•°å­¦æ¨¡å‹
- **Day 3-4**ï¼šå­¦ä¹ æ¿€æ´»å‡½æ•°åŸç†
- **Day 5-7**ï¼šå®ç°åŸºç¡€æ¿€æ´»å‡½æ•°

### ç¬¬2å‘¨ï¼šå¤šå±‚ç¥ç»ç½‘ç»œ

#### å­¦ä¹ å†…å®¹
- **ç½‘ç»œæ¶æ„è®¾è®¡**
- **å‰å‘ä¼ æ’­ç®—æ³•**
- **æƒé‡åˆå§‹åŒ–ç­–ç•¥**

#### ç†è®ºçŸ¥è¯†

**1. ç½‘ç»œæ¶æ„è®¾è®¡**

**åŸºæœ¬æ¦‚å¿µ**ï¼š
å¤šå±‚ç¥ç»ç½‘ç»œï¼ˆMLPï¼‰ç”±è¾“å…¥å±‚ã€éšè—å±‚å’Œè¾“å‡ºå±‚ç»„æˆã€‚æ¯ä¸€å±‚åŒ…å«å¤šä¸ªç¥ç»å…ƒï¼Œå±‚ä¸å±‚ä¹‹é—´å…¨è¿æ¥ã€‚

**ç½‘ç»œç»“æ„**ï¼š
```
è¾“å…¥å±‚ â†’ éšè—å±‚1 â†’ éšè—å±‚2 â†’ ... â†’ è¾“å‡ºå±‚
   â†“        â†“         â†“              â†“
   x      hâ‚(x)     hâ‚‚(x)          y(x)
```

**æ•°å­¦è¡¨ç¤º**ï¼š
- **è¾“å…¥å±‚**ï¼šaâ½â°â¾ = xï¼Œç»´åº¦ä¸ºnâ½â°â¾
- **éšè—å±‚l**ï¼šaâ½Ë¡â¾ = Ïƒ(Wâ½Ë¡â¾aâ½Ë¡â»Â¹â¾ + bâ½Ë¡â¾)ï¼Œç»´åº¦ä¸ºnâ½Ë¡â¾
- **è¾“å‡ºå±‚**ï¼šaâ½á´¸â¾ = Ïƒ(Wâ½á´¸â¾aâ½á´¸â»Â¹â¾ + bâ½á´¸â¾)ï¼Œç»´åº¦ä¸ºnâ½á´¸â¾

**ç½‘ç»œæ·±åº¦ä¸å®½åº¦**ï¼š
- **æ·±åº¦**ï¼šéšè—å±‚æ•°é‡ï¼Œå½±å“ç½‘ç»œè¡¨è¾¾èƒ½åŠ›
- **å®½åº¦**ï¼šæ¯å±‚ç¥ç»å…ƒæ•°é‡ï¼Œå½±å“ç½‘ç»œå®¹é‡
- **æ·±åº¦vså®½åº¦**ï¼šæ·±åº¦ç½‘ç»œé€šå¸¸æ¯”å®½ç½‘ç»œæ›´æœ‰æ•ˆ

**2. å‰å‘ä¼ æ’­ç®—æ³•è¯¦è§£**

**ç®—æ³•æ­¥éª¤**ï¼š
```
å¯¹äºæ¯ä¸€å±‚ l = 1, 2, ..., L:
1. çº¿æ€§å˜æ¢: zâ½Ë¡â¾ = Wâ½Ë¡â¾aâ½Ë¡â»Â¹â¾ + bâ½Ë¡â¾
2. éçº¿æ€§æ¿€æ´»: aâ½Ë¡â¾ = Ïƒ(zâ½Ë¡â¾)
```

**çŸ©é˜µå½¢å¼**ï¼š
```
zâ½Ë¡â¾ = Wâ½Ë¡â¾aâ½Ë¡â»Â¹â¾ + bâ½Ë¡â¾
aâ½Ë¡â¾ = Ïƒ(zâ½Ë¡â¾)
å…¶ä¸­ï¼š
- Wâ½Ë¡â¾ âˆˆ â„^(nâ½Ë¡â¾ Ã— nâ½Ë¡â»Â¹â¾)
- bâ½Ë¡â¾ âˆˆ â„^(nâ½Ë¡â¾)
- aâ½Ë¡â¾ âˆˆ â„^(nâ½Ë¡â¾)
```

**è®¡ç®—å¤æ‚åº¦**ï¼š
- **æ—¶é—´å¤æ‚åº¦**ï¼šO(Î£nâ½Ë¡â¾nâ½Ë¡â»Â¹â¾)
- **ç©ºé—´å¤æ‚åº¦**ï¼šO(Î£nâ½Ë¡â¾)

**æ•°å€¼ç¨³å®šæ€§è€ƒè™‘**ï¼š
- **æ¢¯åº¦çˆ†ç‚¸**ï¼šæƒé‡è¿‡å¤§å¯¼è‡´æ¢¯åº¦æŒ‡æ•°å¢é•¿
- **æ¢¯åº¦æ¶ˆå¤±**ï¼šæƒé‡è¿‡å°å¯¼è‡´æ¢¯åº¦æŒ‡æ•°è¡°å‡
- **æ•°å€¼æº¢å‡º**ï¼šæ¿€æ´»å‡½æ•°è¾“å…¥è¿‡å¤§å¯¼è‡´æº¢å‡º

**3. æƒé‡åˆå§‹åŒ–ç­–ç•¥è¯¦è§£**

**ä¸ºä»€ä¹ˆéœ€è¦å¥½çš„åˆå§‹åŒ–**ï¼š
- **å¯¹ç§°æ€§ç ´å**ï¼šé¿å…æ‰€æœ‰ç¥ç»å…ƒå­¦ä¹ ç›¸åŒç‰¹å¾
- **æ¢¯åº¦ä¼ æ’­**ï¼šç¡®ä¿æ¢¯åº¦èƒ½å¤Ÿæœ‰æ•ˆä¼ æ’­
- **æ”¶æ•›é€Ÿåº¦**ï¼šå½±å“è®­ç»ƒæ”¶æ•›é€Ÿåº¦

**Xavieråˆå§‹åŒ–ï¼ˆGlorotåˆå§‹åŒ–ï¼‰**ï¼š
```
W ~ N(0, 2/(n_in + n_out))
æˆ–
W ~ U(-âˆš(6/(n_in + n_out)), âˆš(6/(n_in + n_out)))
```
- **åŸç†**ï¼šä¿æŒæ¯å±‚è¾“å…¥å’Œè¾“å‡ºçš„æ–¹å·®ç›¸ç­‰
- **é€‚ç”¨**ï¼šSigmoidã€Tanhæ¿€æ´»å‡½æ•°
- **æ¨å¯¼**ï¼šVar(aâ½Ë¡â¾) = Var(aâ½Ë¡â»Â¹â¾) â‡’ Var(W) = 2/(n_in + n_out)

**Heåˆå§‹åŒ–**ï¼š
```
W ~ N(0, 2/n_in)
æˆ–
W ~ U(-âˆš(6/n_in), âˆš(6/n_in))
```
- **åŸç†**ï¼šä¸“é—¨ä¸ºReLUæ¿€æ´»å‡½æ•°è®¾è®¡
- **é€‚ç”¨**ï¼šReLUåŠå…¶å˜ä½“
- **æ¨å¯¼**ï¼šè€ƒè™‘ReLUçš„ç¨€ç–æ¿€æ´»ç‰¹æ€§

**éšæœºåˆå§‹åŒ–**ï¼š
```
W ~ N(0, 0.01)  # å°æ–¹å·®åˆå§‹åŒ–
```
- **ä¼˜ç‚¹**ï¼šç®€å•ç›´æ¥
- **ç¼ºç‚¹**ï¼šå¯èƒ½å¯¼è‡´æ¢¯åº¦æ¶ˆå¤±æˆ–çˆ†ç‚¸
- **é€‚ç”¨**ï¼šç®€å•ç½‘ç»œæˆ–è°ƒè¯•é˜¶æ®µ

**4. ç½‘ç»œå®¹é‡ä¸è¡¨è¾¾èƒ½åŠ›**

**ä¸‡èƒ½è¿‘ä¼¼å®šç†**ï¼š
å…·æœ‰å•ä¸ªéšè—å±‚çš„å‰é¦ˆç¥ç»ç½‘ç»œå¯ä»¥ä»¥ä»»æ„ç²¾åº¦è¿‘ä¼¼ä»»ä½•è¿ç»­å‡½æ•°ã€‚

**æ•°å­¦è¡¨è¿°**ï¼š
å¯¹äºä»»æ„è¿ç»­å‡½æ•° f: [0,1]â¿ â†’ â„ å’Œä»»æ„ Îµ > 0ï¼Œå­˜åœ¨ä¸€ä¸ªå…·æœ‰å•ä¸ªéšè—å±‚çš„å‰é¦ˆç¥ç»ç½‘ç»œï¼Œä½¿å¾—ï¼š
|f(x) - NN(x)| < Îµï¼Œå¯¹äºæ‰€æœ‰ x âˆˆ [0,1]â¿

**å®é™…æ„ä¹‰**ï¼š
- **ç†è®ºä¿è¯**ï¼šç†è®ºä¸Šç¥ç»ç½‘ç»œå¯ä»¥å­¦ä¹ ä»»ä½•è¿ç»­å‡½æ•°
- **å®è·µé™åˆ¶**ï¼šå®é™…ä¸­éœ€è¦åˆé€‚çš„ç½‘ç»œç»“æ„å’Œè®­ç»ƒæ–¹æ³•
- **æ·±åº¦ä¼˜åŠ¿**ï¼šæ·±å±‚ç½‘ç»œé€šå¸¸æ¯”æµ…å±‚ç½‘ç»œæ›´æœ‰æ•ˆ

**5. ç½‘ç»œæ·±åº¦ä¸å®½åº¦çš„æƒè¡¡**

**æ·±åº¦ç½‘ç»œçš„ä¼˜åŠ¿**ï¼š
- **å±‚æ¬¡ç‰¹å¾**ï¼šæ·±å±‚ç½‘ç»œå¯ä»¥å­¦ä¹ å±‚æ¬¡åŒ–çš„ç‰¹å¾è¡¨ç¤º
- **å‚æ•°æ•ˆç‡**ï¼šç”¨æ›´å°‘çš„å‚æ•°å®ç°ç›¸åŒçš„è¡¨è¾¾èƒ½åŠ›
- **æŠ½è±¡èƒ½åŠ›**ï¼šé«˜å±‚ç‰¹å¾æ›´åŠ æŠ½è±¡å’Œè¯­ä¹‰åŒ–

**å®½åº¦ç½‘ç»œçš„ä¼˜åŠ¿**ï¼š
- **å¹¶è¡Œè®¡ç®—**ï¼šå®½å±‚å¯ä»¥æ›´å¥½åœ°åˆ©ç”¨å¹¶è¡Œè®¡ç®—
- **æ¢¯åº¦ä¼ æ’­**ï¼šæ¢¯åº¦åœ¨å®½ç½‘ç»œä¸­ä¼ æ’­æ›´ç¨³å®š
- **è®­ç»ƒç®€å•**ï¼šå®½ç½‘ç»œé€šå¸¸æ›´å®¹æ˜“è®­ç»ƒ

**æ·±åº¦vså®½åº¦çš„é€‰æ‹©**ï¼š
- **æ•°æ®å¤æ‚åº¦**ï¼šå¤æ‚æ•°æ®éœ€è¦æ›´æ·±çš„ç½‘ç»œ
- **è®¡ç®—èµ„æº**ï¼šæ·±åº¦ç½‘ç»œéœ€è¦æ›´å¤šè®¡ç®—èµ„æº
- **è®­ç»ƒç¨³å®šæ€§**ï¼šæ·±å±‚ç½‘ç»œè®­ç»ƒæ›´å›°éš¾
- **ä»»åŠ¡ç‰¹æ€§**ï¼šä¸åŒä»»åŠ¡å¯¹æ·±åº¦å’Œå®½åº¦æœ‰ä¸åŒè¦æ±‚

**6. å‰å‘ä¼ æ’­çš„æ•°å€¼ç¨³å®šæ€§**

**æ¢¯åº¦çˆ†ç‚¸é—®é¢˜**ï¼š
- **åŸå› **ï¼šæƒé‡åˆå§‹åŒ–è¿‡å¤§æˆ–å­¦ä¹ ç‡è¿‡é«˜
- **è¡¨ç°**ï¼šæ¢¯åº¦å€¼è¿‡å¤§ï¼Œå‚æ•°æ›´æ–°ä¸ç¨³å®š
- **æ£€æµ‹**ï¼šè§‚å¯Ÿæ¢¯åº¦èŒƒæ•°æ˜¯å¦è¿‡å¤§
- **è§£å†³**ï¼šæ¢¯åº¦è£å‰ªã€æƒé‡æ­£åˆ™åŒ–ã€è°ƒæ•´å­¦ä¹ ç‡

**æ¢¯åº¦æ¶ˆå¤±é—®é¢˜**ï¼š
- **åŸå› **ï¼šæ¿€æ´»å‡½æ•°é¥±å’Œæˆ–æƒé‡è¿‡å°
- **è¡¨ç°**ï¼šæ·±å±‚ç½‘ç»œæ¢¯åº¦æ¥è¿‘é›¶
- **æ£€æµ‹**ï¼šè§‚å¯Ÿä¸åŒå±‚çš„æ¢¯åº¦å¤§å°
- **è§£å†³**ï¼šä½¿ç”¨ReLUæ¿€æ´»å‡½æ•°ã€æ®‹å·®è¿æ¥ã€æ‰¹å½’ä¸€åŒ–

**æ•°å€¼æº¢å‡ºé—®é¢˜**ï¼š
- **åŸå› **ï¼šæ¿€æ´»å‡½æ•°è¾“å…¥è¿‡å¤§
- **è¡¨ç°**ï¼šè®¡ç®—å‡ºç°NaNæˆ–Inf
- **æ£€æµ‹**ï¼šæ£€æŸ¥ä¸­é—´è®¡ç®—ç»“æœ
- **è§£å†³**ï¼šä½¿ç”¨æ•°å€¼ç¨³å®šçš„æ¿€æ´»å‡½æ•°å®ç°

**7. ç½‘ç»œæ¶æ„è®¾è®¡åŸåˆ™**

**è¾“å…¥å±‚è®¾è®¡**ï¼š
- **ç»´åº¦åŒ¹é…**ï¼šè¾“å…¥ç»´åº¦å¿…é¡»ä¸æ•°æ®ç‰¹å¾ç»´åº¦åŒ¹é…
- **æ•°æ®é¢„å¤„ç†**ï¼šå½’ä¸€åŒ–ã€æ ‡å‡†åŒ–è¾“å…¥æ•°æ®
- **ç‰¹å¾å·¥ç¨‹**ï¼šæ ¹æ®ä»»åŠ¡é€‰æ‹©åˆé€‚çš„ç‰¹å¾

**éšè—å±‚è®¾è®¡**ï¼š
- **å±‚æ•°é€‰æ‹©**ï¼šä»ç®€å•å¼€å§‹ï¼Œé€æ­¥å¢åŠ å¤æ‚åº¦
- **ç¥ç»å…ƒæ•°é‡**ï¼šé€šå¸¸é€å±‚é€’å‡æˆ–ä¿æŒæ’å®š
- **æ¿€æ´»å‡½æ•°**ï¼šReLUæ˜¯éšè—å±‚çš„é¦–é€‰
- **æ­£åˆ™åŒ–**ï¼šä½¿ç”¨Dropoutã€æ‰¹å½’ä¸€åŒ–ç­‰

**è¾“å‡ºå±‚è®¾è®¡**ï¼š
- **å›å½’é—®é¢˜**ï¼šçº¿æ€§æ¿€æ´»å‡½æ•°ï¼Œè¾“å‡ºç»´åº¦ä¸º1
- **äºŒåˆ†ç±»**ï¼šSigmoidæ¿€æ´»å‡½æ•°ï¼Œè¾“å‡ºç»´åº¦ä¸º1
- **å¤šåˆ†ç±»**ï¼šSoftmaxæ¿€æ´»å‡½æ•°ï¼Œè¾“å‡ºç»´åº¦ä¸ºç±»åˆ«æ•°

**8. ç½‘ç»œè¡¨è¾¾èƒ½åŠ›åˆ†æ**

**å‚æ•°æ•°é‡è®¡ç®—**ï¼š
```
æ€»å‚æ•° = Î£(nâ½Ë¡â¾ Ã— nâ½Ë¡â»Â¹â¾ + nâ½Ë¡â¾)
å…¶ä¸­nâ½Ë¡â¾æ˜¯ç¬¬lå±‚çš„ç¥ç»å…ƒæ•°é‡
```

**è®¡ç®—å¤æ‚åº¦åˆ†æ**ï¼š
- **å‰å‘ä¼ æ’­**ï¼šO(Î£nâ½Ë¡â¾nâ½Ë¡â»Â¹â¾)
- **å†…å­˜ä½¿ç”¨**ï¼šO(Î£nâ½Ë¡â¾)
- **æ¢¯åº¦è®¡ç®—**ï¼šO(Î£nâ½Ë¡â¾nâ½Ë¡â»Â¹â¾)

**è¿‡æ‹Ÿåˆä¸æ¬ æ‹Ÿåˆ**ï¼š
- **è¿‡æ‹Ÿåˆ**ï¼šç½‘ç»œå®¹é‡è¿‡å¤§ï¼Œåœ¨è®­ç»ƒé›†ä¸Šè¡¨ç°å¥½ä½†æ³›åŒ–å·®
- **æ¬ æ‹Ÿåˆ**ï¼šç½‘ç»œå®¹é‡ä¸è¶³ï¼Œåœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸Šè¡¨ç°éƒ½å·®
- **è§£å†³æ–¹æ¡ˆ**ï¼šè°ƒæ•´ç½‘ç»œå¤§å°ã€ä½¿ç”¨æ­£åˆ™åŒ–ã€å¢åŠ æ•°æ®

**9. ç½‘ç»œåˆå§‹åŒ–ç­–ç•¥è¯¦è§£**

**Xavieråˆå§‹åŒ–çš„æ•°å­¦æ¨å¯¼**ï¼š
å‡è®¾è¾“å…¥xçš„æ–¹å·®ä¸ºVar(x)ï¼Œæƒé‡Wçš„æ–¹å·®ä¸ºVar(W)ï¼Œåˆ™ï¼š
```
Var(z) = n_in Ã— Var(W) Ã— Var(x)
```
ä¸ºäº†ä¿æŒæ–¹å·®ä¸å˜ï¼Œéœ€è¦ï¼š
```
Var(W) = 1/n_in
```
å¯¹äºå‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­éƒ½é€‚ç”¨ï¼š
```
Var(W) = 2/(n_in + n_out)
```

**Heåˆå§‹åŒ–çš„æ•°å­¦æ¨å¯¼**ï¼š
å¯¹äºReLUæ¿€æ´»å‡½æ•°ï¼Œä¸€åŠçš„ç¥ç»å…ƒè¾“å‡ºä¸º0ï¼Œå› æ­¤ï¼š
```
Var(z) = n_in/2 Ã— Var(W) Ã— Var(x)
```
ä¸ºäº†ä¿æŒæ–¹å·®ä¸å˜ï¼š
```
Var(W) = 2/n_in
```

**10. ç½‘ç»œè®­ç»ƒçš„ç†è®ºåŸºç¡€**

**æŸå¤±å‡½æ•°çš„å‡¸æ€§**ï¼š
- **å‡¸å‡½æ•°**ï¼šå…¨å±€æœ€å°å€¼å”¯ä¸€ï¼Œæ¢¯åº¦ä¸‹é™æ”¶æ•›åˆ°å…¨å±€æœ€ä¼˜
- **éå‡¸å‡½æ•°**ï¼šå¤šä¸ªå±€éƒ¨æœ€å°å€¼ï¼Œæ¢¯åº¦ä¸‹é™å¯èƒ½é™·å…¥å±€éƒ¨æœ€ä¼˜
- **ç¥ç»ç½‘ç»œ**ï¼šæŸå¤±å‡½æ•°é€šå¸¸æ˜¯éå‡¸çš„

**æ”¶æ•›æ€§åˆ†æ**ï¼š
- **ç†è®ºä¿è¯**ï¼šåœ¨å‡¸å‡½æ•°ä¸Šï¼Œæ¢¯åº¦ä¸‹é™æ”¶æ•›åˆ°å…¨å±€æœ€ä¼˜
- **å®é™…è¡¨ç°**ï¼šåœ¨éå‡¸å‡½æ•°ä¸Šï¼Œé€šå¸¸æ”¶æ•›åˆ°å±€éƒ¨æœ€ä¼˜
- **åˆå§‹åŒ–å½±å“**ï¼šå¥½çš„åˆå§‹åŒ–æœ‰åŠ©äºæ‰¾åˆ°æ›´å¥½çš„å±€éƒ¨æœ€ä¼˜

**11. ç½‘ç»œæ¶æ„çš„ç”Ÿç‰©å­¦å¯å‘**

**è§†è§‰çš®å±‚ç»“æ„**ï¼š
- **ç®€å•ç»†èƒ**ï¼šæ£€æµ‹è¾¹ç¼˜ã€çº¿æ¡ç­‰ç®€å•ç‰¹å¾
- **å¤æ‚ç»†èƒ**ï¼šæ£€æµ‹æ–¹å‘ã€è¿åŠ¨ç­‰å¤æ‚ç‰¹å¾
- **è¶…å¤æ‚ç»†èƒ**ï¼šæ£€æµ‹å½¢çŠ¶ã€ç‰©ä½“ç­‰é«˜çº§ç‰¹å¾

**ç¥ç»ç½‘ç»œæ¨¡æ‹Ÿ**ï¼š
- **æµ…å±‚**ï¼šå­¦ä¹ ç®€å•ç‰¹å¾ï¼ˆè¾¹ç¼˜ã€çº¹ç†ï¼‰
- **ä¸­å±‚**ï¼šå­¦ä¹ ä¸­ç­‰å¤æ‚åº¦ç‰¹å¾ï¼ˆå½¢çŠ¶ã€éƒ¨ä»¶ï¼‰
- **æ·±å±‚**ï¼šå­¦ä¹ é«˜çº§ç‰¹å¾ï¼ˆç‰©ä½“ã€è¯­ä¹‰ï¼‰

**ç½‘ç»œæ¶æ„çš„å…·ä½“ä¾‹å­**ï¼š

**å›¾åƒè¯†åˆ«ç½‘ç»œçš„å±‚æ¬¡ç»“æ„**ï¼š
- **ç¬¬ä¸€å±‚**ï¼šæ£€æµ‹è¾¹ç¼˜å’Œçº¿æ¡
  - ä¾‹å­ï¼šæ£€æµ‹æ°´å¹³çº¿ã€å‚ç›´çº¿ã€å¯¹è§’çº¿
  - å®é™…æ„ä¹‰ï¼šå°±åƒäººçœ¼é¦–å…ˆæ³¨æ„åˆ°ç‰©ä½“çš„è½®å»“
  - åº”ç”¨ï¼šè¾¹ç¼˜æ£€æµ‹ã€è½®å»“æå–

- **ç¬¬äºŒå±‚**ï¼šæ£€æµ‹ç®€å•å½¢çŠ¶
  - ä¾‹å­ï¼šæ£€æµ‹åœ†å½¢ã€æ–¹å½¢ã€ä¸‰è§’å½¢
  - å®é™…æ„ä¹‰ï¼šç»„åˆè¾¹ç¼˜å½¢æˆåŸºæœ¬å½¢çŠ¶
  - åº”ç”¨ï¼šå‡ ä½•å½¢çŠ¶è¯†åˆ«

- **ç¬¬ä¸‰å±‚**ï¼šæ£€æµ‹å¤æ‚ç‰¹å¾
  - ä¾‹å­ï¼šæ£€æµ‹çœ¼ç›ã€é¼»å­ã€å˜´å·´
  - å®é™…æ„ä¹‰ï¼šç»„åˆå½¢çŠ¶å½¢æˆé¢éƒ¨ç‰¹å¾
  - åº”ç”¨ï¼šäººè„¸æ£€æµ‹

- **ç¬¬å››å±‚**ï¼šæ£€æµ‹å®Œæ•´ç‰©ä½“
  - ä¾‹å­ï¼šæ£€æµ‹äººè„¸ã€æ±½è½¦ã€åŠ¨ç‰©
  - å®é™…æ„ä¹‰ï¼šç»„åˆç‰¹å¾å½¢æˆå®Œæ•´ç‰©ä½“
  - åº”ç”¨ï¼šç‰©ä½“è¯†åˆ«

**è¯­éŸ³è¯†åˆ«ç½‘ç»œçš„å±‚æ¬¡ç»“æ„**ï¼š
- **ç¬¬ä¸€å±‚**ï¼šæ£€æµ‹éŸ³é¢‘ç‰¹å¾
  - ä¾‹å­ï¼šæ£€æµ‹é¢‘ç‡ã€æŒ¯å¹…å˜åŒ–
  - å®é™…æ„ä¹‰ï¼šæå–éŸ³é¢‘çš„åŸºæœ¬ç‰¹å¾
  - åº”ç”¨ï¼šéŸ³é¢‘é¢„å¤„ç†

- **ç¬¬äºŒå±‚**ï¼šæ£€æµ‹éŸ³ç´ 
  - ä¾‹å­ï¼šæ£€æµ‹"a"ã€"e"ã€"i"ç­‰éŸ³ç´ 
  - å®é™…æ„ä¹‰ï¼šè¯†åˆ«åŸºæœ¬çš„è¯­éŸ³å•ä½
  - åº”ç”¨ï¼šéŸ³ç´ è¯†åˆ«

- **ç¬¬ä¸‰å±‚**ï¼šæ£€æµ‹å•è¯
  - ä¾‹å­ï¼šæ£€æµ‹"hello"ã€"world"ç­‰å•è¯
  - å®é™…æ„ä¹‰ï¼šç»„åˆéŸ³ç´ å½¢æˆå•è¯
  - åº”ç”¨ï¼šå•è¯è¯†åˆ«

- **ç¬¬å››å±‚**ï¼šç†è§£è¯­ä¹‰
  - ä¾‹å­ï¼šç†è§£å¥å­çš„å«ä¹‰
  - å®é™…æ„ä¹‰ï¼šç†è§£è¯­éŸ³çš„è¯­ä¹‰å†…å®¹
  - åº”ç”¨ï¼šè¯­éŸ³ç†è§£

**ç½‘ç»œæ·±åº¦ä¸å®½åº¦çš„å®é™…æ„ä¹‰**ï¼š

**æ·±åº¦ç½‘ç»œçš„ä¼˜åŠ¿**ï¼š
- **å±‚æ¬¡åŒ–å­¦ä¹ **ï¼šå°±åƒå­¦ä¹ æ•°å­¦ï¼Œå…ˆå­¦åŠ æ³•ï¼Œå†å­¦ä¹˜æ³•ï¼Œæœ€åå­¦å¾®ç§¯åˆ†
  - æµ…å±‚ï¼šå­¦ä¹ åŸºç¡€æ¦‚å¿µ
  - æ·±å±‚ï¼šå­¦ä¹ å¤æ‚æ¦‚å¿µ
- **ç‰¹å¾å¤ç”¨**ï¼šå°±åƒæ­ç§¯æœ¨ï¼Œç”¨åŸºç¡€ç§¯æœ¨æ­å»ºå¤æ‚ç»“æ„
  - åº•å±‚ç‰¹å¾ï¼šå¯ä»¥è¢«å¤šä¸ªé«˜å±‚ç‰¹å¾ä½¿ç”¨
  - æ•ˆç‡ï¼šå‡å°‘é‡å¤å­¦ä¹ 

**å®½åº¦ç½‘ç»œçš„ä¼˜åŠ¿**ï¼š
- **å¹¶è¡Œå¤„ç†**ï¼šå°±åƒå¤šä¸ªäººåŒæ—¶å·¥ä½œ
  - ä¾‹å­ï¼šå¤šä¸ªä¸“å®¶åŒæ—¶åˆ†æä¸åŒæ–¹é¢
  - æ•ˆç‡ï¼šæé«˜å¤„ç†é€Ÿåº¦
- **å†—ä½™æ€§**ï¼šå°±åƒå¤‡ä»½ç³»ç»Ÿï¼Œæé«˜å¯é æ€§
  - ä¾‹å­ï¼šå¤šä¸ªç¥ç»å…ƒæ£€æµ‹åŒä¸€ç‰¹å¾
  - é²æ£’æ€§ï¼šå³ä½¿éƒ¨åˆ†ç¥ç»å…ƒå¤±æ•ˆï¼Œç³»ç»Ÿä»èƒ½å·¥ä½œ

**å®é™…åº”ç”¨ä¸­çš„é€‰æ‹©**ï¼š
- **å›¾åƒè¯†åˆ«**ï¼šé€šå¸¸ä½¿ç”¨æ·±åº¦ç½‘ç»œ
  - åŸå› ï¼šå›¾åƒç‰¹å¾å…·æœ‰æ˜æ˜¾çš„å±‚æ¬¡æ€§
  - ä¾‹å­ï¼šè¾¹ç¼˜â†’å½¢çŠ¶â†’éƒ¨ä»¶â†’ç‰©ä½“
- **æ¨èç³»ç»Ÿ**ï¼šé€šå¸¸ä½¿ç”¨å®½åº¦ç½‘ç»œ
  - åŸå› ï¼šéœ€è¦åŒæ—¶è€ƒè™‘å¤šç§ç‰¹å¾
  - ä¾‹å­ï¼šç”¨æˆ·å…´è¶£ã€å•†å“ç‰¹å¾ã€æ—¶é—´å› ç´ 
- **è¯­éŸ³è¯†åˆ«**ï¼šæ·±åº¦å’Œå®½åº¦ç»“åˆ
  - åŸå› ï¼šæ—¢éœ€è¦å±‚æ¬¡åŒ–å¤„ç†ï¼Œåˆéœ€è¦å¹¶è¡Œåˆ†æ
  - ä¾‹å­ï¼šCNN+RNNçš„æ··åˆæ¶æ„

**12. ç½‘ç»œè®¾è®¡çš„æœ€ä½³å®è·µ**

**æ¸è¿›å¼è®¾è®¡**ï¼š
1. **ä»ç®€å•å¼€å§‹**ï¼šå…ˆè®¾è®¡ç®€å•çš„ç½‘ç»œ
2. **é€æ­¥å¢åŠ å¤æ‚åº¦**ï¼šæ ¹æ®æ€§èƒ½è°ƒæ•´ç½‘ç»œç»“æ„
3. **éªŒè¯æ”¹è¿›**ï¼šæ¯æ¬¡ä¿®æ”¹åéªŒè¯æ€§èƒ½æå‡
4. **é¿å…è¿‡æ‹Ÿåˆ**ï¼šä½¿ç”¨éªŒè¯é›†ç›‘æ§æ³›åŒ–æ€§èƒ½

**è¶…å‚æ•°è°ƒä¼˜**ï¼š
- **å­¦ä¹ ç‡**ï¼šæœ€é‡è¦çš„è¶…å‚æ•°ï¼Œå½±å“æ”¶æ•›é€Ÿåº¦å’Œç¨³å®šæ€§
- **æ‰¹é‡å¤§å°**ï¼šå½±å“æ¢¯åº¦ä¼°è®¡çš„å‡†ç¡®æ€§å’Œå†…å­˜ä½¿ç”¨
- **ç½‘ç»œå¤§å°**ï¼šå½±å“æ¨¡å‹å®¹é‡å’Œè®¡ç®—å¤æ‚åº¦
- **æ­£åˆ™åŒ–å‚æ•°**ï¼šæ§åˆ¶è¿‡æ‹Ÿåˆç¨‹åº¦

**ç½‘ç»œå®¹é‡**ï¼š
- **å‚æ•°æ•°é‡**ï¼šÎ£(nâ½Ë¡â¾ Ã— nâ½Ë¡â»Â¹â¾ + nâ½Ë¡â¾)
- **VCç»´åº¦**ï¼šè¡¡é‡ç½‘ç»œå¤æ‚åº¦
- **è¡¨è¾¾èƒ½åŠ›**ï¼šç½‘ç»œèƒ½å¤Ÿè¡¨ç¤ºçš„å‡½æ•°ç©ºé—´

**è¿‡æ‹Ÿåˆä¸æ¬ æ‹Ÿåˆ**ï¼š
- **æ¬ æ‹Ÿåˆ**ï¼šç½‘ç»œå®¹é‡ä¸è¶³ï¼Œæ— æ³•å­¦ä¹ å¤æ‚æ¨¡å¼
- **è¿‡æ‹Ÿåˆ**ï¼šç½‘ç»œå®¹é‡è¿‡å¤§ï¼Œè®°å¿†è®­ç»ƒæ•°æ®
- **è§£å†³æ–¹æ¡ˆ**ï¼šæ­£åˆ™åŒ–ã€æ—©åœã€æ•°æ®å¢å¼º

**5. ç½‘ç»œæ¶æ„è®¾è®¡åŸåˆ™**

**è¾“å…¥å±‚è®¾è®¡**ï¼š
- **ç»´åº¦åŒ¹é…**ï¼šè¾“å…¥ç»´åº¦å¿…é¡»ä¸æ•°æ®ç‰¹å¾ç»´åº¦åŒ¹é…
- **æ•°æ®é¢„å¤„ç†**ï¼šå½’ä¸€åŒ–ã€æ ‡å‡†åŒ–ã€ç¼–ç 

**éšè—å±‚è®¾è®¡**ï¼š
- **å±‚æ•°é€‰æ‹©**ï¼šä»ç®€å•å¼€å§‹ï¼Œé€æ­¥å¢åŠ å¤æ‚åº¦
- **ç¥ç»å…ƒæ•°é‡**ï¼šé€šå¸¸é€å±‚é€’å‡æˆ–ä¿æŒæ’å®š
- **æ¿€æ´»å‡½æ•°**ï¼šReLUåŠå…¶å˜ä½“æ˜¯é¦–é€‰

**è¾“å‡ºå±‚è®¾è®¡**ï¼š
- **å›å½’é—®é¢˜**ï¼šçº¿æ€§æ¿€æ´»å‡½æ•°ï¼Œè¾“å‡ºç»´åº¦=1
- **äºŒåˆ†ç±»**ï¼šSigmoidæ¿€æ´»å‡½æ•°ï¼Œè¾“å‡ºç»´åº¦=1
- **å¤šåˆ†ç±»**ï¼šSoftmaxæ¿€æ´»å‡½æ•°ï¼Œè¾“å‡ºç»´åº¦=ç±»åˆ«æ•°

**6. å‰å‘ä¼ æ’­çš„æ•°å€¼ä¼˜åŒ–**

**æ‰¹é‡å¤„ç†**ï¼š
```c
// æ‰¹é‡å‰å‘ä¼ æ’­
void forward_propagation_batch(NeuralNetwork* nn, float* input, float* output, int batch_size) {
    for (int b = 0; b < batch_size; b++) {
        float* batch_input = &input[b * nn->layers[0].input_size];
        float* batch_output = &output[b * nn->layers[nn->num_layers-1].output_size];
        forward_propagation_single(nn, batch_input, batch_output);
    }
}
```

**å†…å­˜ä¼˜åŒ–**ï¼š
```c
// å†…å­˜æ± ç®¡ç†
typedef struct {
    float* pool;
    int used;
    int capacity;
} MemoryPool;

float* memory_pool_alloc(MemoryPool* pool, int size) {
    if (pool->used + size <= pool->capacity) {
        float* ptr = &pool->pool[pool->used];
        pool->used += size;
        return ptr;
    }
    return NULL;
}
```

**å¹¶è¡ŒåŒ–è€ƒè™‘**ï¼š
- **SIMDæŒ‡ä»¤**ï¼šåˆ©ç”¨å‘é‡åŒ–æŒ‡ä»¤åŠ é€ŸçŸ©é˜µè¿ç®—
- **å¤šçº¿ç¨‹**ï¼šå¹¶è¡Œå¤„ç†ä¸åŒæ ·æœ¬
- **GPUåŠ é€Ÿ**ï¼šå¤§è§„æ¨¡å¹¶è¡Œè®¡ç®—

#### å®è·µç»ƒä¹ 

**ç»ƒä¹ 1ï¼šå¤šå±‚ç¥ç»ç½‘ç»œæ•°æ®ç»“æ„è®¾è®¡**

```c
// å®Œæ•´çš„ç¥ç»ç½‘ç»œæ•°æ®ç»“æ„
typedef struct {
    int input_size;
    int output_size;
    float* weights;
    float* biases;
    activation_func_t activation;
    activation_func_t activation_derivative;
} Layer;

typedef struct {
    Layer* layers;
    int num_layers;
    float learning_rate;
    int* layer_sizes;  // å­˜å‚¨æ¯å±‚çš„ç¥ç»å…ƒæ•°é‡
} NeuralNetwork;

// æƒé‡åˆå§‹åŒ–ç­–ç•¥
void xavier_init(float* weights, int input_size, int output_size) {
    float scale = sqrtf(2.0f / (input_size + output_size));
    for (int i = 0; i < input_size * output_size; i++) {
        weights[i] = ((float)rand() / RAND_MAX - 0.5f) * 2.0f * scale;
    }
}

void he_init(float* weights, int input_size, int output_size) {
    float scale = sqrtf(2.0f / input_size);
    for (int i = 0; i < input_size * output_size; i++) {
        weights[i] = ((float)rand() / RAND_MAX - 0.5f) * 2.0f * scale;
    }
}

void random_init(float* weights, int input_size, int output_size, float scale) {
    for (int i = 0; i < input_size * output_size; i++) {
        weights[i] = ((float)rand() / RAND_MAX - 0.5f) * 2.0f * scale;
    }
}

// ç¥ç»ç½‘ç»œåˆ›å»º
NeuralNetwork* nn_create(int* layer_sizes, int num_layers, float learning_rate) {
    NeuralNetwork* nn = malloc(sizeof(NeuralNetwork));
    nn->num_layers = num_layers - 1;  // å±‚æ•° = ç¥ç»å…ƒæ•°é‡ - 1
    nn->learning_rate = learning_rate;
    nn->layers = malloc(nn->num_layers * sizeof(Layer));
    nn->layer_sizes = malloc(num_layers * sizeof(int));
    
    // å¤åˆ¶å±‚å¤§å°ä¿¡æ¯
    for (int i = 0; i < num_layers; i++) {
        nn->layer_sizes[i] = layer_sizes[i];
    }
    
    // åˆå§‹åŒ–æ¯ä¸€å±‚
    for (int i = 0; i < nn->num_layers; i++) {
        Layer* layer = &nn->layers[i];
        layer->input_size = layer_sizes[i];
        layer->output_size = layer_sizes[i + 1];
        
        // åˆ†é…å†…å­˜
        layer->weights = malloc(layer->output_size * layer->input_size * sizeof(float));
        layer->biases = malloc(layer->output_size * sizeof(float));
        
        // åˆå§‹åŒ–æƒé‡ï¼ˆä½¿ç”¨Xavieråˆå§‹åŒ–ï¼‰
        xavier_init(layer->weights, layer->input_size, layer->output_size);
        
        // åˆå§‹åŒ–åç½®
        for (int j = 0; j < layer->output_size; j++) {
            layer->biases[j] = 0.0f;
        }
        
        // è®¾ç½®æ¿€æ´»å‡½æ•°
        if (i == nn->num_layers - 1) {
            // è¾“å‡ºå±‚ä½¿ç”¨çº¿æ€§æ¿€æ´»å‡½æ•°
            layer->activation = linear_activation;
            layer->activation_derivative = linear_derivative;
        } else {
            // éšè—å±‚ä½¿ç”¨ReLUæ¿€æ´»å‡½æ•°
            layer->activation = relu;
            layer->activation_derivative = relu_derivative;
        }
    }
    
    return nn;
}

// çº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆç”¨äºè¾“å‡ºå±‚ï¼‰
float linear_activation(float x) {
    return x;
}

float linear_derivative(float x) {
    return 1.0f;
}
```

**ç»ƒä¹ 2ï¼šå‰å‘ä¼ æ’­å®ç°**

```c
// å•å±‚å‰å‘ä¼ æ’­
void layer_forward(Layer* layer, float* input, float* output) {
    // è®¡ç®—çº¿æ€§ç»„åˆ z = Wx + b
    for (int i = 0; i < layer->output_size; i++) {
        float sum = layer->biases[i];
        for (int j = 0; j < layer->input_size; j++) {
            sum += layer->weights[i * layer->input_size + j] * input[j];
        }
        output[i] = layer->activation(sum);
    }
}

// å®Œæ•´çš„å‰å‘ä¼ æ’­
void nn_forward(NeuralNetwork* nn, float* input, float* output) {
    float* current_input = input;
    float* layer_output = malloc(nn->layers[0].output_size * sizeof(float));
    
    // é€å±‚å‰å‘ä¼ æ’­
    for (int l = 0; l < nn->num_layers; l++) {
        Layer* layer = &nn->layers[l];
        
        // è®¡ç®—å½“å‰å±‚è¾“å‡º
        layer_forward(layer, current_input, layer_output);
        
        // å¦‚æœä¸æ˜¯æœ€åä¸€å±‚ï¼Œå‡†å¤‡ä¸‹ä¸€å±‚çš„è¾“å…¥
        if (l < nn->num_layers - 1) {
            current_input = layer_output;
            layer_output = malloc(nn->layers[l + 1].output_size * sizeof(float));
        }
    }
    
    // å¤åˆ¶æœ€ç»ˆè¾“å‡º
    for (int i = 0; i < nn->layers[nn->num_layers - 1].output_size; i++) {
        output[i] = layer_output[i];
    }
    
    free(layer_output);
}

// å¸¦ç¼“å­˜çš„å‰å‘ä¼ æ’­ï¼ˆç”¨äºåå‘ä¼ æ’­ï¼‰
typedef struct {
    float* activations[MAX_LAYERS + 1];  // æ¯å±‚çš„æ¿€æ´»å€¼
    float* z_values[MAX_LAYERS];         // æ¯å±‚çš„çº¿æ€§ç»„åˆå€¼
} ForwardCache;

void nn_forward_with_cache(NeuralNetwork* nn, float* input, float* output, ForwardCache* cache) {
    // å­˜å‚¨è¾“å…¥å±‚æ¿€æ´»å€¼
    cache->activations[0] = input;
    
    float* current_input = input;
    
    for (int l = 0; l < nn->num_layers; l++) {
        Layer* layer = &nn->layers[l];
        
        // åˆ†é…å½“å‰å±‚çš„ç¼“å­˜
        cache->z_values[l] = malloc(layer->output_size * sizeof(float));
        cache->activations[l + 1] = malloc(layer->output_size * sizeof(float));
        
        // è®¡ç®—çº¿æ€§ç»„åˆ
        for (int i = 0; i < layer->output_size; i++) {
            float sum = layer->biases[i];
            for (int j = 0; j < layer->input_size; j++) {
                sum += layer->weights[i * layer->input_size + j] * current_input[j];
            }
            cache->z_values[l][i] = sum;
            cache->activations[l + 1][i] = layer->activation(sum);
        }
        
        current_input = cache->activations[l + 1];
    }
    
    // å¤åˆ¶è¾“å‡º
    int output_size = nn->layers[nn->num_layers - 1].output_size;
    for (int i = 0; i < output_size; i++) {
        output[i] = cache->activations[nn->num_layers][i];
    }
}
```

**ç»ƒä¹ 3ï¼šæƒé‡åˆå§‹åŒ–æµ‹è¯•**

```c
// æƒé‡åˆå§‹åŒ–æµ‹è¯•
void test_weight_initialization() {
    printf("=== æƒé‡åˆå§‹åŒ–æµ‹è¯• ===\n");
    
    const int input_size = 10;
    const int output_size = 5;
    const int num_tests = 1000;
    
    float* weights_xavier = malloc(input_size * output_size * sizeof(float));
    float* weights_he = malloc(input_size * output_size * sizeof(float));
    float* weights_random = malloc(input_size * output_size * sizeof(float));
    
    // ç»Ÿè®¡å˜é‡
    float xavier_mean = 0, he_mean = 0, random_mean = 0;
    float xavier_var = 0, he_var = 0, random_var = 0;
    
    for (int test = 0; test < num_tests; test++) {
        // Xavieråˆå§‹åŒ–
        xavier_init(weights_xavier, input_size, output_size);
        for (int i = 0; i < input_size * output_size; i++) {
            xavier_mean += weights_xavier[i];
            xavier_var += weights_xavier[i] * weights_xavier[i];
        }
        
        // Heåˆå§‹åŒ–
        he_init(weights_he, input_size, output_size);
        for (int i = 0; i < input_size * output_size; i++) {
            he_mean += weights_he[i];
            he_var += weights_he[i] * weights_he[i];
        }
        
        // éšæœºåˆå§‹åŒ–
        random_init(weights_random, input_size, output_size, 0.01f);
        for (int i = 0; i < input_size * output_size; i++) {
            random_mean += weights_random[i];
            random_var += weights_random[i] * weights_random[i];
        }
    }
    
    // è®¡ç®—ç»Ÿè®¡é‡
    xavier_mean /= (num_tests * input_size * output_size);
    he_mean /= (num_tests * input_size * output_size);
    random_mean /= (num_tests * input_size * output_size);
    
    xavier_var = xavier_var / (num_tests * input_size * output_size) - xavier_mean * xavier_mean;
    he_var = he_var / (num_tests * input_size * output_size) - he_mean * he_mean;
    random_var = random_var / (num_tests * input_size * output_size) - random_mean * random_mean;
    
    printf("Xavieråˆå§‹åŒ–: å‡å€¼=%.4f, æ–¹å·®=%.4f\n", xavier_mean, xavier_var);
    printf("Heåˆå§‹åŒ–: å‡å€¼=%.4f, æ–¹å·®=%.4f\n", he_mean, he_var);
    printf("éšæœºåˆå§‹åŒ–: å‡å€¼=%.4f, æ–¹å·®=%.4f\n", random_mean, random_var);
    
    free(weights_xavier);
    free(weights_he);
    free(weights_random);
}
```

**ç»ƒä¹ 4ï¼šç½‘ç»œæ¶æ„æµ‹è¯•**

```c
// ç½‘ç»œæ¶æ„æµ‹è¯•
void test_network_architecture() {
    printf("=== ç½‘ç»œæ¶æ„æµ‹è¯• ===\n");
    
    // åˆ›å»ºä¸€ä¸ªç®€å•çš„ç½‘ç»œï¼š2è¾“å…¥ -> 3éšè— -> 1è¾“å‡º
    int layer_sizes[] = {2, 3, 1};
    NeuralNetwork* nn = nn_create(layer_sizes, 3, 0.01f);
    
    printf("ç½‘ç»œç»“æ„:\n");
    printf("è¾“å…¥å±‚: %d ä¸ªç¥ç»å…ƒ\n", nn->layer_sizes[0]);
    for (int i = 0; i < nn->num_layers; i++) {
        Layer* layer = &nn->layers[i];
        printf("ç¬¬%då±‚: %d -> %d ä¸ªç¥ç»å…ƒ\n", i + 1, layer->input_size, layer->output_size);
    }
    
    // æµ‹è¯•å‰å‘ä¼ æ’­
    float input[] = {0.5f, -0.3f};
    float output[1];
    
    nn_forward(nn, input, output);
    
    printf("è¾“å…¥: [%.1f, %.1f]\n", input[0], input[1]);
    printf("è¾“å‡º: %.4f\n", output[0]);
    
    // æµ‹è¯•ä¸åŒè¾“å…¥
    float test_inputs[][2] = {{1.0f, 0.0f}, {0.0f, 1.0f}, {-1.0f, -1.0f}};
    printf("\nä¸åŒè¾“å…¥çš„æµ‹è¯•:\n");
    for (int i = 0; i < 3; i++) {
        nn_forward(nn, test_inputs[i], output);
        printf("è¾“å…¥[%.1f, %.1f] -> è¾“å‡º: %.4f\n", 
               test_inputs[i][0], test_inputs[i][1], output[0]);
    }
    
    // æ¸…ç†
    nn_free(nn);
}

// ç¥ç»ç½‘ç»œé‡Šæ”¾å‡½æ•°
void nn_free(NeuralNetwork* nn) {
    for (int i = 0; i < nn->num_layers; i++) {
        free(nn->layers[i].weights);
        free(nn->layers[i].biases);
    }
    free(nn->layers);
    free(nn->layer_sizes);
    free(nn);
}
```

**ç»ƒä¹ 5ï¼šæ€§èƒ½ä¼˜åŒ–æµ‹è¯•**

```c
// æ€§èƒ½ä¼˜åŒ–æµ‹è¯•
void performance_optimization_test() {
    printf("=== æ€§èƒ½ä¼˜åŒ–æµ‹è¯• ===\n");
    
    // åˆ›å»ºè¾ƒå¤§çš„ç½‘ç»œè¿›è¡Œæ€§èƒ½æµ‹è¯•
    int layer_sizes[] = {100, 50, 25, 10, 1};
    NeuralNetwork* nn = nn_create(layer_sizes, 5, 0.01f);
    
    // ç”Ÿæˆæµ‹è¯•æ•°æ®
    const int num_samples = 1000;
    float* inputs = malloc(100 * num_samples * sizeof(float));
    float* outputs = malloc(num_samples * sizeof(float));
    
    for (int i = 0; i < 100 * num_samples; i++) {
        inputs[i] = ((float)rand() / RAND_MAX - 0.5f) * 2.0f;
    }
    
    // æµ‹è¯•æ ‡å‡†å‰å‘ä¼ æ’­æ€§èƒ½
    clock_t start = clock();
    for (int i = 0; i < num_samples; i++) {
        nn_forward(nn, &inputs[i * 100], &outputs[i]);
    }
    clock_t end = clock();
    double standard_time = ((double)(end - start)) / CLOCKS_PER_SEC;
    
    printf("æ ‡å‡†å‰å‘ä¼ æ’­: %.4f ç§’ (%d ä¸ªæ ·æœ¬)\n", standard_time, num_samples);
    printf("å¹³å‡æ¯ä¸ªæ ·æœ¬: %.6f ç§’\n", standard_time / num_samples);
    
    // æµ‹è¯•æ‰¹é‡å¤„ç†æ€§èƒ½
    start = clock();
    for (int batch = 0; batch < num_samples; batch += 32) {
        int batch_size = (batch + 32 <= num_samples) ? 32 : (num_samples - batch);
        for (int i = 0; i < batch_size; i++) {
            nn_forward(nn, &inputs[(batch + i) * 100], &outputs[batch + i]);
        }
    }
    end = clock();
    double batch_time = ((double)(end - start)) / CLOCKS_PER_SEC;
    
    printf("æ‰¹é‡å¤„ç†: %.4f ç§’ (%d ä¸ªæ ·æœ¬)\n", batch_time, num_samples);
    printf("æ€§èƒ½æå‡: %.2f%%\n", (standard_time - batch_time) / standard_time * 100);
    
    free(inputs);
    free(outputs);
    nn_free(nn);
}
```

**ç»ƒä¹ 6ï¼šå†…å­˜ä¼˜åŒ–æµ‹è¯•**

```c
// å†…å­˜æ± å®ç°
typedef struct {
    float* pool;
    int used;
    int capacity;
} MemoryPool;

MemoryPool* memory_pool_create(int capacity) {
    MemoryPool* pool = malloc(sizeof(MemoryPool));
    pool->pool = malloc(capacity * sizeof(float));
    pool->used = 0;
    pool->capacity = capacity;
    return pool;
}

float* memory_pool_alloc(MemoryPool* pool, int size) {
    if (pool->used + size <= pool->capacity) {
        float* ptr = &pool->pool[pool->used];
        pool->used += size;
        return ptr;
    }
    return NULL;
}

void memory_pool_reset(MemoryPool* pool) {
    pool->used = 0;
}

void memory_pool_free(MemoryPool* pool) {
    free(pool->pool);
    free(pool);
}

// ä½¿ç”¨å†…å­˜æ± çš„å‰å‘ä¼ æ’­
void nn_forward_with_pool(NeuralNetwork* nn, float* input, float* output, MemoryPool* pool) {
    memory_pool_reset(pool);
    
    float* current_input = input;
    float* layer_output = memory_pool_alloc(pool, nn->layers[0].output_size);
    
    for (int l = 0; l < nn->num_layers; l++) {
        Layer* layer = &nn->layers[l];
        
        layer_forward(layer, current_input, layer_output);
        
        if (l < nn->num_layers - 1) {
            current_input = layer_output;
            layer_output = memory_pool_alloc(pool, nn->layers[l + 1].output_size);
        }
    }
    
    for (int i = 0; i < nn->layers[nn->num_layers - 1].output_size; i++) {
        output[i] = layer_output[i];
    }
}

// å†…å­˜ä½¿ç”¨æµ‹è¯•
void memory_usage_test() {
    printf("=== å†…å­˜ä½¿ç”¨æµ‹è¯• ===\n");
    
    int layer_sizes[] = {50, 30, 20, 10, 1};
    NeuralNetwork* nn = nn_create(layer_sizes, 5, 0.01f);
    
    // è®¡ç®—ç½‘ç»œå‚æ•°æ•°é‡
    int total_params = 0;
    for (int i = 0; i < nn->num_layers; i++) {
        Layer* layer = &nn->layers[i];
        total_params += layer->output_size * layer->input_size + layer->output_size;
    }
    
    printf("ç½‘ç»œå‚æ•°æ•°é‡: %d\n", total_params);
    printf("å‚æ•°å†…å­˜å ç”¨: %.2f KB\n", total_params * sizeof(float) / 1024.0f);
    
    // æµ‹è¯•å†…å­˜æ± 
    MemoryPool* pool = memory_pool_create(10000);
    float input[50];
    float output[1];
    
    for (int i = 0; i < 50; i++) {
        input[i] = ((float)rand() / RAND_MAX - 0.5f) * 2.0f;
    }
    
    nn_forward_with_pool(nn, input, output, pool);
    printf("ä½¿ç”¨å†…å­˜æ± çš„å‰å‘ä¼ æ’­å®Œæˆ\n");
    
    memory_pool_free(pool);
    nn_free(nn);
}
```

### ç¬¬3å‘¨ï¼šæŸå¤±å‡½æ•°ä¸ä¼˜åŒ–åŸºç¡€

#### å­¦ä¹ å†…å®¹
- **æŸå¤±å‡½æ•°åŸç†**
- **æ¢¯åº¦ä¸‹é™ç®—æ³•**
- **å­¦ä¹ ç‡é€‰æ‹©**

#### ç†è®ºçŸ¥è¯†

**1. æŸå¤±å‡½æ•°è¯¦è§£**

**æŸå¤±å‡½æ•°çš„ä½œç”¨**ï¼š
æŸå¤±å‡½æ•°è¡¡é‡æ¨¡å‹é¢„æµ‹å€¼ä¸çœŸå®å€¼ä¹‹é—´çš„å·®å¼‚ï¼Œæ˜¯è®­ç»ƒè¿‡ç¨‹çš„æŒ‡å¯¼ä¿¡å·ã€‚å¥½çš„æŸå¤±å‡½æ•°åº”è¯¥ï¼š
- èƒ½å¤Ÿåæ˜ é—®é¢˜çš„æœ¬è´¨
- å…·æœ‰è‰¯å¥½çš„æ•°å­¦æ€§è´¨ï¼ˆè¿ç»­ã€å¯å¾®ï¼‰
- æä¾›æœ‰æ•ˆçš„æ¢¯åº¦ä¿¡æ¯

**å‡æ–¹è¯¯å·®(MSE)**ï¼š
```
L = (1/n)Î£(y_pred - y_true)Â²
```
- **é€‚ç”¨åœºæ™¯**ï¼šå›å½’é—®é¢˜
- **ä¼˜ç‚¹**ï¼šè¿ç»­å¯å¾®ï¼Œæ¢¯åº¦è®¡ç®—ç®€å•
- **ç¼ºç‚¹**ï¼šå¯¹å¼‚å¸¸å€¼æ•æ„Ÿï¼Œæ¢¯åº¦åœ¨è¯¯å·®å¤§æ—¶å¯èƒ½çˆ†ç‚¸
- **æ¢¯åº¦**ï¼šâˆ‚L/âˆ‚y_pred = 2(y_pred - y_true)/n

**å¹³å‡ç»å¯¹è¯¯å·®(MAE)**ï¼š
```
L = (1/n)Î£|y_pred - y_true|
```
- **é€‚ç”¨åœºæ™¯**ï¼šå›å½’é—®é¢˜ï¼Œå¯¹å¼‚å¸¸å€¼ä¸æ•æ„Ÿ
- **ä¼˜ç‚¹**ï¼šå¯¹å¼‚å¸¸å€¼é²æ£’
- **ç¼ºç‚¹**ï¼šåœ¨é›¶ç‚¹ä¸å¯å¾®
- **æ¢¯åº¦**ï¼šâˆ‚L/âˆ‚y_pred = sign(y_pred - y_true)/n

**äº¤å‰ç†µæŸå¤±**ï¼š
```
L = -Î£y_true * log(y_pred)
```
- **é€‚ç”¨åœºæ™¯**ï¼šåˆ†ç±»é—®é¢˜
- **ä¼˜ç‚¹**ï¼šä¸æœ€å¤§ä¼¼ç„¶ä¼°è®¡ç­‰ä»·ï¼Œæ¢¯åº¦è®¡ç®—ç®€å•
- **ç¼ºç‚¹**ï¼šæ•°å€¼ä¸ç¨³å®šï¼ˆlog(0) = -âˆï¼‰
- **æ¢¯åº¦**ï¼šâˆ‚L/âˆ‚y_pred = -y_true/y_pred

**äºŒå…ƒäº¤å‰ç†µ**ï¼š
```
L = -[y_true * log(y_pred) + (1-y_true) * log(1-y_pred)]
```
- **é€‚ç”¨åœºæ™¯**ï¼šäºŒåˆ†ç±»é—®é¢˜
- **ä¼˜ç‚¹**ï¼šä¸“é—¨ä¸ºäºŒåˆ†ç±»è®¾è®¡
- **æ¢¯åº¦**ï¼šâˆ‚L/âˆ‚y_pred = (y_pred - y_true)/[y_pred(1-y_pred)]

**2. æ¢¯åº¦ä¸‹é™ç®—æ³•è¯¦è§£**

**æ•°å­¦åŸºç¡€**ï¼š
æ¢¯åº¦ä¸‹é™åŸºäºæ³°å‹’å±•å¼€çš„ä¸€é˜¶è¿‘ä¼¼ï¼š
```
f(Î¸ + Î”Î¸) â‰ˆ f(Î¸) + âˆ‡f(Î¸)áµ€Î”Î¸
```
å½“Î”Î¸ = -Î±âˆ‡f(Î¸)æ—¶ï¼Œf(Î¸ + Î”Î¸) < f(Î¸)ï¼ˆå½“Î±è¶³å¤Ÿå°æ—¶ï¼‰

**æ‰¹é‡æ¢¯åº¦ä¸‹é™(BGD)**ï¼š
```
Î¸ = Î¸ - Î±âˆ‡J(Î¸)
å…¶ä¸­âˆ‡J(Î¸) = (1/n)Î£âˆ‡L(Î¸, x_i, y_i)
```
- **ä¼˜ç‚¹**ï¼šæ¢¯åº¦ä¼°è®¡å‡†ç¡®ï¼Œæ”¶æ•›ç¨³å®š
- **ç¼ºç‚¹**ï¼šè®¡ç®—é‡å¤§ï¼Œå†…å­˜æ¶ˆè€—å¤§
- **é€‚ç”¨**ï¼šå°æ•°æ®é›†æˆ–ç²¾ç¡®ä¼˜åŒ–

**éšæœºæ¢¯åº¦ä¸‹é™(SGD)**ï¼š
```
Î¸ = Î¸ - Î±âˆ‡L(Î¸, x_i, y_i)
```
- **ä¼˜ç‚¹**ï¼šè®¡ç®—é‡å°ï¼Œå¯ä»¥é€ƒç¦»å±€éƒ¨æœ€ä¼˜
- **ç¼ºç‚¹**ï¼šæ¢¯åº¦ä¼°è®¡ä¸å‡†ç¡®ï¼Œæ”¶æ•›ä¸ç¨³å®š
- **é€‚ç”¨**ï¼šå¤§æ•°æ®é›†æˆ–åœ¨çº¿å­¦ä¹ 

**å°æ‰¹é‡æ¢¯åº¦ä¸‹é™(Mini-batch SGD)**ï¼š
```
Î¸ = Î¸ - Î±âˆ‡J(Î¸)
å…¶ä¸­âˆ‡J(Î¸) = (1/batch_size)Î£âˆ‡L(Î¸, x_i, y_i)
```
- **ä¼˜ç‚¹**ï¼šå¹³è¡¡äº†è®¡ç®—æ•ˆç‡å’Œæ¢¯åº¦å‡†ç¡®æ€§
- **ç¼ºç‚¹**ï¼šéœ€è¦è°ƒå‚batch_size
- **é€‚ç”¨**ï¼šæ·±åº¦å­¦ä¹ çš„ä¸»æµæ–¹æ³•

**3. å­¦ä¹ ç‡é€‰æ‹©ä¸è°ƒåº¦**

**å­¦ä¹ ç‡çš„é‡è¦æ€§**ï¼š
å­¦ä¹ ç‡æ˜¯æ¢¯åº¦ä¸‹é™æœ€é‡è¦çš„è¶…å‚æ•°ï¼Œç›´æ¥å½±å“ï¼š
- æ”¶æ•›é€Ÿåº¦
- æ”¶æ•›ç¨³å®šæ€§
- æœ€ç»ˆè§£çš„è´¨é‡

**å­¦ä¹ ç‡é€‰æ‹©åŸåˆ™**ï¼š
- **å¤ªå¤§**ï¼šå¯èƒ½å¯¼è‡´å‘æ•£æˆ–éœ‡è¡
- **å¤ªå°**ï¼šæ”¶æ•›å¤ªæ…¢ï¼Œå¯èƒ½é™·å…¥å±€éƒ¨æœ€ä¼˜
- **åˆé€‚**ï¼šåœ¨ä¿è¯æ”¶æ•›çš„å‰æä¸‹å°½å¯èƒ½å¤§

**4. æŸå¤±å‡½æ•°çš„æ•°å­¦æ€§è´¨**

**å‡¸æ€§ä¸ä¼˜åŒ–**ï¼š
- **å‡¸æŸå¤±å‡½æ•°**ï¼šå…¨å±€æœ€å°å€¼å”¯ä¸€ï¼Œæ¢¯åº¦ä¸‹é™æ”¶æ•›åˆ°å…¨å±€æœ€ä¼˜
- **éå‡¸æŸå¤±å‡½æ•°**ï¼šå¤šä¸ªå±€éƒ¨æœ€å°å€¼ï¼Œæ”¶æ•›åˆ°å±€éƒ¨æœ€ä¼˜
- **ç¥ç»ç½‘ç»œæŸå¤±**ï¼šé€šå¸¸æ˜¯éå‡¸çš„ï¼Œä½†å±€éƒ¨æœ€ä¼˜é€šå¸¸è¶³å¤Ÿå¥½

**å‡¸å‡½æ•°çš„å…·ä½“ä¾‹å­**ï¼š
- **äºŒæ¬¡å‡½æ•°**ï¼šf(x) = xÂ²ï¼Œè¿™æ˜¯æœ€ç®€å•çš„å‡¸å‡½æ•°
  - å®é™…æ„ä¹‰ï¼šåœ¨æœºå™¨å­¦ä¹ ä¸­ï¼ŒMSEæŸå¤±å‡½æ•°å°±æ˜¯å‡¸çš„
  - å‡ ä½•æ„ä¹‰ï¼šå‡½æ•°å›¾åƒå‘ä¸Šå¼¯æ›²ï¼Œä»»æ„ä¸¤ç‚¹è¿çº¿åœ¨å‡½æ•°å›¾åƒä¸Šæ–¹
  - ä¼˜åŒ–æ„ä¹‰ï¼šåªæœ‰ä¸€ä¸ªå…¨å±€æœ€å°å€¼ï¼Œæ¢¯åº¦ä¸‹é™ä¸€å®šèƒ½æ‰¾åˆ°

- **çº¿æ€§å‡½æ•°**ï¼šf(x) = ax + bï¼Œæ—¢æ˜¯å‡¸å‡½æ•°ä¹Ÿæ˜¯å‡¹å‡½æ•°
  - å®é™…æ„ä¹‰ï¼šçº¿æ€§å›å½’çš„æŸå¤±å‡½æ•°
  - å‡ ä½•æ„ä¹‰ï¼šç›´çº¿ï¼Œä»»æ„ä¸¤ç‚¹è¿çº¿ä¸å‡½æ•°å›¾åƒé‡åˆ
  - ä¼˜åŒ–æ„ä¹‰ï¼šå¦‚æœaâ‰ 0ï¼Œæœ‰å”¯ä¸€è§£ï¼›å¦‚æœa=0ï¼Œæ‰€æœ‰ç‚¹éƒ½æ˜¯æœ€ä¼˜è§£

- **æŒ‡æ•°å‡½æ•°**ï¼šf(x) = e^xï¼Œä¸¥æ ¼å‡¸å‡½æ•°
  - å®é™…æ„ä¹‰ï¼šåœ¨æ¦‚ç‡æ¨¡å‹ä¸­ç»å¸¸å‡ºç°
  - å‡ ä½•æ„ä¹‰ï¼šå‡½æ•°å›¾åƒå¿«é€Ÿä¸Šå‡ï¼Œå‘ä¸Šå¼¯æ›²
  - ä¼˜åŒ–æ„ä¹‰ï¼šæ— ä¸‹ç•Œï¼Œä½†æœ‰å”¯ä¸€çš„æœ€å°å€¼ç‚¹ï¼ˆåœ¨xâ†’-âˆæ—¶ï¼‰

**å‡¹å‡½æ•°çš„å…·ä½“ä¾‹å­**ï¼š
- **å¯¹æ•°å‡½æ•°**ï¼šf(x) = log(x)ï¼Œä¸¥æ ¼å‡¹å‡½æ•°
  - å®é™…æ„ä¹‰ï¼šä¿¡æ¯è®ºä¸­çš„ç†µå‡½æ•°ï¼Œäº¤å‰ç†µæŸå¤±
  - å‡ ä½•æ„ä¹‰ï¼šå‡½æ•°å›¾åƒå‘ä¸‹å¼¯æ›²ï¼Œä»»æ„ä¸¤ç‚¹è¿çº¿åœ¨å‡½æ•°å›¾åƒä¸‹æ–¹
  - ä¼˜åŒ–æ„ä¹‰ï¼šåœ¨x>0æ—¶æœ‰æœ€å¤§å€¼ï¼Œåœ¨xâ†’0+æ—¶è¶‹å‘-âˆ

- **è´ŸäºŒæ¬¡å‡½æ•°**ï¼šf(x) = -xÂ²ï¼Œä¸¥æ ¼å‡¹å‡½æ•°
  - å®é™…æ„ä¹‰ï¼šæŸäº›æœ€å¤§åŒ–é—®é¢˜
  - å‡ ä½•æ„ä¹‰ï¼šå‡½æ•°å›¾åƒå‘ä¸‹å¼¯æ›²ï¼ŒæŠ›ç‰©çº¿å¼€å£å‘ä¸‹
  - ä¼˜åŒ–æ„ä¹‰ï¼šæœ‰å”¯ä¸€çš„æœ€å¤§å€¼ç‚¹x=0

**å‡¸å‡½æ•°åœ¨ä¼˜åŒ–ä¸­çš„æ„ä¹‰**ï¼š
- **å…¨å±€æœ€ä¼˜ä¿è¯**ï¼šå‡¸å‡½æ•°åªæœ‰ä¸€ä¸ªå…¨å±€æœ€å°å€¼
- **æ”¶æ•›æ€§**ï¼šæ¢¯åº¦ä¸‹é™åœ¨å‡¸å‡½æ•°ä¸Šä¸€å®šæ”¶æ•›åˆ°å…¨å±€æœ€ä¼˜
- **æ”¶æ•›é€Ÿåº¦**ï¼šå‡¸å‡½æ•°é€šå¸¸æœ‰è¾ƒå¥½çš„æ”¶æ•›é€Ÿåº¦
- **å®é™…åº”ç”¨**ï¼šçº¿æ€§å›å½’ã€æ”¯æŒå‘é‡æœºã€é€»è¾‘å›å½’ç­‰

**éå‡¸å‡½æ•°åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„æ„ä¹‰**ï¼š
- **å±€éƒ¨æœ€ä¼˜**ï¼šå¯èƒ½æœ‰å¤šä¸ªå±€éƒ¨æœ€å°å€¼
- **éç‚¹é—®é¢˜**ï¼šåœ¨é«˜ç»´ç©ºé—´ä¸­ï¼Œéç‚¹æ¯”å±€éƒ¨æœ€å°å€¼æ›´å¸¸è§
- **å®é™…è¡¨ç°**ï¼šè™½ç„¶ç†è®ºå¤æ‚ï¼Œä½†å®è·µä¸­å±€éƒ¨æœ€ä¼˜é€šå¸¸è¶³å¤Ÿå¥½
- **é€ƒç¦»ç­–ç•¥**ï¼šä½¿ç”¨åŠ¨é‡ã€éšæœºæ€§ç­‰æ–¹æ³•é€ƒç¦»å±€éƒ¨æœ€ä¼˜

**æ¢¯åº¦æ€§è´¨**ï¼š
- **æ¢¯åº¦å¤§å°**ï¼šå½±å“å‚æ•°æ›´æ–°æ­¥é•¿
- **æ¢¯åº¦æ–¹å‘**ï¼šæŒ‡å‘æŸå¤±å‡½æ•°ä¸‹é™æœ€å¿«çš„æ–¹å‘
- **æ¢¯åº¦æ¶ˆå¤±**ï¼šæ¢¯åº¦æ¥è¿‘é›¶ï¼Œå‚æ•°æ›´æ–°ç¼“æ…¢
- **æ¢¯åº¦çˆ†ç‚¸**ï¼šæ¢¯åº¦è¿‡å¤§ï¼Œå‚æ•°æ›´æ–°ä¸ç¨³å®š

**5. æŸå¤±å‡½æ•°é€‰æ‹©ç­–ç•¥**

**å›å½’é—®é¢˜**ï¼š
- **MSE**ï¼šæ ‡å‡†é€‰æ‹©ï¼Œå¯¹å¼‚å¸¸å€¼æ•æ„Ÿ
- **MAE**ï¼šå¯¹å¼‚å¸¸å€¼é²æ£’ï¼Œä½†æ¢¯åº¦ä¸è¿ç»­
- **HuberæŸå¤±**ï¼šç»“åˆMSEå’ŒMAEçš„ä¼˜ç‚¹
- **å¯¹æ•°æŸå¤±**ï¼šé€‚ç”¨äºç›¸å¯¹è¯¯å·®é‡è¦çš„åœºæ™¯

**æŸå¤±å‡½æ•°çš„å…·ä½“ä¾‹å­å’Œå®é™…æ„ä¹‰**ï¼š

**MSEæŸå¤±å‡½æ•°çš„å®é™…åº”ç”¨**ï¼š
- **æˆ¿ä»·é¢„æµ‹**ï¼šé¢„æµ‹æˆ¿å±‹ä»·æ ¼
  - è¾“å…¥ï¼šæˆ¿å±‹é¢ç§¯ã€ä½ç½®ã€æˆ¿é¾„ç­‰ç‰¹å¾
  - é¢„æµ‹ï¼šæˆ¿ä»·ï¼ˆè¿ç»­å€¼ï¼‰
  - æŸå¤±ï¼šMSE = (é¢„æµ‹ä»·æ ¼ - å®é™…ä»·æ ¼)Â²
  - ç‰¹ç‚¹ï¼šå¯¹å¼‚å¸¸å€¼æ•æ„Ÿï¼Œå¤§è¯¯å·®è¢«å¹³æ–¹æ”¾å¤§
  - ä¾‹å­ï¼šé¢„æµ‹100ä¸‡ï¼Œå®é™…110ä¸‡ï¼ŒæŸå¤±100ï¼›é¢„æµ‹100ä¸‡ï¼Œå®é™…200ä¸‡ï¼ŒæŸå¤±10000

**MAEæŸå¤±å‡½æ•°çš„å®é™…åº”ç”¨**ï¼š
- **æ¸©åº¦é¢„æµ‹**ï¼šé¢„æµ‹æ˜å¤©æ¸©åº¦
  - è¾“å…¥ï¼šä»Šå¤©æ¸©åº¦ã€æ¹¿åº¦ã€é£é€Ÿç­‰
  - é¢„æµ‹ï¼šæ˜å¤©æ¸©åº¦
  - æŸå¤±ï¼šMAE = |é¢„æµ‹æ¸©åº¦ - å®é™…æ¸©åº¦|
  - ç‰¹ç‚¹ï¼šå¯¹å¼‚å¸¸å€¼ä¸æ•æ„Ÿï¼Œçº¿æ€§æƒ©ç½š
  - ä¾‹å­ï¼šé¢„æµ‹20Â°Cï¼Œå®é™…25Â°Cï¼ŒæŸå¤±5ï¼›é¢„æµ‹20Â°Cï¼Œå®é™…30Â°Cï¼ŒæŸå¤±10

**äº¤å‰ç†µæŸå¤±å‡½æ•°çš„å®é™…åº”ç”¨**ï¼š
- **åƒåœ¾é‚®ä»¶åˆ†ç±»**ï¼šåˆ¤æ–­é‚®ä»¶æ˜¯å¦ä¸ºåƒåœ¾é‚®ä»¶
  - è¾“å…¥ï¼šé‚®ä»¶å†…å®¹ç‰¹å¾
  - é¢„æµ‹ï¼šåƒåœ¾é‚®ä»¶æ¦‚ç‡ï¼ˆ0-1ï¼‰
  - å®é™…ï¼š1ï¼ˆåƒåœ¾é‚®ä»¶ï¼‰æˆ–0ï¼ˆæ­£å¸¸é‚®ä»¶ï¼‰
  - æŸå¤±ï¼šäº¤å‰ç†µ = -[y*log(p) + (1-y)*log(1-p)]
  - ç‰¹ç‚¹ï¼šä¸æœ€å¤§ä¼¼ç„¶ä¼°è®¡ç­‰ä»·ï¼Œæ•°å€¼ç¨³å®š

**åˆ†ç±»é—®é¢˜**ï¼š
- **äº¤å‰ç†µ**ï¼šæ ‡å‡†é€‰æ‹©ï¼Œä¸æœ€å¤§ä¼¼ç„¶ä¼°è®¡ç­‰ä»·
- **Focal Loss**ï¼šå¤„ç†ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜
- **Hinge Loss**ï¼šæ”¯æŒå‘é‡æœºçš„æŸå¤±å‡½æ•°
- **KLæ•£åº¦**ï¼šè¡¡é‡æ¦‚ç‡åˆ†å¸ƒå·®å¼‚

**Focal Lossçš„å®é™…åº”ç”¨**ï¼š
- **åŒ»å­¦è¯Šæ–­**ï¼šæ£€æµ‹ç½•è§ç–¾ç—…
  - é—®é¢˜ï¼šå¥åº·æ ·æœ¬å 99%ï¼Œç–¾ç—…æ ·æœ¬å 1%
  - ä¼ ç»Ÿæ–¹æ³•ï¼šæ¨¡å‹å€¾å‘äºé¢„æµ‹"å¥åº·"
  - Focal Lossï¼šå¯¹éš¾åˆ†ç±»æ ·æœ¬ç»™äºˆæ›´é«˜æƒé‡
  - æ•ˆæœï¼šæé«˜å¯¹ç½•è§ç–¾ç—…çš„æ£€æµ‹ç‡

**æŸå¤±å‡½æ•°åœ¨ä¸åŒåœºæ™¯ä¸­çš„é€‰æ‹©**ï¼š
- **é‡‘èé¢„æµ‹**ï¼šä½¿ç”¨HuberæŸå¤±ï¼Œå¹³è¡¡MSEå’ŒMAE
  - åŸå› ï¼šé‡‘èæ•°æ®å¸¸æœ‰å¼‚å¸¸å€¼ï¼Œä½†éœ€è¦å¹³æ»‘æ¢¯åº¦
- **å›¾åƒç”Ÿæˆ**ï¼šä½¿ç”¨æ„ŸçŸ¥æŸå¤±ï¼Œç»“åˆå†…å®¹æŸå¤±å’Œé£æ ¼æŸå¤±
  - åŸå› ï¼šä¸ä»…å…³æ³¨åƒç´ å·®å¼‚ï¼Œè¿˜å…³æ³¨è¯­ä¹‰ç›¸ä¼¼æ€§
- **æ¨èç³»ç»Ÿ**ï¼šä½¿ç”¨æ’åºæŸå¤±ï¼Œå¦‚BPRæŸå¤±
  - åŸå› ï¼šå…³æ³¨ç›¸å¯¹æ’åºï¼Œè€Œéç»å¯¹è¯„åˆ†

**6. æ¢¯åº¦ä¸‹é™çš„æ”¶æ•›ç†è®º**

**æ”¶æ•›æ¡ä»¶**ï¼š
- **Lipschitzè¿ç»­æ€§**ï¼šæ¢¯åº¦å‡½æ•°æ»¡è¶³Lipschitzæ¡ä»¶
- **å­¦ä¹ ç‡é™åˆ¶**ï¼šÎ± < 2/Lï¼Œå…¶ä¸­Læ˜¯Lipschitzå¸¸æ•°
- **å‡¸æ€§**ï¼šå¯¹äºå‡¸å‡½æ•°ï¼Œæ”¶æ•›åˆ°å…¨å±€æœ€ä¼˜

**æ”¶æ•›é€Ÿåº¦**ï¼š
- **çº¿æ€§æ”¶æ•›**ï¼šè¯¯å·®ä»¥å‡ ä½•çº§æ•°å‡å°
- **æ¬¡çº¿æ€§æ”¶æ•›**ï¼šè¯¯å·®å‡å°é€Ÿåº¦é€æ¸å˜æ…¢
- **è¶…çº¿æ€§æ”¶æ•›**ï¼šè¯¯å·®å‡å°é€Ÿåº¦é€æ¸åŠ å¿«

**7. å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥**

**å›ºå®šå­¦ä¹ ç‡**ï¼š
- **ä¼˜ç‚¹**ï¼šç®€å•ï¼Œè®¡ç®—å¼€é”€å°
- **ç¼ºç‚¹**ï¼šéœ€è¦æ‰‹åŠ¨è°ƒå‚ï¼Œå¯èƒ½ä¸æ˜¯æœ€ä¼˜
- **é€‚ç”¨**ï¼šç®€å•é—®é¢˜æˆ–åˆæ­¥å®éªŒ

**å­¦ä¹ ç‡çš„å…·ä½“ä¾‹å­å’Œå®é™…æ„ä¹‰**ï¼š

**å­¦ä¹ ç‡è¿‡å¤§çš„é—®é¢˜**ï¼š
- **å®é™…ä¾‹å­**ï¼šå°±åƒå¼€è½¦æ—¶æ²¹é—¨è¸©å¾—å¤ªçŒ›
  - ç°è±¡ï¼šè½¦å­å†²è¿‡å¤´ï¼Œé”™è¿‡ç›®æ ‡
  - ä¼˜åŒ–ä¸­ï¼šå‚æ•°æ›´æ–°è¿‡å¤§ï¼Œè·³è¿‡æœ€ä¼˜è§£
  - è¡¨ç°ï¼šæŸå¤±å‡½æ•°éœ‡è¡ï¼Œä¸æ”¶æ•›
  - ä¾‹å­ï¼šå­¦ä¹ ç‡0.1æ—¶ï¼Œå‚æ•°ä»1è·³åˆ°-0.8ï¼Œé”™è¿‡æœ€ä¼˜å€¼0

**å­¦ä¹ ç‡è¿‡å°çš„é—®é¢˜**ï¼š
- **å®é™…ä¾‹å­**ï¼šå°±åƒå¼€è½¦æ—¶æ²¹é—¨è¸©å¾—å¤ªè½»
  - ç°è±¡ï¼šè½¦å­å‰è¿›å¤ªæ…¢ï¼Œåˆ°è¾¾ç›®æ ‡éœ€è¦å¾ˆé•¿æ—¶é—´
  - ä¼˜åŒ–ä¸­ï¼šå‚æ•°æ›´æ–°å¤ªå°ï¼Œæ”¶æ•›å¤ªæ…¢
  - è¡¨ç°ï¼šæŸå¤±å‡½æ•°ä¸‹é™å¾ˆæ…¢
  - ä¾‹å­ï¼šå­¦ä¹ ç‡0.0001æ—¶ï¼Œéœ€è¦10000æ­¥æ‰èƒ½åˆ°è¾¾æœ€ä¼˜å€¼

**å­¦ä¹ ç‡è¡°å‡çš„å®é™…æ„ä¹‰**ï¼š
- **ç±»æ¯”**ï¼šå°±åƒå­¦ä¹ æ–°æŠ€èƒ½çš„è¿‡ç¨‹
  - åˆæœŸï¼šå¤§æ­¥å‰è¿›ï¼Œå¿«é€ŸæŒæ¡åŸºæœ¬æ¦‚å¿µ
  - ä¸­æœŸï¼šä¸­ç­‰æ­¥é•¿ï¼Œç»†åŒ–æŠ€èƒ½
  - åæœŸï¼šå°æ­¥è°ƒæ•´ï¼Œç²¾ç¡®ä¼˜åŒ–
- **æ•°å­¦ä¾‹å­**ï¼š
  - åˆå§‹å­¦ä¹ ç‡ï¼š0.1
  - è¡°å‡ç­–ç•¥ï¼šæ¯100æ­¥è¡°å‡ä¸ºåŸæ¥çš„0.9å€
  - ç»“æœï¼šå­¦ä¹ ç‡é€æ¸å‡å°ï¼Œé¿å…åæœŸéœ‡è¡

**è‡ªé€‚åº”å­¦ä¹ ç‡çš„å…·ä½“ä¾‹å­**ï¼š

**AdaGradçš„å®é™…åº”ç”¨**ï¼š
- **ç¨€ç–æ•°æ®é—®é¢˜**ï¼šæŸäº›ç‰¹å¾å¾ˆå°‘å‡ºç°
  - ä¾‹å­ï¼šåœ¨æ–‡æœ¬åˆ†ç±»ä¸­ï¼Œ"ç½•è§è¯æ±‡"å¾ˆå°‘å‡ºç°
  - é—®é¢˜ï¼šè¿™äº›ç‰¹å¾çš„å­¦ä¹ ç‡åº”è¯¥æ›´å¤§
  - AdaGradï¼šè‡ªåŠ¨ä¸ºç½•è§ç‰¹å¾åˆ†é…æ›´å¤§çš„å­¦ä¹ ç‡
  - æ•ˆæœï¼šæé«˜å¯¹ç¨€ç–ç‰¹å¾çš„å­¦ä¹ æ•ˆç‡

**RMSpropçš„å®é™…åº”ç”¨**ï¼š
- **éå¹³ç¨³ç›®æ ‡å‡½æ•°**ï¼šæŸå¤±å‡½æ•°å½¢çŠ¶å˜åŒ–
  - ä¾‹å­ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¸åŒå‚æ•°çš„é‡è¦æ€§å‘ç”Ÿå˜åŒ–
  - é—®é¢˜ï¼šå›ºå®šå­¦ä¹ ç‡æ— æ³•é€‚åº”è¿™ç§å˜åŒ–
  - RMSpropï¼šä½¿ç”¨ç§»åŠ¨å¹³å‡é€‚åº”å˜åŒ–
  - æ•ˆæœï¼šåœ¨å˜åŒ–çš„æŸå¤±å‡½æ•°ä¸Šè¡¨ç°æ›´å¥½

**Adamçš„å®é™…åº”ç”¨**ï¼š
- **æ·±åº¦å­¦ä¹ æ ‡å‡†**ï¼šç»“åˆåŠ¨é‡å’Œè‡ªé€‚åº”
  - ä¼˜åŠ¿ï¼šæ—¢æœ‰åŠ¨é‡å¸®åŠ©é€ƒç¦»å±€éƒ¨æœ€ä¼˜ï¼Œåˆæœ‰è‡ªé€‚åº”å­¦ä¹ ç‡
  - é€‚ç”¨ï¼šå¤§å¤šæ•°æ·±åº¦å­¦ä¹ ä»»åŠ¡
  - è¶…å‚æ•°ï¼šÎ²â‚=0.9, Î²â‚‚=0.999, Îµ=1e-8
  - æ•ˆæœï¼šé€šå¸¸æ¯”æ‰‹åŠ¨è°ƒå‚çš„SGDè¡¨ç°æ›´å¥½

**å­¦ä¹ ç‡é€‰æ‹©çš„å®é™…æŒ‡å¯¼**ï¼š

**æ ¹æ®é—®é¢˜ç‰¹æ€§é€‰æ‹©**ï¼š
- **å‡¸ä¼˜åŒ–é—®é¢˜**ï¼šå¯ä»¥ä½¿ç”¨è¾ƒå¤§çš„å­¦ä¹ ç‡
  - ä¾‹å­ï¼šçº¿æ€§å›å½’ã€é€»è¾‘å›å½’
  - åŸå› ï¼šæœ‰ç†è®ºä¿è¯ï¼Œä¸ä¼šå‘æ•£
- **éå‡¸ä¼˜åŒ–é—®é¢˜**ï¼šä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡
  - ä¾‹å­ï¼šæ·±åº¦ç¥ç»ç½‘ç»œ
  - åŸå› ï¼šé¿å…éœ‡è¡ï¼Œç¨³å®šæ”¶æ•›

**æ ¹æ®æ•°æ®ç‰¹æ€§é€‰æ‹©**ï¼š
- **å¤§æ•°æ®é›†**ï¼šå¯ä»¥ä½¿ç”¨è¾ƒå¤§çš„å­¦ä¹ ç‡
  - åŸå› ï¼šæ¢¯åº¦ä¼°è®¡æ›´å‡†ç¡®
- **å°æ•°æ®é›†**ï¼šä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡
  - åŸå› ï¼šæ¢¯åº¦å™ªå£°å¤§ï¼Œéœ€è¦ç¨³å®š

**æ ¹æ®æ¨¡å‹ç‰¹æ€§é€‰æ‹©**ï¼š
- **æµ…å±‚ç½‘ç»œ**ï¼šå¯ä»¥ä½¿ç”¨è¾ƒå¤§çš„å­¦ä¹ ç‡
  - åŸå› ï¼šæ¢¯åº¦ä¼ æ’­ç¨³å®š
- **æ·±å±‚ç½‘ç»œ**ï¼šä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡
  - åŸå› ï¼šé¿å…æ¢¯åº¦çˆ†ç‚¸æˆ–æ¶ˆå¤±

**å­¦ä¹ ç‡è¡°å‡**ï¼š
```
Î±(t) = Î±â‚€ / (1 + decay_rate Ã— t)
æˆ–
Î±(t) = Î±â‚€ Ã— decay_rate^t
```
- **ä¼˜ç‚¹**ï¼šè‡ªåŠ¨è°ƒæ•´ï¼Œé€šå¸¸æ¯”å›ºå®šå­¦ä¹ ç‡å¥½
- **ç¼ºç‚¹**ï¼šéœ€è¦é€‰æ‹©è¡°å‡ç­–ç•¥å’Œå‚æ•°
- **é€‚ç”¨**ï¼šå¤§å¤šæ•°æ·±åº¦å­¦ä¹ ä»»åŠ¡

**è‡ªé€‚åº”å­¦ä¹ ç‡**ï¼š
- **AdaGrad**ï¼šæ ¹æ®å†å²æ¢¯åº¦è°ƒæ•´å­¦ä¹ ç‡
- **RMSprop**ï¼šä½¿ç”¨ç§»åŠ¨å¹³å‡è°ƒæ•´å­¦ä¹ ç‡
- **Adam**ï¼šç»“åˆåŠ¨é‡å’Œè‡ªé€‚åº”å­¦ä¹ ç‡

**8. ä¼˜åŒ–ç®—æ³•çš„ç†è®ºåŸºç¡€**

**ä¸€é˜¶ä¼˜åŒ–æ–¹æ³•**ï¼š
- **æ¢¯åº¦ä¸‹é™**ï¼šä½¿ç”¨ä¸€é˜¶å¯¼æ•°ä¿¡æ¯
- **ç‰›é¡¿æ³•**ï¼šä½¿ç”¨äºŒé˜¶å¯¼æ•°ä¿¡æ¯ï¼Œæ”¶æ•›æ›´å¿«ä½†è®¡ç®—å¤æ‚
- **æ‹Ÿç‰›é¡¿æ³•**ï¼šè¿‘ä¼¼ç‰›é¡¿æ³•ï¼Œå¹³è¡¡æ”¶æ•›é€Ÿåº¦å’Œè®¡ç®—å¤æ‚åº¦

**éšæœºä¼˜åŒ–**ï¼š
- **éšæœºæ€§æ¥æº**ï¼šæ•°æ®é‡‡æ ·ã€æ¢¯åº¦å™ªå£°
- **æ–¹å·®-åå·®æƒè¡¡**ï¼šå°æ‰¹é‡å‡å°‘æ–¹å·®ä½†å¢åŠ è®¡ç®—é‡
- **æ”¶æ•›ä¿è¯**ï¼šåœ¨æœŸæœ›æ„ä¹‰ä¸‹æ”¶æ•›

**9. æŸå¤±å‡½æ•°çš„æ•°å€¼ç¨³å®šæ€§**

**æ•°å€¼æº¢å‡º**ï¼š
- **åŸå› **ï¼šæŒ‡æ•°å‡½æ•°æˆ–å¯¹æ•°å‡½æ•°è¾“å…¥è¿‡å¤§
- **å½±å“**ï¼šè®¡ç®—é”™è¯¯æˆ–ç¨‹åºå´©æºƒ
- **è§£å†³**ï¼šä½¿ç”¨æ•°å€¼ç¨³å®šçš„å®ç°

**æ•°å€¼ä¸‹æº¢**ï¼š
- **åŸå› **ï¼šæ•°å€¼å¤ªå°ï¼Œç²¾åº¦ä¸è¶³
- **å½±å“**ï¼šæ¢¯åº¦æ¥è¿‘é›¶ï¼Œè®­ç»ƒåœæ»
- **è§£å†³**ï¼šä½¿ç”¨å¯¹æ•°ç©ºé—´è®¡ç®—

**10. ä¼˜åŒ–ç®—æ³•çš„é€‰æ‹©æŒ‡å—**

**é—®é¢˜è§„æ¨¡**ï¼š
- **å°è§„æ¨¡**ï¼šæ‰¹é‡æ¢¯åº¦ä¸‹é™æˆ–ç‰›é¡¿æ³•
- **ä¸­ç­‰è§„æ¨¡**ï¼šå°æ‰¹é‡æ¢¯åº¦ä¸‹é™
- **å¤§è§„æ¨¡**ï¼šéšæœºæ¢¯åº¦ä¸‹é™æˆ–åœ¨çº¿å­¦ä¹ 

**é—®é¢˜ç‰¹æ€§**ï¼š
- **å‡¸ä¼˜åŒ–**ï¼šæ¢¯åº¦ä¸‹é™ã€ç‰›é¡¿æ³•
- **éå‡¸ä¼˜åŒ–**ï¼šéšæœºæ¢¯åº¦ä¸‹é™ã€åŠ¨é‡æ–¹æ³•
- **çº¦æŸä¼˜åŒ–**ï¼šæŠ•å½±æ¢¯åº¦ä¸‹é™ã€æ‹‰æ ¼æœ—æ—¥æ–¹æ³•

**è®¡ç®—èµ„æº**ï¼š
- **å†…å­˜å—é™**ï¼šéšæœºæ¢¯åº¦ä¸‹é™
- **è®¡ç®—å—é™**ï¼šç®€å•æ¢¯åº¦ä¸‹é™
- **é€šä¿¡å—é™**ï¼šæœ¬åœ°ä¼˜åŒ–æ–¹æ³•

**11. æŸå¤±å‡½æ•°çš„ç”Ÿç‰©å­¦æ„ä¹‰**

**æ„ŸçŸ¥å­¦ä¹ **ï¼š
- **é”™è¯¯ä¿¡å·**ï¼šå¤§è„‘é€šè¿‡é”™è¯¯ä¿¡å·è°ƒæ•´è¿æ¥å¼ºåº¦
- **å¼ºåŒ–å­¦ä¹ **ï¼šå¥–åŠ±ä¿¡å·æŒ‡å¯¼å­¦ä¹ è¿‡ç¨‹
- **æ— ç›‘ç£å­¦ä¹ **ï¼šé€šè¿‡é‡æ„è¯¯å·®å­¦ä¹ è¡¨ç¤º

**ç¥ç»ç½‘ç»œæ¨¡æ‹Ÿ**ï¼š
- **æŸå¤±å‡½æ•°**ï¼šæ¨¡æ‹Ÿé”™è¯¯ä¿¡å·
- **æ¢¯åº¦ä¸‹é™**ï¼šæ¨¡æ‹Ÿçªè§¦å¯å¡‘æ€§
- **å­¦ä¹ ç‡**ï¼šæ¨¡æ‹Ÿå­¦ä¹ é€Ÿåº¦

**12. ä¼˜åŒ–ç®—æ³•çš„å†å²å‘å±•**

**æ—©æœŸå‘å±•**ï¼š
- **1940å¹´ä»£**ï¼šæ¢¯åº¦ä¸‹é™æ–¹æ³•æå‡º
- **1960å¹´ä»£**ï¼šéšæœºæ¢¯åº¦ä¸‹é™å¼•å…¥
- **1970å¹´ä»£**ï¼šæ‹Ÿç‰›é¡¿æ³•å‘å±•

**ç°ä»£å‘å±•**ï¼š
- **1980å¹´ä»£**ï¼šåŠ¨é‡æ–¹æ³•å¼•å…¥
- **1990å¹´ä»£**ï¼šè‡ªé€‚åº”æ–¹æ³•å‘å±•
- **2000å¹´ä»£**ï¼šæ·±åº¦å­¦ä¹ ä¼˜åŒ–ç®—æ³•

**æœ€æ–°è¶‹åŠ¿**ï¼š
- **è‡ªé€‚åº”ä¼˜åŒ–**ï¼šAdamã€AdaBeliefç­‰
- **äºŒé˜¶æ–¹æ³•**ï¼šK-FACã€Shampooç­‰
- **åˆ†å¸ƒå¼ä¼˜åŒ–**ï¼šå¼‚æ­¥SGDã€æ¨¡å‹å¹¶è¡Œç­‰

**å›ºå®šå­¦ä¹ ç‡**ï¼š
```
Î± = constant
```
- **ä¼˜ç‚¹**ï¼šç®€å•ç›´æ¥
- **ç¼ºç‚¹**ï¼šå¯èƒ½ä¸æ˜¯æœ€ä¼˜é€‰æ‹©
- **é€‚ç”¨**ï¼šç®€å•é—®é¢˜æˆ–è°ƒè¯•é˜¶æ®µ

**å­¦ä¹ ç‡è¡°å‡**ï¼š
```
Î± = Î±â‚€ / (1 + decay * epoch)
æˆ–
Î± = Î±â‚€ * decay^epoch
```
- **åŸç†**ï¼šéšç€è®­ç»ƒè¿›è¡Œé€æ¸å‡å°å­¦ä¹ ç‡
- **ä¼˜ç‚¹**ï¼šæ—©æœŸå¿«é€Ÿæ”¶æ•›ï¼ŒåæœŸç²¾ç»†è°ƒä¼˜
- **ç¼ºç‚¹**ï¼šéœ€è¦è°ƒå‚decay

**è‡ªé€‚åº”å­¦ä¹ ç‡**ï¼š
- **AdaGrad**ï¼šæ ¹æ®å†å²æ¢¯åº¦è°ƒæ•´å­¦ä¹ ç‡
- **RMSprop**ï¼šä½¿ç”¨ç§»åŠ¨å¹³å‡çš„æ¢¯åº¦å¹³æ–¹æ ¹
- **Adam**ï¼šç»“åˆåŠ¨é‡å’Œè‡ªé€‚åº”å­¦ä¹ ç‡

**4. ä¼˜åŒ–ç®—æ³•çš„æ”¶æ•›æ€§åˆ†æ**

**å‡¸ä¼˜åŒ–ç†è®º**ï¼š
å¯¹äºå‡¸å‡½æ•°ï¼Œæ¢¯åº¦ä¸‹é™å…·æœ‰ä»¥ä¸‹æ€§è´¨ï¼š
- **æ”¶æ•›æ€§**ï¼šåœ¨åˆé€‚çš„å­¦ä¹ ç‡ä¸‹æ”¶æ•›åˆ°å…¨å±€æœ€ä¼˜
- **æ”¶æ•›é€Ÿåº¦**ï¼šçº¿æ€§æ”¶æ•›ï¼ˆLipschitzè¿ç»­ï¼‰
- **è¯¯å·®ç•Œ**ï¼šO(1/T)çš„æ”¶æ•›é€Ÿåº¦

**éå‡¸ä¼˜åŒ–**ï¼š
å¯¹äºéå‡¸å‡½æ•°ï¼ˆå¦‚ç¥ç»ç½‘ç»œï¼‰ï¼š
- **å±€éƒ¨æœ€ä¼˜**ï¼šå¯èƒ½æ”¶æ•›åˆ°å±€éƒ¨æœ€ä¼˜
- **éç‚¹é—®é¢˜**ï¼šåœ¨é«˜ç»´ç©ºé—´ä¸­éç‚¹æ¯”å±€éƒ¨æœ€ä¼˜æ›´å¸¸è§
- **æ”¶æ•›ä¿è¯**ï¼šç†è®ºä¸Šéš¾ä»¥ä¿è¯å…¨å±€æ”¶æ•›

**æ”¶æ•›åˆ¤æ®**ï¼š
- **æ¢¯åº¦èŒƒæ•°**ï¼š||âˆ‡J(Î¸)|| < Îµ
- **å‡½æ•°å€¼å˜åŒ–**ï¼š|J(Î¸_t) - J(Î¸_{t-1})| < Îµ
- **å‚æ•°å˜åŒ–**ï¼š||Î¸_t - Î¸_{t-1}|| < Îµ

**5. æ•°å€¼ç¨³å®šæ€§ä¸æ¢¯åº¦è£å‰ª**

**æ¢¯åº¦çˆ†ç‚¸é—®é¢˜**ï¼š
å½“æ¢¯åº¦å€¼è¿‡å¤§æ—¶ï¼Œå‚æ•°æ›´æ–°å¯èƒ½è¿‡å¤§ï¼Œå¯¼è‡´è®­ç»ƒä¸ç¨³å®šã€‚

**æ¢¯åº¦è£å‰ª**ï¼š
```c
// æ¢¯åº¦è£å‰ªå®ç°
void gradient_clipping(float* gradients, int size, float threshold) {
    float norm = 0;
    for (int i = 0; i < size; i++) {
        norm += gradients[i] * gradients[i];
    }
    norm = sqrtf(norm);
    
    if (norm > threshold) {
        float scale = threshold / norm;
        for (int i = 0; i < size; i++) {
            gradients[i] *= scale;
        }
    }
}
```

**æ¢¯åº¦æ¶ˆå¤±é—®é¢˜**ï¼š
å½“æ¢¯åº¦å€¼è¿‡å°æ—¶ï¼Œå‚æ•°æ›´æ–°ç¼“æ…¢ï¼Œå¯¼è‡´è®­ç»ƒåœæ»ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼š
- **æƒé‡åˆå§‹åŒ–**ï¼šä½¿ç”¨Xavieræˆ–Heåˆå§‹åŒ–
- **æ¿€æ´»å‡½æ•°**ï¼šä½¿ç”¨ReLUç­‰ç¼“è§£æ¢¯åº¦æ¶ˆå¤±
- **æ®‹å·®è¿æ¥**ï¼šåœ¨æ·±å±‚ç½‘ç»œä¸­å¼•å…¥è·³è·ƒè¿æ¥

**6. ä¼˜åŒ–ç®—æ³•çš„å®é™…è€ƒè™‘**

**æ‰¹é‡å¤§å°é€‰æ‹©**ï¼š
- **å°æ‰¹é‡**ï¼šæ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œæ›´å¤šçš„å™ªå£°
- **å¤§æ‰¹é‡**ï¼šæ›´ç¨³å®šçš„æ¢¯åº¦ï¼Œæ›´å¿«çš„æ”¶æ•›
- **ç»éªŒæ³•åˆ™**ï¼š32-256æ˜¯å¸¸ç”¨çš„æ‰¹é‡å¤§å°

**å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥**ï¼š
```c
// å­¦ä¹ ç‡è°ƒåº¦å™¨
typedef struct {
    float initial_lr;
    float current_lr;
    float decay_rate;
    int decay_steps;
    int step_count;
} LearningRateScheduler;

float get_learning_rate(LearningRateScheduler* scheduler) {
    if (scheduler->step_count % scheduler->decay_steps == 0) {
        scheduler->current_lr *= scheduler->decay_rate;
    }
    return scheduler->current_lr;
}
```

**æ—©åœæœºåˆ¶**ï¼š
```c
// æ—©åœæ£€æŸ¥
bool check_early_stopping(float* val_losses, int patience, int current_epoch) {
    if (current_epoch < patience) return false;
    
    for (int i = current_epoch - patience; i < current_epoch; i++) {
        if (val_losses[i] <= val_losses[i+1]) {
            return false;
        }
    }
    return true;
}
```

**7. æŸå¤±å‡½æ•°çš„æ•°å€¼ç¨³å®šæ€§**

**äº¤å‰ç†µçš„æ•°å€¼ç¨³å®šæ€§**ï¼š
```c
// æ•°å€¼ç¨³å®šçš„äº¤å‰ç†µ
float cross_entropy_stable(float y_pred, float y_true) {
    // é¿å…log(0)
    float epsilon = 1e-15;
    y_pred = fmaxf(epsilon, fminf(1.0f - epsilon, y_pred));
    return -y_true * logf(y_pred) - (1.0f - y_true) * logf(1.0f - y_pred);
}
```

**MSEçš„æ•°å€¼ç¨³å®šæ€§**ï¼š
```c
// é¿å…æº¢å‡ºçš„MSE
float mse_stable(float* predictions, float* targets, int size) {
    float sum = 0;
    for (int i = 0; i < size; i++) {
        float diff = predictions[i] - targets[i];
        // é¿å…å¤§æ•°ç›¸ä¹˜å¯¼è‡´æº¢å‡º
        if (fabsf(diff) < 1e6) {
            sum += diff * diff;
        }
    }
    return sum / size;
}
```

**8. æŸå¤±å‡½æ•°çš„Cè¯­è¨€å®ç°**

**å®Œæ•´æŸå¤±å‡½æ•°åº“**ï¼š
```c
// æŸå¤±å‡½æ•°ç±»å‹å®šä¹‰
typedef float (*loss_func_t)(float*, float*, int);

// å‡æ–¹è¯¯å·®æŸå¤±
float mse_loss(float* predictions, float* targets, int size) {
    float loss = 0;
    for (int i = 0; i < size; i++) {
        float diff = predictions[i] - targets[i];
        loss += diff * diff;
    }
    return loss / size;
}

// MSEæ¢¯åº¦
void mse_gradient(float* predictions, float* targets, float* gradients, int size) {
    for (int i = 0; i < size; i++) {
        gradients[i] = 2.0f * (predictions[i] - targets[i]) / size;
    }
}

// å¹³å‡ç»å¯¹è¯¯å·®æŸå¤±
float mae_loss(float* predictions, float* targets, int size) {
    float loss = 0;
    for (int i = 0; i < size; i++) {
        loss += fabsf(predictions[i] - targets[i]);
    }
    return loss / size;
}

// MAEæ¢¯åº¦
void mae_gradient(float* predictions, float* targets, float* gradients, int size) {
    for (int i = 0; i < size; i++) {
        float diff = predictions[i] - targets[i];
        gradients[i] = (diff > 0 ? 1.0f : -1.0f) / size;
    }
}

// äºŒå…ƒäº¤å‰ç†µæŸå¤±
float binary_cross_entropy_loss(float* predictions, float* targets, int size) {
    float loss = 0;
    float epsilon = 1e-15;
    
    for (int i = 0; i < size; i++) {
        float pred = fmaxf(epsilon, fminf(1.0f - epsilon, predictions[i]));
        loss += -targets[i] * logf(pred) - (1.0f - targets[i]) * logf(1.0f - pred);
    }
    return loss / size;
}

// äºŒå…ƒäº¤å‰ç†µæ¢¯åº¦
void binary_cross_entropy_gradient(float* predictions, float* targets, float* gradients, int size) {
    float epsilon = 1e-15;
    
    for (int i = 0; i < size; i++) {
        float pred = fmaxf(epsilon, fminf(1.0f - epsilon, predictions[i]));
        gradients[i] = (pred - targets[i]) / (pred * (1.0f - pred)) / size;
    }
}

// æŸå¤±å‡½æ•°æŸ¥æ‰¾è¡¨
loss_func_t loss_functions[] = {
    mse_loss,
    mae_loss,
    binary_cross_entropy_loss
};
```

**9. æ¢¯åº¦ä¸‹é™ä¼˜åŒ–å™¨å®ç°**

**åŸºç¡€æ¢¯åº¦ä¸‹é™**ï¼š
```c
// åŸºç¡€æ¢¯åº¦ä¸‹é™ä¼˜åŒ–å™¨
typedef struct {
    float learning_rate;
    int num_parameters;
} SGD;

SGD* sgd_create(float learning_rate, int num_parameters) {
    SGD* sgd = malloc(sizeof(SGD));
    sgd->learning_rate = learning_rate;
    sgd->num_parameters = num_parameters;
    return sgd;
}

void sgd_update(SGD* sgd, float* parameters, float* gradients) {
    for (int i = 0; i < sgd->num_parameters; i++) {
        parameters[i] -= sgd->learning_rate * gradients[i];
    }
}

void sgd_free(SGD* sgd) {
    free(sgd);
}
```

**åŠ¨é‡ä¼˜åŒ–å™¨**ï¼š
```c
// åŠ¨é‡ä¼˜åŒ–å™¨
typedef struct {
    float learning_rate;
    float momentum;
    float* velocity;
    int num_parameters;
} MomentumSGD;

MomentumSGD* momentum_sgd_create(float learning_rate, float momentum, int num_parameters) {
    MomentumSGD* optimizer = malloc(sizeof(MomentumSGD));
    optimizer->learning_rate = learning_rate;
    optimizer->momentum = momentum;
    optimizer->num_parameters = num_parameters;
    optimizer->velocity = calloc(num_parameters, sizeof(float));
    return optimizer;
}

void momentum_sgd_update(MomentumSGD* optimizer, float* parameters, float* gradients) {
    for (int i = 0; i < optimizer->num_parameters; i++) {
        optimizer->velocity[i] = optimizer->momentum * optimizer->velocity[i] + 
                                 optimizer->learning_rate * gradients[i];
        parameters[i] -= optimizer->velocity[i];
    }
}

void momentum_sgd_free(MomentumSGD* optimizer) {
    free(optimizer->velocity);
    free(optimizer);
}
```

**Adamä¼˜åŒ–å™¨**ï¼š
```c
// Adamä¼˜åŒ–å™¨
typedef struct {
    float learning_rate;
    float beta1;
    float beta2;
    float epsilon;
    int t;
    float* m;  // ä¸€é˜¶çŸ©ä¼°è®¡
    float* v;  // äºŒé˜¶çŸ©ä¼°è®¡
    int num_parameters;
} Adam;

Adam* adam_create(float learning_rate, int num_parameters) {
    Adam* adam = malloc(sizeof(Adam));
    adam->learning_rate = learning_rate;
    adam->beta1 = 0.9f;
    adam->beta2 = 0.999f;
    adam->epsilon = 1e-8f;
    adam->t = 0;
    adam->num_parameters = num_parameters;
    adam->m = calloc(num_parameters, sizeof(float));
    adam->v = calloc(num_parameters, sizeof(float));
    return adam;
}

void adam_update(Adam* adam, float* parameters, float* gradients) {
    adam->t++;
    
    for (int i = 0; i < adam->num_parameters; i++) {
        // æ›´æ–°ä¸€é˜¶çŸ©ä¼°è®¡
        adam->m[i] = adam->beta1 * adam->m[i] + (1.0f - adam->beta1) * gradients[i];
        
        // æ›´æ–°äºŒé˜¶çŸ©ä¼°è®¡
        adam->v[i] = adam->beta2 * adam->v[i] + (1.0f - adam->beta2) * gradients[i] * gradients[i];
        
        // åå·®ä¿®æ­£
        float m_hat = adam->m[i] / (1.0f - powf(adam->beta1, adam->t));
        float v_hat = adam->v[i] / (1.0f - powf(adam->beta2, adam->t));
        
        // å‚æ•°æ›´æ–°
        parameters[i] -= adam->learning_rate * m_hat / (sqrtf(v_hat) + adam->epsilon);
    }
}

void adam_free(Adam* adam) {
    free(adam->m);
    free(adam->v);
    free(adam);
}
```
- **å°æ‰¹é‡**ï¼šæ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œæ›´å¤šçš„å™ªå£°
- **å¤§æ‰¹é‡**ï¼šæ›´ç¨³å®šçš„æ¢¯åº¦ï¼Œæ›´å¿«çš„æ”¶æ•›
- **ç»éªŒæ³•åˆ™**ï¼š32-256æ˜¯å¸¸ç”¨çš„æ‰¹é‡å¤§å°

**å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥**ï¼š
```c
// å­¦ä¹ ç‡è°ƒåº¦å™¨
typedef struct {
    float initial_lr;
    float current_lr;
    float decay_rate;
    int decay_steps;
    int step_count;
} LearningRateScheduler;

float get_learning_rate(LearningRateScheduler* scheduler) {
    if (scheduler->step_count % scheduler->decay_steps == 0) {
        scheduler->current_lr *= scheduler->decay_rate;
    }
    return scheduler->current_lr;
}
```

**æ—©åœæœºåˆ¶**ï¼š
```c
// æ—©åœæ£€æŸ¥
bool check_early_stopping(float* val_losses, int patience, int current_epoch) {
    if (current_epoch < patience) return false;
    
    for (int i = current_epoch - patience; i < current_epoch; i++) {
        if (val_losses[i] <= val_losses[i+1]) {
            return false;
        }
    }
    return true;
}
```

**7. æŸå¤±å‡½æ•°çš„æ•°å€¼ç¨³å®šæ€§**

**äº¤å‰ç†µçš„æ•°å€¼ç¨³å®šæ€§**ï¼š
```c
// æ•°å€¼ç¨³å®šçš„äº¤å‰ç†µ
float cross_entropy_stable(float y_pred, float y_true) {
    // é¿å…log(0)
    float epsilon = 1e-15;
    y_pred = fmaxf(epsilon, fminf(1.0f - epsilon, y_pred));
    return -y_true * logf(y_pred) - (1.0f - y_true) * logf(1.0f - y_pred);
}
```

**MSEçš„æ•°å€¼ç¨³å®šæ€§**ï¼š
```c
// é¿å…æº¢å‡ºçš„MSE
float mse_stable(float* predictions, float* targets, int size) {
    float sum = 0;
    for (int i = 0; i < size; i++) {
        float diff = predictions[i] - targets[i];
        // é¿å…å¤§æ•°ç›¸ä¹˜å¯¼è‡´æº¢å‡º
        if (fabsf(diff) < 1e6) {
            sum += diff * diff;
        }
    }
    return sum / size;
}
```

#### å®è·µç»ƒä¹ 

**ç»ƒä¹ 1ï¼šæŸå¤±å‡½æ•°æµ‹è¯•**

```c
// æŸå¤±å‡½æ•°æµ‹è¯•ç¨‹åº
void test_loss_functions() {
    printf("=== æŸå¤±å‡½æ•°æµ‹è¯• ===\n");
    
    const int size = 5;
    float predictions[] = {0.1f, 0.3f, 0.5f, 0.7f, 0.9f};
    float targets[] = {0.0f, 0.2f, 0.5f, 0.8f, 1.0f};
    
    printf("é¢„æµ‹å€¼: [%.1f, %.1f, %.1f, %.1f, %.1f]\n", 
           predictions[0], predictions[1], predictions[2], predictions[3], predictions[4]);
    printf("ç›®æ ‡å€¼: [%.1f, %.1f, %.1f, %.1f, %.1f]\n", 
           targets[0], targets[1], targets[2], targets[3], targets[4]);
    
    // æµ‹è¯•MSEæŸå¤±
    float mse = mse_loss(predictions, targets, size);
    printf("MSEæŸå¤±: %.4f\n", mse);
    
    // æµ‹è¯•MAEæŸå¤±
    float mae = mae_loss(predictions, targets, size);
    printf("MAEæŸå¤±: %.4f\n", mae);
    
    // æµ‹è¯•äºŒå…ƒäº¤å‰ç†µæŸå¤±
    float bce = binary_cross_entropy_loss(predictions, targets, size);
    printf("äºŒå…ƒäº¤å‰ç†µæŸå¤±: %.4f\n", bce);
    
    printf("\n");
}

// æ¢¯åº¦æµ‹è¯•
void test_gradients() {
    printf("=== æ¢¯åº¦æµ‹è¯• ===\n");
    
    const int size = 3;
    float predictions[] = {0.2f, 0.5f, 0.8f};
    float targets[] = {0.0f, 0.5f, 1.0f};
    float gradients[size];
    
    // æµ‹è¯•MSEæ¢¯åº¦
    mse_gradient(predictions, targets, gradients, size);
    printf("MSEæ¢¯åº¦: [%.4f, %.4f, %.4f]\n", gradients[0], gradients[1], gradients[2]);
    
    // æµ‹è¯•MAEæ¢¯åº¦
    mae_gradient(predictions, targets, gradients, size);
    printf("MAEæ¢¯åº¦: [%.4f, %.4f, %.4f]\n", gradients[0], gradients[1], gradients[2]);
    
    // æµ‹è¯•äºŒå…ƒäº¤å‰ç†µæ¢¯åº¦
    binary_cross_entropy_gradient(predictions, targets, gradients, size);
    printf("BCEæ¢¯åº¦: [%.4f, %.4f, %.4f]\n", gradients[0], gradients[1], gradients[2]);
    
    printf("\n");
}
```

**ç»ƒä¹ 2ï¼šä¼˜åŒ–å™¨æ€§èƒ½å¯¹æ¯”**

```c
// ä¼˜åŒ–å™¨æ€§èƒ½å¯¹æ¯”æµ‹è¯•
void test_optimizers() {
    printf("=== ä¼˜åŒ–å™¨æ€§èƒ½å¯¹æ¯” ===\n");
    
    const int num_parameters = 100;
    const int num_iterations = 1000;
    
    // åˆ›å»ºæµ‹è¯•å‚æ•°å’Œæ¢¯åº¦
    float* parameters = malloc(num_parameters * sizeof(float));
    float* gradients = malloc(num_parameters * sizeof(float));
    
    // åˆå§‹åŒ–å‚æ•°
    for (int i = 0; i < num_parameters; i++) {
        parameters[i] = ((float)rand() / RAND_MAX - 0.5f) * 2.0f;
    }
    
    // æµ‹è¯•SGD
    SGD* sgd = sgd_create(0.01f, num_parameters);
    clock_t start = clock();
    for (int iter = 0; iter < num_iterations; iter++) {
        // ç”Ÿæˆæ¨¡æ‹Ÿæ¢¯åº¦
        for (int i = 0; i < num_parameters; i++) {
            gradients[i] = ((float)rand() / RAND_MAX - 0.5f) * 2.0f;
        }
        sgd_update(sgd, parameters, gradients);
    }
    clock_t end = clock();
    double sgd_time = ((double)(end - start)) / CLOCKS_PER_SEC;
    printf("SGD: %.4f ç§’ (%d æ¬¡è¿­ä»£)\n", sgd_time, num_iterations);
    
    // æµ‹è¯•åŠ¨é‡SGD
    MomentumSGD* momentum_sgd = momentum_sgd_create(0.01f, 0.9f, num_parameters);
    start = clock();
    for (int iter = 0; iter < num_iterations; iter++) {
        for (int i = 0; i < num_parameters; i++) {
            gradients[i] = ((float)rand() / RAND_MAX - 0.5f) * 2.0f;
        }
        momentum_sgd_update(momentum_sgd, parameters, gradients);
    }
    end = clock();
    double momentum_time = ((double)(end - start)) / CLOCKS_PER_SEC;
    printf("åŠ¨é‡SGD: %.4f ç§’ (%d æ¬¡è¿­ä»£)\n", momentum_time, num_iterations);
    
    // æµ‹è¯•Adam
    Adam* adam = adam_create(0.01f, num_parameters);
    start = clock();
    for (int iter = 0; iter < num_iterations; iter++) {
        for (int i = 0; i < num_parameters; i++) {
            gradients[i] = ((float)rand() / RAND_MAX - 0.5f) * 2.0f;
        }
        adam_update(adam, parameters, gradients);
    }
    end = clock();
    double adam_time = ((double)(end - start)) / CLOCKS_PER_SEC;
    printf("Adam: %.4f ç§’ (%d æ¬¡è¿­ä»£)\n", adam_time, num_iterations);
    
    // æ¸…ç†
    sgd_free(sgd);
    momentum_sgd_free(momentum_sgd);
    adam_free(adam);
    free(parameters);
    free(gradients);
    
    printf("\n");
}
```

**ç»ƒä¹ 3ï¼šå­¦ä¹ ç‡è°ƒåº¦æµ‹è¯•**

```c
// å­¦ä¹ ç‡è°ƒåº¦æµ‹è¯•
void test_learning_rate_scheduling() {
    printf("=== å­¦ä¹ ç‡è°ƒåº¦æµ‹è¯• ===\n");
    
    LearningRateScheduler scheduler;
    scheduler.initial_lr = 0.1f;
    scheduler.current_lr = 0.1f;
    scheduler.decay_rate = 0.9f;
    scheduler.decay_steps = 100;
    scheduler.step_count = 0;
    
    printf("åˆå§‹å­¦ä¹ ç‡: %.3f\n", scheduler.initial_lr);
    printf("è¡°å‡ç‡: %.3f\n", scheduler.decay_rate);
    printf("è¡°å‡æ­¥æ•°: %d\n", scheduler.decay_steps);
    
    printf("\nå­¦ä¹ ç‡å˜åŒ–:\n");
    for (int step = 0; step < 500; step += 50) {
        scheduler.step_count = step;
        float lr = get_learning_rate(&scheduler);
        printf("æ­¥æ•° %d: å­¦ä¹ ç‡ = %.6f\n", step, lr);
    }
    
    printf("\n");
}
```

**ç»ƒä¹ 4ï¼šæ¢¯åº¦è£å‰ªæµ‹è¯•**

```c
// æ¢¯åº¦è£å‰ªæµ‹è¯•
void test_gradient_clipping() {
    printf("=== æ¢¯åº¦è£å‰ªæµ‹è¯• ===\n");
    
    const int size = 10;
    float gradients[size];
    float threshold = 1.0f;
    
    // ç”Ÿæˆå¤§æ¢¯åº¦
    for (int i = 0; i < size; i++) {
        gradients[i] = ((float)rand() / RAND_MAX - 0.5f) * 10.0f;
    }
    
    printf("åŸå§‹æ¢¯åº¦:\n");
    for (int i = 0; i < size; i++) {
        printf("%.3f ", gradients[i]);
    }
    printf("\n");
    
    // è®¡ç®—åŸå§‹æ¢¯åº¦èŒƒæ•°
    float original_norm = 0;
    for (int i = 0; i < size; i++) {
        original_norm += gradients[i] * gradients[i];
    }
    original_norm = sqrtf(original_norm);
    printf("åŸå§‹æ¢¯åº¦èŒƒæ•°: %.3f\n", original_norm);
    
    // åº”ç”¨æ¢¯åº¦è£å‰ª
    gradient_clipping(gradients, size, threshold);
    
    printf("è£å‰ªåæ¢¯åº¦:\n");
    for (int i = 0; i < size; i++) {
        printf("%.3f ", gradients[i]);
    }
    printf("\n");
    
    // è®¡ç®—è£å‰ªåæ¢¯åº¦èŒƒæ•°
    float clipped_norm = 0;
    for (int i = 0; i < size; i++) {
        clipped_norm += gradients[i] * gradients[i];
    }
    clipped_norm = sqrtf(clipped_norm);
    printf("è£å‰ªåæ¢¯åº¦èŒƒæ•°: %.3f\n", clipped_norm);
    
    printf("\n");
}
```

**ç»ƒä¹ 5ï¼šæ•°å€¼ç¨³å®šæ€§æµ‹è¯•**

```c
// æ•°å€¼ç¨³å®šæ€§æµ‹è¯•
void test_numerical_stability() {
    printf("=== æ•°å€¼ç¨³å®šæ€§æµ‹è¯• ===\n");
    
    const int size = 5;
    float predictions[] = {0.0f, 0.0001f, 0.5f, 0.9999f, 1.0f};
    float targets[] = {0.0f, 0.0f, 0.5f, 1.0f, 1.0f};
    
    printf("æµ‹è¯•æ•°å€¼ç¨³å®šæ€§:\n");
    printf("é¢„æµ‹å€¼: [%.4f, %.4f, %.4f, %.4f, %.4f]\n", 
           predictions[0], predictions[1], predictions[2], predictions[3], predictions[4]);
    printf("ç›®æ ‡å€¼: [%.4f, %.4f, %.4f, %.4f, %.4f]\n", 
           targets[0], targets[1], targets[2], targets[3], targets[4]);
    
    // æµ‹è¯•æ ‡å‡†äº¤å‰ç†µ
    float bce_standard = 0;
    for (int i = 0; i < size; i++) {
        bce_standard += -targets[i] * logf(predictions[i]) - 
                        (1.0f - targets[i]) * logf(1.0f - predictions[i]);
    }
    bce_standard /= size;
    
    // æµ‹è¯•ç¨³å®šç‰ˆæœ¬
    float bce_stable = binary_cross_entropy_loss(predictions, targets, size);
    
    printf("æ ‡å‡†BCE: %.6f\n", bce_standard);
    printf("ç¨³å®šBCE: %.6f\n", bce_stable);
    
    // æµ‹è¯•MSEç¨³å®šæ€§
    float predictions_large[] = {1e6f, 1e6f, 1e6f, 1e6f, 1e6f};
    float targets_large[] = {1e6f + 1e3f, 1e6f + 1e3f, 1e6f + 1e3f, 1e6f + 1e3f, 1e6f + 1e3f};
    
    float mse_standard = mse_loss(predictions_large, targets_large, size);
    float mse_stable_result = mse_stable(predictions_large, targets_large, size);
    
    printf("\nå¤§æ•°å€¼MSEæµ‹è¯•:\n");
    printf("æ ‡å‡†MSE: %.6f\n", mse_standard);
    printf("ç¨³å®šMSE: %.6f\n", mse_stable_result);
    
    printf("\n");
}
```

**ç»ƒä¹ 6ï¼šå®Œæ•´ä¼˜åŒ–è®­ç»ƒå¾ªç¯**

```c
// ç®€å•è®­ç»ƒå¾ªç¯
void simple_training_loop() {
    printf("=== ç®€å•è®­ç»ƒå¾ªç¯ ===\n");
    
    // åˆ›å»ºç®€å•çš„çº¿æ€§å›å½’é—®é¢˜
    const int num_samples = 100;
    const int input_size = 2;
    const int output_size = 1;
    
    // ç”Ÿæˆè®­ç»ƒæ•°æ®: y = 2*x1 + 3*x2 + 1
    float* inputs = malloc(num_samples * input_size * sizeof(float));
    float* targets = malloc(num_samples * output_size * sizeof(float));
    
    for (int i = 0; i < num_samples; i++) {
        float x1 = ((float)rand() / RAND_MAX - 0.5f) * 2.0f;
        float x2 = ((float)rand() / RAND_MAX - 0.5f) * 2.0f;
        
        inputs[i * input_size] = x1;
        inputs[i * input_size + 1] = x2;
        targets[i] = 2.0f * x1 + 3.0f * x2 + 1.0f;
    }
    
    // åˆ›å»ºç½‘ç»œ
    int layer_sizes[] = {input_size, 5, output_size};
    NeuralNetwork* nn = nn_create(layer_sizes, 3, 0.01f);
    
    // åˆ›å»ºä¼˜åŒ–å™¨
    Adam* optimizer = adam_create(0.01f, 100);  // å‡è®¾æœ‰100ä¸ªå‚æ•°
    
    // è®­ç»ƒå¾ªç¯
    const int epochs = 100;
    const int batch_size = 10;
    
    printf("å¼€å§‹è®­ç»ƒ...\n");
    for (int epoch = 0; epoch < epochs; epoch++) {
        float total_loss = 0;
        
        for (int batch = 0; batch < num_samples; batch += batch_size) {
            int current_batch_size = (batch + batch_size <= num_samples) ? 
                                   batch_size : (num_samples - batch);
            
            // å‰å‘ä¼ æ’­
            float* batch_predictions = malloc(current_batch_size * output_size * sizeof(float));
            for (int i = 0; i < current_batch_size; i++) {
                nn_forward(nn, &inputs[(batch + i) * input_size], &batch_predictions[i]);
            }
            
            // è®¡ç®—æŸå¤±
            float loss = mse_loss(batch_predictions, &targets[batch], current_batch_size);
            total_loss += loss;
            
            // è®¡ç®—æ¢¯åº¦ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
            float* gradients = malloc(100 * sizeof(float));  // å‡è®¾100ä¸ªå‚æ•°
            for (int i = 0; i < 100; i++) {
                gradients[i] = ((float)rand() / RAND_MAX - 0.5f) * 0.1f;
            }
            
            // æ›´æ–°å‚æ•°
            adam_update(optimizer, (float*)nn, gradients);
            
            free(batch_predictions);
            free(gradients);
        }
        
        if (epoch % 20 == 0) {
            printf("Epoch %d, Average Loss: %.6f\n", epoch, total_loss / (num_samples / batch_size));
        }
    }
    
    printf("è®­ç»ƒå®Œæˆï¼\n");
    
    // æµ‹è¯•è®­ç»ƒç»“æœ
    float test_input[] = {1.0f, 1.0f};
    float test_output[1];
    nn_forward(nn, test_input, test_output);
    float expected = 2.0f * test_input[0] + 3.0f * test_input[1] + 1.0f;
    
    printf("æµ‹è¯•è¾“å…¥: [%.1f, %.1f]\n", test_input[0], test_input[1]);
    printf("é¢„æµ‹è¾“å‡º: %.4f\n", test_output[0]);
    printf("æœŸæœ›è¾“å‡º: %.4f\n", expected);
    printf("è¯¯å·®: %.4f\n", fabsf(test_output[0] - expected));
    
    // æ¸…ç†
    adam_free(optimizer);
    nn_free(nn);
    free(inputs);
    free(targets);
    
    printf("\n");
}
```

**ç»ƒä¹ 7ï¼šä¸»æµ‹è¯•ç¨‹åº**

```c
// ä¸»æµ‹è¯•ç¨‹åº
int main() {
    printf("æŸå¤±å‡½æ•°ä¸ä¼˜åŒ–åŸºç¡€æµ‹è¯•ç¨‹åº\n");
    printf("========================\n\n");
    
    // è®¾ç½®éšæœºç§å­
    srand(time(NULL));
    
    // è¿è¡Œæ‰€æœ‰æµ‹è¯•
    test_loss_functions();
    test_gradients();
    test_optimizers();
    test_learning_rate_scheduling();
    test_gradient_clipping();
    test_numerical_stability();
    simple_training_loop();
    
    printf("æ‰€æœ‰æµ‹è¯•å®Œæˆï¼\n");
    return 0;
}
```

---

## ğŸ§® é˜¶æ®µäºŒï¼šç®—æ³•åŸç†ä¸æ¨å¯¼ï¼ˆ3å‘¨ï¼‰

### ç¬¬4å‘¨ï¼šåå‘ä¼ æ’­ç®—æ³•åŸç†

#### å­¦ä¹ å†…å®¹
- **é“¾å¼æ³•åˆ™åº”ç”¨**
- **æ¢¯åº¦è®¡ç®—æ¨å¯¼**
- **è¯¯å·®åå‘ä¼ æ’­**

#### ç†è®ºçŸ¥è¯†

**1. é“¾å¼æ³•åˆ™ä¸åå‘ä¼ æ’­åŸºç¡€**

**é“¾å¼æ³•åˆ™å›é¡¾**ï¼š
å¯¹äºå¤åˆå‡½æ•°f(g(x))ï¼Œå…¶å¯¼æ•°ä¸ºï¼š
```
df/dx = df/dg * dg/dx
```

åœ¨ç¥ç»ç½‘ç»œä¸­ï¼ŒæŸå¤±å‡½æ•°Læ˜¯ç½‘ç»œå‚æ•°Î¸çš„å¤åˆå‡½æ•°ï¼š
```
L = L(aâ½á´¸â¾(Î¸))
```

**åå‘ä¼ æ’­çš„æ ¸å¿ƒæ€æƒ³**ï¼š
- åˆ©ç”¨é“¾å¼æ³•åˆ™è®¡ç®—æŸå¤±å‡½æ•°å¯¹æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦
- ä»è¾“å‡ºå±‚å¼€å§‹ï¼Œé€å±‚å‘åè®¡ç®—æ¢¯åº¦
- é¿å…é‡å¤è®¡ç®—ï¼Œæé«˜æ•ˆç‡

**2. åå‘ä¼ æ’­ç®—æ³•è¯¦è§£**

**æ•°å­¦ç¬¦å·å®šä¹‰**ï¼š
- aâ½Ë¡â¾ï¼šç¬¬lå±‚çš„æ¿€æ´»å€¼
- zâ½Ë¡â¾ï¼šç¬¬lå±‚çš„çº¿æ€§ç»„åˆ
- Wâ½Ë¡â¾ï¼šç¬¬lå±‚çš„æƒé‡çŸ©é˜µ
- bâ½Ë¡â¾ï¼šç¬¬lå±‚çš„åç½®å‘é‡
- Î´â½Ë¡â¾ï¼šç¬¬lå±‚çš„è¯¯å·®é¡¹

**å‰å‘ä¼ æ’­å›é¡¾**ï¼š
```
zâ½Ë¡â¾ = Wâ½Ë¡â¾aâ½Ë¡â»Â¹â¾ + bâ½Ë¡â¾
aâ½Ë¡â¾ = Ïƒ(zâ½Ë¡â¾)
```

**åå‘ä¼ æ’­ç®—æ³•**ï¼š

**æ­¥éª¤1ï¼šè®¡ç®—è¾“å‡ºå±‚è¯¯å·®**
```
Î´â½á´¸â¾ = âˆ‚L/âˆ‚aâ½á´¸â¾ âŠ™ Ïƒ'(zâ½á´¸â¾)
```
å…¶ä¸­âŠ™è¡¨ç¤ºé€å…ƒç´ ç›¸ä¹˜ï¼ˆHadamardç§¯ï¼‰

**æ­¥éª¤2ï¼šè®¡ç®—éšè—å±‚è¯¯å·®**
```
Î´â½Ë¡â¾ = (Wâ½Ë¡âºÂ¹â¾)áµ€Î´â½Ë¡âºÂ¹â¾ âŠ™ Ïƒ'(zâ½Ë¡â¾)
```

**æ­¥éª¤3ï¼šè®¡ç®—æƒé‡æ¢¯åº¦**
```
âˆ‚L/âˆ‚Wâ½Ë¡â¾ = Î´â½Ë¡â¾(aâ½Ë¡â»Â¹â¾)áµ€
âˆ‚L/âˆ‚bâ½Ë¡â¾ = Î´â½Ë¡â¾
```

**3. é“¾å¼æ³•åˆ™çš„è¯¦ç»†æ¨å¯¼**

**è¾“å‡ºå±‚æ¢¯åº¦æ¨å¯¼**ï¼š
```
âˆ‚L/âˆ‚Wâ½á´¸â¾ = âˆ‚L/âˆ‚aâ½á´¸â¾ * âˆ‚aâ½á´¸â¾/âˆ‚zâ½á´¸â¾ * âˆ‚zâ½á´¸â¾/âˆ‚Wâ½á´¸â¾
           = Î´â½á´¸â¾ * (aâ½á´¸â»Â¹â¾)áµ€
```

**éšè—å±‚æ¢¯åº¦æ¨å¯¼**ï¼š
```
âˆ‚L/âˆ‚Wâ½Ë¡â¾ = âˆ‚L/âˆ‚aâ½Ë¡â¾ * âˆ‚aâ½Ë¡â¾/âˆ‚zâ½Ë¡â¾ * âˆ‚zâ½Ë¡â¾/âˆ‚Wâ½Ë¡â¾
           = Î´â½Ë¡â¾ * (aâ½Ë¡â»Â¹â¾)áµ€
```

**è¯¯å·®ä¼ æ’­æ¨å¯¼**ï¼š
```
Î´â½Ë¡â¾ = âˆ‚L/âˆ‚zâ½Ë¡â¾
       = âˆ‚L/âˆ‚aâ½Ë¡â¾ * âˆ‚aâ½Ë¡â¾/âˆ‚zâ½Ë¡â¾
       = (Wâ½Ë¡âºÂ¹â¾)áµ€Î´â½Ë¡âºÂ¹â¾ * Ïƒ'(zâ½Ë¡â¾)
```

**4. æ¿€æ´»å‡½æ•°å¯¼æ•°çš„è®¡ç®—**

**Sigmoidå¯¼æ•°**ï¼š
```
Ïƒ'(x) = Ïƒ(x)(1-Ïƒ(x))
```

**ReLUå¯¼æ•°**ï¼š
```
ReLU'(x) = 1 if x > 0, else 0
```

**Tanhå¯¼æ•°**ï¼š
```
tanh'(x) = 1 - tanhÂ²(x)
```

**5. åå‘ä¼ æ’­çš„çŸ©é˜µå½¢å¼**

**æ‰¹é‡å¤„ç†**ï¼š
å½“å¤„ç†æ‰¹é‡æ•°æ®æ—¶ï¼Œåå‘ä¼ æ’­ä½¿ç”¨çŸ©é˜µå½¢å¼ï¼š
```
Î´â½Ë¡â¾ = (Wâ½Ë¡âºÂ¹â¾)áµ€Î´â½Ë¡âºÂ¹â¾ âŠ™ Ïƒ'(Zâ½Ë¡â¾)
âˆ‚L/âˆ‚Wâ½Ë¡â¾ = Î´â½Ë¡â¾(Aâ½Ë¡â»Â¹â¾)áµ€
âˆ‚L/âˆ‚bâ½Ë¡â¾ = Î´â½Ë¡â¾1
```
å…¶ä¸­å¤§å†™å­—æ¯è¡¨ç¤ºçŸ©é˜µï¼Œ1è¡¨ç¤ºå…¨1å‘é‡ã€‚

**è®¡ç®—å¤æ‚åº¦**ï¼š
- **æ—¶é—´å¤æ‚åº¦**ï¼šO(Î£nâ½Ë¡â¾nâ½Ë¡â»Â¹â¾)
- **ç©ºé—´å¤æ‚åº¦**ï¼šO(Î£nâ½Ë¡â¾)ï¼Œéœ€è¦å­˜å‚¨å‰å‘ä¼ æ’­çš„ä¸­é—´ç»“æœ

**6. åå‘ä¼ æ’­çš„æ•°å€¼ç¨³å®šæ€§**

**æ¢¯åº¦æ¶ˆå¤±é—®é¢˜**ï¼š
- **åŸå› **ï¼šæ¿€æ´»å‡½æ•°å¯¼æ•°åœ¨é¥±å’ŒåŒºåŸŸæ¥è¿‘0
- **å½±å“**ï¼šæ·±å±‚ç½‘ç»œæ¢¯åº¦æ— æ³•æœ‰æ•ˆä¼ æ’­
- **æ£€æµ‹**ï¼šè§‚å¯Ÿä¸åŒå±‚çš„æ¢¯åº¦å¤§å°
- **è§£å†³**ï¼šä½¿ç”¨ReLUæ¿€æ´»å‡½æ•°ã€æ®‹å·®è¿æ¥ã€æ‰¹å½’ä¸€åŒ–

**æ¢¯åº¦çˆ†ç‚¸é—®é¢˜**ï¼š
- **åŸå› **ï¼šæƒé‡è¿‡å¤§æˆ–å­¦ä¹ ç‡è¿‡é«˜
- **å½±å“**ï¼šæ¢¯åº¦å€¼è¿‡å¤§ï¼Œå‚æ•°æ›´æ–°ä¸ç¨³å®š
- **æ£€æµ‹**ï¼šè§‚å¯Ÿæ¢¯åº¦èŒƒæ•°æ˜¯å¦è¿‡å¤§
- **è§£å†³**ï¼šæ¢¯åº¦è£å‰ªã€æƒé‡æ­£åˆ™åŒ–

**æ•°å€¼æº¢å‡º**ï¼š
- **åŸå› **ï¼šæ¿€æ´»å‡½æ•°è¾“å…¥è¿‡å¤§
- **å½±å“**ï¼šè®¡ç®—é”™è¯¯æˆ–ç¨‹åºå´©æºƒ
- **è§£å†³**ï¼šä½¿ç”¨æ•°å€¼ç¨³å®šçš„æ¿€æ´»å‡½æ•°å®ç°

**7. åå‘ä¼ æ’­çš„ç”Ÿç‰©å­¦æ„ä¹‰**

**èµ«å¸ƒå­¦ä¹ è§„åˆ™**ï¼š
- **åŸç†**ï¼š"ä¸€èµ·æ¿€æ´»çš„ç¥ç»å…ƒè¿æ¥ä¼šå¢å¼º"
- **æ•°å­¦è¡¨ç¤º**ï¼šÎ”w = Î· * pre_activation * post_error
- **åå‘ä¼ æ’­**ï¼šå®ç°äº†èµ«å¸ƒè§„åˆ™çš„æ•°å­¦å½¢å¼

**è¯¯å·®ä¿¡å·ä¼ æ’­**ï¼š
- **ç”Ÿç‰©å­¦**ï¼šå¤§è„‘é€šè¿‡è¯¯å·®ä¿¡å·è°ƒæ•´çªè§¦å¼ºåº¦
- **äººå·¥ç½‘ç»œ**ï¼šåå‘ä¼ æ’­è®¡ç®—è¯¯å·®ä¿¡å·
- **ç›¸ä¼¼æ€§**ï¼šéƒ½åŸºäºè¯¯å·®ä¿¡å·è°ƒæ•´è¿æ¥æƒé‡

**8. åå‘ä¼ æ’­ç®—æ³•çš„å†å²å‘å±•**

**æ—©æœŸå‘å±•**ï¼š
- **1960å¹´ä»£**ï¼šæ„ŸçŸ¥æœºå­¦ä¹ è§„åˆ™
- **1970å¹´ä»£**ï¼šWidrow-Hoffç®—æ³•
- **1980å¹´ä»£**ï¼šåå‘ä¼ æ’­ç®—æ³•æå‡º

**ç°ä»£å‘å±•**ï¼š
- **1990å¹´ä»£**ï¼šè‡ªåŠ¨å¾®åˆ†æŠ€æœ¯å‘å±•
- **2000å¹´ä»£**ï¼šæ·±åº¦å­¦ä¹ æ¡†æ¶å…´èµ·
- **2010å¹´ä»£**ï¼šè‡ªåŠ¨å¾®åˆ†æˆä¸ºæ ‡å‡†

**9. åå‘ä¼ æ’­çš„å˜ä½“ç®—æ³•**

**è‡ªåŠ¨å¾®åˆ†**ï¼š
- **å‰å‘æ¨¡å¼**ï¼šè®¡ç®—é›…å¯æ¯”çŸ©é˜µ
- **åå‘æ¨¡å¼**ï¼šè®¡ç®—æ¢¯åº¦ï¼ˆåå‘ä¼ æ’­ï¼‰
- **æ··åˆæ¨¡å¼**ï¼šç»“åˆä¸¤ç§æ¨¡å¼çš„ä¼˜åŠ¿

**è®¡ç®—å›¾ä¼˜åŒ–**ï¼š
- **å›¾ä¼˜åŒ–**ï¼šä¼˜åŒ–è®¡ç®—å›¾çš„æ‰§è¡Œé¡ºåº
- **å†…å­˜ä¼˜åŒ–**ï¼šå‡å°‘ä¸­é—´ç»“æœçš„å­˜å‚¨
- **å¹¶è¡ŒåŒ–**ï¼šåˆ©ç”¨å¹¶è¡Œè®¡ç®—åŠ é€Ÿ

**10. åå‘ä¼ æ’­çš„æ”¶æ•›ç†è®º**

**æ”¶æ•›æ¡ä»¶**ï¼š
- **æ¢¯åº¦æœ‰ç•Œ**ï¼šæ¢¯åº¦èŒƒæ•°æœ‰ä¸Šç•Œ
- **å­¦ä¹ ç‡åˆé€‚**ï¼šå­¦ä¹ ç‡æ»¡è¶³æ”¶æ•›æ¡ä»¶
- **æŸå¤±å‡½æ•°æ€§è´¨**ï¼šæŸå¤±å‡½æ•°å…·æœ‰è‰¯å¥½çš„æ€§è´¨

**æ”¶æ•›é€Ÿåº¦**ï¼š
- **çº¿æ€§æ”¶æ•›**ï¼šè¯¯å·®ä»¥å‡ ä½•çº§æ•°å‡å°
- **æ¬¡çº¿æ€§æ”¶æ•›**ï¼šè¯¯å·®å‡å°é€Ÿåº¦é€æ¸å˜æ…¢
- **è¶…çº¿æ€§æ”¶æ•›**ï¼šè¯¯å·®å‡å°é€Ÿåº¦é€æ¸åŠ å¿«

**11. åå‘ä¼ æ’­çš„è°ƒè¯•æŠ€å·§**

**æ¢¯åº¦æ£€æŸ¥**ï¼š
- **æ•°å€¼æ¢¯åº¦**ï¼šä½¿ç”¨æœ‰é™å·®åˆ†è®¡ç®—æ¢¯åº¦
- **è§£ææ¢¯åº¦**ï¼šä½¿ç”¨åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦
- **æ¯”è¾ƒ**ï¼šæ¯”è¾ƒä¸¤ç§æ–¹æ³•çš„ç»“æœ

**æ¢¯åº¦ç›‘æ§**ï¼š
- **æ¢¯åº¦èŒƒæ•°**ï¼šç›‘æ§æ¢¯åº¦çš„L2èŒƒæ•°
- **æ¢¯åº¦åˆ†å¸ƒ**ï¼šè§‚å¯Ÿæ¢¯åº¦çš„åˆ†å¸ƒæƒ…å†µ
- **å±‚é—´æ¢¯åº¦**ï¼šæ¯”è¾ƒä¸åŒå±‚çš„æ¢¯åº¦å¤§å°

**12. åå‘ä¼ æ’­çš„ä¼˜åŒ–ç­–ç•¥**

**å†…å­˜ä¼˜åŒ–**ï¼š
- **æ¢¯åº¦ç´¯ç§¯**ï¼šç´¯ç§¯å¤šä¸ªæ‰¹æ¬¡çš„æ¢¯åº¦
- **æ¢¯åº¦å‹ç¼©**ï¼šå‹ç¼©æ¢¯åº¦å‡å°‘é€šä¿¡å¼€é”€
- **æ£€æŸ¥ç‚¹**ï¼šä¿å­˜ä¸­é—´ç»“æœé¿å…é‡å¤è®¡ç®—

**è®¡ç®—ä¼˜åŒ–**ï¼š
- **å¹¶è¡ŒåŒ–**ï¼šåˆ©ç”¨å¤šæ ¸CPUæˆ–GPU
- **å‘é‡åŒ–**ï¼šä½¿ç”¨SIMDæŒ‡ä»¤åŠ é€Ÿè®¡ç®—
- **ç¼“å­˜ä¼˜åŒ–**ï¼šä¼˜åŒ–å†…å­˜è®¿é—®æ¨¡å¼

**13. åå‘ä¼ æ’­çš„ç†è®ºåŸºç¡€**

**å¾®ç§¯åˆ†åŸºç¡€**ï¼š
- **é“¾å¼æ³•åˆ™**ï¼šå¤åˆå‡½æ•°æ±‚å¯¼çš„åŸºæœ¬æ³•åˆ™
- **åå¯¼æ•°**ï¼šå¤šå˜é‡å‡½æ•°çš„å¯¼æ•°
- **æ¢¯åº¦**ï¼šå¤šå˜é‡å‡½æ•°çš„æœ€é€Ÿä¸‹é™æ–¹å‘

**çº¿æ€§ä»£æ•°åŸºç¡€**ï¼š
- **çŸ©é˜µä¹˜æ³•**ï¼šæƒé‡çŸ©é˜µä¸æ¿€æ´»å‘é‡çš„ä¹˜æ³•
- **çŸ©é˜µè½¬ç½®**ï¼šæ¢¯åº¦ä¼ æ’­ä¸­çš„çŸ©é˜µè½¬ç½®
- **Hadamardç§¯**ï¼šé€å…ƒç´ ç›¸ä¹˜

**14. åå‘ä¼ æ’­çš„æ‰©å±•åº”ç”¨**

**å·ç§¯ç¥ç»ç½‘ç»œ**ï¼š
- **å·ç§¯å±‚**ï¼šä½¿ç”¨å·ç§¯è¿ç®—çš„å‰å‘å’Œåå‘ä¼ æ’­
- **æ± åŒ–å±‚**ï¼šæœ€å¤§æ± åŒ–å’Œå¹³å‡æ± åŒ–çš„æ¢¯åº¦è®¡ç®—
- **æ‰¹å½’ä¸€åŒ–**ï¼šå½’ä¸€åŒ–å±‚çš„æ¢¯åº¦è®¡ç®—

**å¾ªç¯ç¥ç»ç½‘ç»œ**ï¼š
- **æ—¶é—´å±•å¼€**ï¼šå°†å¾ªç¯ç½‘ç»œå±•å¼€ä¸ºå‰é¦ˆç½‘ç»œ
- **æ¢¯åº¦æˆªæ–­**ï¼šé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
- **LSTM**ï¼šé•¿çŸ­æœŸè®°å¿†ç½‘ç»œçš„æ¢¯åº¦è®¡ç®—

**15. åå‘ä¼ æ’­çš„ç°ä»£å‘å±•**

**è‡ªåŠ¨å¾®åˆ†æ¡†æ¶**ï¼š
- **TensorFlow**ï¼šGoogleå¼€å‘çš„è‡ªåŠ¨å¾®åˆ†æ¡†æ¶
- **PyTorch**ï¼šFacebookå¼€å‘çš„åŠ¨æ€è®¡ç®—å›¾æ¡†æ¶
- **JAX**ï¼šGoogleå¼€å‘çš„å‡½æ•°å¼è‡ªåŠ¨å¾®åˆ†æ¡†æ¶

**é«˜é˜¶å¯¼æ•°**ï¼š
- **HessiançŸ©é˜µ**ï¼šäºŒé˜¶å¯¼æ•°çš„çŸ©é˜µè¡¨ç¤º
- **è‡ªç„¶æ¢¯åº¦**ï¼šä½¿ç”¨Fisherä¿¡æ¯çŸ©é˜µçš„æ¢¯åº¦
- **äºŒé˜¶ä¼˜åŒ–**ï¼šä½¿ç”¨äºŒé˜¶ä¿¡æ¯çš„ä¼˜åŒ–æ–¹æ³•

**Leaky ReLUå¯¼æ•°**ï¼š
```
LeakyReLU'(x) = 1 if x > 0, else Î±
```

**5. åå‘ä¼ æ’­çš„çŸ©é˜µå½¢å¼**

**æ‰¹é‡å¤„ç†**ï¼š
å¯¹äºæ‰¹é‡å¤§å°ä¸ºmçš„æ•°æ®ï¼š
```
Zâ½Ë¡â¾ = Wâ½Ë¡â¾Aâ½Ë¡â»Â¹â¾ + bâ½Ë¡â¾
Aâ½Ë¡â¾ = Ïƒ(Zâ½Ë¡â¾)
```

å…¶ä¸­ï¼š
- Zâ½Ë¡â¾ âˆˆ â„^(nâ½Ë¡â¾ Ã— m)
- Aâ½Ë¡â¾ âˆˆ â„^(nâ½Ë¡â¾ Ã— m)
- Wâ½Ë¡â¾ âˆˆ â„^(nâ½Ë¡â¾ Ã— nâ½Ë¡â»Â¹â¾)
- bâ½Ë¡â¾ âˆˆ â„^(nâ½Ë¡â¾ Ã— 1)

**æ¢¯åº¦è®¡ç®—**ï¼š
```
âˆ‚L/âˆ‚Wâ½Ë¡â¾ = Î´â½Ë¡â¾(Aâ½Ë¡â»Â¹â¾)áµ€
âˆ‚L/âˆ‚bâ½Ë¡â¾ = Î´â½Ë¡â¾ * 1
```

**6. åå‘ä¼ æ’­çš„æ•°å€¼ç¨³å®šæ€§**

**æ¢¯åº¦çˆ†ç‚¸**ï¼š
å½“æƒé‡è¿‡å¤§æ—¶ï¼Œæ¢¯åº¦å¯èƒ½æŒ‡æ•°å¢é•¿ï¼š
```
Î´â½Ë¡â¾ = (Wâ½Ë¡âºÂ¹â¾)áµ€Î´â½Ë¡âºÂ¹â¾ * Ïƒ'(zâ½Ë¡â¾)
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
- æ¢¯åº¦è£å‰ª
- æƒé‡æ­£åˆ™åŒ–
- åˆé€‚çš„æƒé‡åˆå§‹åŒ–

**æ¢¯åº¦æ¶ˆå¤±**ï¼š
å½“æƒé‡è¿‡å°æ—¶ï¼Œæ¢¯åº¦å¯èƒ½æŒ‡æ•°è¡°å‡ï¼š
```
Î´â½Ë¡â¾ = (Wâ½Ë¡âºÂ¹â¾)áµ€Î´â½Ë¡âºÂ¹â¾ * Ïƒ'(zâ½Ë¡â¾)
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
- ä½¿ç”¨ReLUç­‰æ¿€æ´»å‡½æ•°
- æ®‹å·®è¿æ¥
- æ‰¹å½’ä¸€åŒ–

**7. åå‘ä¼ æ’­ç®—æ³•çš„å®ç°ç»†èŠ‚**

**å†…å­˜ç®¡ç†**ï¼š
```c
// ç¼“å­˜å‰å‘ä¼ æ’­çš„ä¸­é—´ç»“æœ
typedef struct {
    float* activations[MAX_LAYERS];
    float* z_values[MAX_LAYERS];
    int num_layers;
} ForwardCache;

void forward_propagation_with_cache(NeuralNetwork* nn, float* input, ForwardCache* cache) {
    // å­˜å‚¨è¾“å…¥å±‚
    cache->activations[0] = input;
    
    for (int l = 0; l < nn->num_layers; l++) {
        Layer* layer = &nn->layers[l];
        
        // è®¡ç®—zå€¼
        cache->z_values[l] = malloc(layer->output_size * sizeof(float));
        compute_linear_combination(layer, cache->activations[l], cache->z_values[l]);
        
        // è®¡ç®—æ¿€æ´»å€¼
        cache->activations[l+1] = malloc(layer->output_size * sizeof(float));
        apply_activation(layer->activation, cache->z_values[l], cache->activations[l+1]);
    }
}
```

**æ¢¯åº¦è®¡ç®—**ï¼š
```c
// è®¡ç®—è¾“å‡ºå±‚è¯¯å·®
void compute_output_error(NeuralNetwork* nn, float* output, float* target, float* delta) {
    int output_size = nn->layers[nn->num_layers-1].output_size;
    
    for (int i = 0; i < output_size; i++) {
        // æŸå¤±å‡½æ•°å¯¹è¾“å‡ºçš„æ¢¯åº¦
        float loss_gradient = 2 * (output[i] - target[i]);
        
        // æ¿€æ´»å‡½æ•°å¯¼æ•°
        float activation_derivative = compute_activation_derivative(
            nn->layers[nn->num_layers-1].activation_derivative,
            nn->cache.z_values[nn->num_layers-1][i]
        );
        
        delta[i] = loss_gradient * activation_derivative;
    }
}
```

**æƒé‡æ›´æ–°**ï¼š
```c
// æ›´æ–°æƒé‡å’Œåç½®
void update_weights(Layer* layer, float* delta, float learning_rate) {
    int input_size = layer->input_size;
    int output_size = layer->output_size;
    
    // æ›´æ–°æƒé‡
    for (int i = 0; i < output_size; i++) {
        for (int j = 0; j < input_size; j++) {
            float gradient = delta[i] * layer->input_cache[j];
            layer->weights[i * input_size + j] -= learning_rate * gradient;
        }
    }
    
    // æ›´æ–°åç½®
    for (int i = 0; i < output_size; i++) {
        layer->biases[i] -= learning_rate * delta[i];
    }
}
```

**8. åå‘ä¼ æ’­çš„å¤æ‚åº¦åˆ†æ**

**æ—¶é—´å¤æ‚åº¦**ï¼š
- **å‰å‘ä¼ æ’­**ï¼šO(Î£nâ½Ë¡â¾nâ½Ë¡â»Â¹â¾)
- **åå‘ä¼ æ’­**ï¼šO(Î£nâ½Ë¡â¾nâ½Ë¡â»Â¹â¾)
- **æ€»ä½“å¤æ‚åº¦**ï¼šO(Î£nâ½Ë¡â¾nâ½Ë¡â»Â¹â¾)

**ç©ºé—´å¤æ‚åº¦**ï¼š
- **å­˜å‚¨æ¿€æ´»å€¼**ï¼šO(Î£nâ½Ë¡â¾)
- **å­˜å‚¨æ¢¯åº¦**ï¼šO(Î£nâ½Ë¡â¾nâ½Ë¡â»Â¹â¾)
- **æ€»ä½“å¤æ‚åº¦**ï¼šO(Î£nâ½Ë¡â¾nâ½Ë¡â»Â¹â¾)

**9. åå‘ä¼ æ’­çš„éªŒè¯æ–¹æ³•**

**æ¢¯åº¦æ£€æŸ¥**ï¼š
```c
// æ•°å€¼æ¢¯åº¦æ£€æŸ¥
bool gradient_check(NeuralNetwork* nn, float* input, float* target, float epsilon) {
    float* numerical_gradients = malloc(get_total_parameters(nn) * sizeof(float));
    float* analytical_gradients = malloc(get_total_parameters(nn) * sizeof(float));
    
    // è®¡ç®—æ•°å€¼æ¢¯åº¦
    compute_numerical_gradients(nn, input, target, numerical_gradients, epsilon);
    
    // è®¡ç®—è§£ææ¢¯åº¦
    forward_propagation(nn, input);
    backward_propagation(nn, input, target);
    extract_gradients(nn, analytical_gradients);
    
    // æ¯”è¾ƒæ¢¯åº¦
    bool is_correct = compare_gradients(numerical_gradients, analytical_gradients, 1e-7);
    
    free(numerical_gradients);
    free(analytical_gradients);
    
    return is_correct;
}
```

**ç›¸å¯¹è¯¯å·®è®¡ç®—**ï¼š
```c
float relative_error(float* grad1, float* grad2, int size) {
    float sum_diff = 0;
    float sum_grad = 0;
    
    for (int i = 0; i < size; i++) {
        sum_diff += (grad1[i] - grad2[i]) * (grad1[i] - grad2[i]);
        sum_grad += grad1[i] * grad1[i];
    }
    
    return sqrtf(sum_diff / sum_grad);
}
```

**10. åå‘ä¼ æ’­çš„Cè¯­è¨€å®ç°**

**å®Œæ•´åå‘ä¼ æ’­ç®—æ³•**ï¼š
```c
// åå‘ä¼ æ’­ä¸»å‡½æ•°
void nn_backward(NeuralNetwork* nn, float* input, float* target, ForwardCache* cache) {
    int num_layers = nn->num_layers;
    
    // è®¡ç®—è¾“å‡ºå±‚è¯¯å·®
    float* delta = malloc(nn->layers[num_layers-1].output_size * sizeof(float));
    compute_output_error(nn, cache->activations[num_layers], target, delta);
    
    // åå‘ä¼ æ’­è¯¯å·®
    for (int l = num_layers - 1; l >= 0; l--) {
        Layer* layer = &nn->layers[l];
        
        // è®¡ç®—æƒé‡æ¢¯åº¦
        compute_weight_gradients(layer, delta, cache->activations[l], cache->z_values[l]);
        
        // å¦‚æœä¸æ˜¯è¾“å…¥å±‚ï¼Œè®¡ç®—ä¸‹ä¸€å±‚çš„è¯¯å·®
        if (l > 0) {
            float* prev_delta = malloc(layer->input_size * sizeof(float));
            compute_hidden_error(layer, delta, prev_delta, cache->z_values[l-1]);
            free(delta);
            delta = prev_delta;
        }
    }
    
    free(delta);
}

// è®¡ç®—éšè—å±‚è¯¯å·®
void compute_hidden_error(Layer* layer, float* delta, float* prev_delta, float* z_values) {
    int input_size = layer->input_size;
    int output_size = layer->output_size;
    
    // åˆå§‹åŒ–å‰ä¸€å±‚è¯¯å·®
    for (int i = 0; i < input_size; i++) {
        prev_delta[i] = 0;
    }
    
    // è®¡ç®—è¯¯å·®ä¼ æ’­
    for (int i = 0; i < output_size; i++) {
        float activation_derivative = layer->activation_derivative(z_values[i]);
        
        for (int j = 0; j < input_size; j++) {
            prev_delta[j] += delta[i] * layer->weights[i * input_size + j] * activation_derivative;
        }
    }
}

// è®¡ç®—æƒé‡æ¢¯åº¦
void compute_weight_gradients(Layer* layer, float* delta, float* activations, float* z_values) {
    int input_size = layer->input_size;
    int output_size = layer->output_size;
    
    // è®¡ç®—æƒé‡æ¢¯åº¦
    for (int i = 0; i < output_size; i++) {
        float activation_derivative = layer->activation_derivative(z_values[i]);
        
        for (int j = 0; j < input_size; j++) {
            float gradient = delta[i] * activations[j] * activation_derivative;
            layer->weight_gradients[i * input_size + j] = gradient;
        }
        
        // è®¡ç®—åç½®æ¢¯åº¦
        layer->bias_gradients[i] = delta[i] * activation_derivative;
    }
}

// æ›´æ–°ç½‘ç»œå‚æ•°
void update_network_parameters(NeuralNetwork* nn, float learning_rate) {
    for (int l = 0; l < nn->num_layers; l++) {
        Layer* layer = &nn->layers[l];
        int weight_size = layer->output_size * layer->input_size;
        
        // æ›´æ–°æƒé‡
        for (int i = 0; i < weight_size; i++) {
            layer->weights[i] -= learning_rate * layer->weight_gradients[i];
        }
        
        // æ›´æ–°åç½®
        for (int i = 0; i < layer->output_size; i++) {
            layer->biases[i] -= learning_rate * layer->bias_gradients[i];
        }
    }
}
```

**11. æ•°å€¼æ¢¯åº¦æ£€æŸ¥å®ç°**

**æ•°å€¼æ¢¯åº¦è®¡ç®—**ï¼š
```c
// è®¡ç®—æ•°å€¼æ¢¯åº¦
void compute_numerical_gradients(NeuralNetwork* nn, float* input, float* target, 
                                float* numerical_gradients, float epsilon) {
    int total_params = get_total_parameters(nn);
    float* original_params = malloc(total_params * sizeof(float));
    
    // ä¿å­˜åŸå§‹å‚æ•°
    extract_parameters(nn, original_params);
    
    // è®¡ç®—æ•°å€¼æ¢¯åº¦
    for (int i = 0; i < total_params; i++) {
        // å‰å‘æ‰°åŠ¨
        float* perturbed_params = malloc(total_params * sizeof(float));
        memcpy(perturbed_params, original_params, total_params * sizeof(float));
        perturbed_params[i] += epsilon;
        
        // è®¡ç®—æ‰°åŠ¨åçš„æŸå¤±
        set_parameters(nn, perturbed_params);
        float* output = malloc(nn->layers[nn->num_layers-1].output_size * sizeof(float));
        nn_forward(nn, input, output);
        float loss_plus = mse_loss(output, target, nn->layers[nn->num_layers-1].output_size);
        
        // åå‘æ‰°åŠ¨
        perturbed_params[i] = original_params[i] - epsilon;
        set_parameters(nn, perturbed_params);
        nn_forward(nn, input, output);
        float loss_minus = mse_loss(output, target, nn->layers[nn->num_layers-1].output_size);
        
        // è®¡ç®—æ•°å€¼æ¢¯åº¦
        numerical_gradients[i] = (loss_plus - loss_minus) / (2 * epsilon);
        
        free(perturbed_params);
        free(output);
    }
    
    // æ¢å¤åŸå§‹å‚æ•°
    set_parameters(nn, original_params);
    free(original_params);
}

// æ¯”è¾ƒæ¢¯åº¦
bool compare_gradients(float* grad1, float* grad2, int size, float tolerance) {
    for (int i = 0; i < size; i++) {
        if (fabsf(grad1[i] - grad2[i]) > tolerance) {
            return false;
        }
    }
    return true;
}
```

**5. åå‘ä¼ æ’­çš„çŸ©é˜µå½¢å¼**

**æ‰¹é‡å¤„ç†**ï¼š
å¯¹äºæ‰¹é‡å¤§å°ä¸ºmçš„æ•°æ®ï¼š
```
Zâ½Ë¡â¾ = Wâ½Ë¡â¾Aâ½Ë¡â»Â¹â¾ + bâ½Ë¡â¾
Aâ½Ë¡â¾ = Ïƒ(Zâ½Ë¡â¾)
```

å…¶ä¸­ï¼š
- Zâ½Ë¡â¾ âˆˆ â„^(nâ½Ë¡â¾ Ã— m)
- Aâ½Ë¡â¾ âˆˆ â„^(nâ½Ë¡â¾ Ã— m)
- Wâ½Ë¡â¾ âˆˆ â„^(nâ½Ë¡â¾ Ã— nâ½Ë¡â»Â¹â¾)
- bâ½Ë¡â¾ âˆˆ â„^(nâ½Ë¡â¾ Ã— 1)

**æ¢¯åº¦è®¡ç®—**ï¼š
```
âˆ‚L/âˆ‚Wâ½Ë¡â¾ = Î´â½Ë¡â¾(Aâ½Ë¡â»Â¹â¾)áµ€
âˆ‚L/âˆ‚bâ½Ë¡â¾ = Î´â½Ë¡â¾ * 1
```

**6. åå‘ä¼ æ’­çš„æ•°å€¼ç¨³å®šæ€§**

**æ¢¯åº¦çˆ†ç‚¸**ï¼š
å½“æƒé‡è¿‡å¤§æ—¶ï¼Œæ¢¯åº¦å¯èƒ½æŒ‡æ•°å¢é•¿ï¼š
```
Î´â½Ë¡â¾ = (Wâ½Ë¡âºÂ¹â¾)áµ€Î´â½Ë¡âºÂ¹â¾ * Ïƒ'(zâ½Ë¡â¾)
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
- æ¢¯åº¦è£å‰ª
- æƒé‡æ­£åˆ™åŒ–
- åˆé€‚çš„æƒé‡åˆå§‹åŒ–

**æ¢¯åº¦æ¶ˆå¤±**ï¼š
å½“æƒé‡è¿‡å°æ—¶ï¼Œæ¢¯åº¦å¯èƒ½æŒ‡æ•°è¡°å‡ï¼š
```
Î´â½Ë¡â¾ = (Wâ½Ë¡âºÂ¹â¾)áµ€Î´â½Ë¡âºÂ¹â¾ * Ïƒ'(zâ½Ë¡â¾)
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
- ä½¿ç”¨ReLUç­‰æ¿€æ´»å‡½æ•°
- æ®‹å·®è¿æ¥
- æ‰¹å½’ä¸€åŒ–

**7. åå‘ä¼ æ’­ç®—æ³•çš„å®ç°ç»†èŠ‚**

**å†…å­˜ç®¡ç†**ï¼š
```c
// ç¼“å­˜å‰å‘ä¼ æ’­çš„ä¸­é—´ç»“æœ
typedef struct {
    float* activations[MAX_LAYERS];
    float* z_values[MAX_LAYERS];
    int num_layers;
} ForwardCache;

void forward_propagation_with_cache(NeuralNetwork* nn, float* input, ForwardCache* cache) {
    // å­˜å‚¨è¾“å…¥å±‚
    cache->activations[0] = input;
    
    for (int l = 0; l < nn->num_layers; l++) {
        Layer* layer = &nn->layers[l];
        
        // è®¡ç®—zå€¼
        cache->z_values[l] = malloc(layer->output_size * sizeof(float));
        compute_linear_combination(layer, cache->activations[l], cache->z_values[l]);
        
        // è®¡ç®—æ¿€æ´»å€¼
        cache->activations[l+1] = malloc(layer->output_size * sizeof(float));
        apply_activation(layer->activation, cache->z_values[l], cache->activations[l+1]);
    }
}
```

**æ¢¯åº¦è®¡ç®—**ï¼š
```c
// è®¡ç®—è¾“å‡ºå±‚è¯¯å·®
void compute_output_error(NeuralNetwork* nn, float* output, float* target, float* delta) {
    int output_size = nn->layers[nn->num_layers-1].output_size;
    
    for (int i = 0; i < output_size; i++) {
        // æŸå¤±å‡½æ•°å¯¹è¾“å‡ºçš„æ¢¯åº¦
        float loss_gradient = 2 * (output[i] - target[i]);
        
        // æ¿€æ´»å‡½æ•°å¯¼æ•°
        float activation_derivative = compute_activation_derivative(
            nn->layers[nn->num_layers-1].activation_derivative,
            nn->cache.z_values[nn->num_layers-1][i]
        );
        
        delta[i] = loss_gradient * activation_derivative;
    }
}
```

**æƒé‡æ›´æ–°**ï¼š
```c
// æ›´æ–°æƒé‡å’Œåç½®
void update_weights(Layer* layer, float* delta, float learning_rate) {
    int input_size = layer->input_size;
    int output_size = layer->output_size;
    
    // æ›´æ–°æƒé‡
    for (int i = 0; i < output_size; i++) {
        for (int j = 0; j < input_size; j++) {
            float gradient = delta[i] * layer->input_cache[j];
            layer->weights[i * input_size + j] -= learning_rate * gradient;
        }
    }
    
    // æ›´æ–°åç½®
    for (int i = 0; i < output_size; i++) {
        layer->biases[i] -= learning_rate * delta[i];
    }
}
```

**8. åå‘ä¼ æ’­çš„å¤æ‚åº¦åˆ†æ**

**æ—¶é—´å¤æ‚åº¦**ï¼š
- **å‰å‘ä¼ æ’­**ï¼šO(Î£nâ½Ë¡â¾nâ½Ë¡â»Â¹â¾)
- **åå‘ä¼ æ’­**ï¼šO(Î£nâ½Ë¡â¾nâ½Ë¡â»Â¹â¾)
- **æ€»ä½“å¤æ‚åº¦**ï¼šO(Î£nâ½Ë¡â¾nâ½Ë¡â»Â¹â¾)

**ç©ºé—´å¤æ‚åº¦**ï¼š
- **å­˜å‚¨æ¿€æ´»å€¼**ï¼šO(Î£nâ½Ë¡â¾)
- **å­˜å‚¨æ¢¯åº¦**ï¼šO(Î£nâ½Ë¡â¾nâ½Ë¡â»Â¹â¾)
- **æ€»ä½“å¤æ‚åº¦**ï¼šO(Î£nâ½Ë¡â¾nâ½Ë¡â»Â¹â¾)

**9. åå‘ä¼ æ’­çš„éªŒè¯æ–¹æ³•**

**æ¢¯åº¦æ£€æŸ¥**ï¼š
```c
// æ•°å€¼æ¢¯åº¦æ£€æŸ¥
bool gradient_check(NeuralNetwork* nn, float* input, float* target, float epsilon) {
    float* numerical_gradients = malloc(get_total_parameters(nn) * sizeof(float));
    float* analytical_gradients = malloc(get_total_parameters(nn) * sizeof(float));
    
    // è®¡ç®—æ•°å€¼æ¢¯åº¦
    compute_numerical_gradients(nn, input, target, numerical_gradients, epsilon);
    
    // è®¡ç®—è§£ææ¢¯åº¦
    forward_propagation(nn, input);
    backward_propagation(nn, input, target);
    extract_gradients(nn, analytical_gradients);
    
    // æ¯”è¾ƒæ¢¯åº¦
    bool is_correct = compare_gradients(numerical_gradients, analytical_gradients, 1e-7);
    
    free(numerical_gradients);
    free(analytical_gradients);
    
    return is_correct;
}
```

**ç›¸å¯¹è¯¯å·®è®¡ç®—**ï¼š
```c
float relative_error(float* grad1, float* grad2, int size) {
    float sum_diff = 0;
    float sum_grad = 0;
    
    for (int i = 0; i < size; i++) {
        sum_diff += (grad1[i] - grad2[i]) * (grad1[i] - grad2[i]);
        sum_grad += grad1[i] * grad1[i];
    }
    
    return sqrtf(sum_diff / sum_grad);
}
```

#### å®è·µç»ƒä¹ 

**ç»ƒä¹ 1ï¼šåå‘ä¼ æ’­åŸºç¡€æµ‹è¯•**

```c
// åå‘ä¼ æ’­åŸºç¡€æµ‹è¯•
void test_backpropagation_basic() {
    printf("=== åå‘ä¼ æ’­åŸºç¡€æµ‹è¯• ===\n");
    
    // åˆ›å»ºç®€å•ç½‘ç»œï¼š2è¾“å…¥ -> 3éšè— -> 1è¾“å‡º
    int layer_sizes[] = {2, 3, 1};
    NeuralNetwork* nn = nn_create(layer_sizes, 3, 0.01f);
    
    // åˆ›å»ºå‰å‘ç¼“å­˜
    ForwardCache cache;
    cache.num_layers = nn->num_layers;
    
    // æµ‹è¯•æ•°æ®
    float input[] = {0.5f, -0.3f};
    float target[] = {0.8f};
    
    printf("è¾“å…¥: [%.1f, %.1f]\n", input[0], input[1]);
    printf("ç›®æ ‡: %.1f\n", target[0]);
    
    // å‰å‘ä¼ æ’­
    float output[1];
    nn_forward_with_cache(nn, input, output, &cache);
    printf("å‰å‘ä¼ æ’­è¾“å‡º: %.4f\n", output[0]);
    
    // åå‘ä¼ æ’­
    nn_backward(nn, input, target, &cache);
    
    // æ£€æŸ¥æ¢¯åº¦
    printf("åå‘ä¼ æ’­å®Œæˆï¼Œæ£€æŸ¥æ¢¯åº¦...\n");
    for (int l = 0; l < nn->num_layers; l++) {
        Layer* layer = &nn->layers[l];
        printf("ç¬¬%då±‚æƒé‡æ¢¯åº¦èŒƒå›´: [%.4f, %.4f]\n", 
               l+1, layer->weight_gradients[0], 
               layer->weight_gradients[layer->output_size * layer->input_size - 1]);
    }
    
    // æ¸…ç†ç¼“å­˜
    for (int l = 0; l < nn->num_layers; l++) {
        free(cache.activations[l+1]);
        free(cache.z_values[l]);
    }
    
    nn_free(nn);
    printf("\n");
}
```

**ç»ƒä¹ 2ï¼šæ¢¯åº¦æ£€æŸ¥éªŒè¯**

```c
// æ¢¯åº¦æ£€æŸ¥éªŒè¯
void test_gradient_checking() {
    printf("=== æ¢¯åº¦æ£€æŸ¥éªŒè¯ ===\n");
    
    // åˆ›å»ºæµ‹è¯•ç½‘ç»œ
    int layer_sizes[] = {2, 4, 1};
    NeuralNetwork* nn = nn_create(layer_sizes, 3, 0.01f);
    
    // æµ‹è¯•æ•°æ®
    float input[] = {0.3f, 0.7f};
    float target[] = {0.5f};
    
    printf("æ‰§è¡Œæ¢¯åº¦æ£€æŸ¥...\n");
    
    // æ‰§è¡Œæ¢¯åº¦æ£€æŸ¥
    bool is_correct = gradient_check(nn, input, target, 1e-7);
    
    if (is_correct) {
        printf("âœ“ æ¢¯åº¦æ£€æŸ¥é€šè¿‡ï¼åå‘ä¼ æ’­å®ç°æ­£ç¡®ã€‚\n");
    } else {
        printf("âœ— æ¢¯åº¦æ£€æŸ¥å¤±è´¥ï¼åå‘ä¼ æ’­å®ç°æœ‰è¯¯ã€‚\n");
    }
    
    nn_free(nn);
    printf("\n");
}

// ç®€åŒ–çš„æ¢¯åº¦æ£€æŸ¥å®ç°
bool simple_gradient_check(NeuralNetwork* nn, float* input, float* target) {
    const float epsilon = 1e-6f;
    const float tolerance = 1e-5f;
    
    // è®¡ç®—è§£ææ¢¯åº¦
    ForwardCache cache;
    cache.num_layers = nn->num_layers;
    
    float output[1];
    nn_forward_with_cache(nn, input, output, &cache);
    nn_backward(nn, input, target, &cache);
    
    // æå–è§£ææ¢¯åº¦
    float* analytical_gradients = malloc(100 * sizeof(float));  // å‡è®¾100ä¸ªå‚æ•°
    extract_gradients_simple(nn, analytical_gradients);
    
    // è®¡ç®—æ•°å€¼æ¢¯åº¦
    float* numerical_gradients = malloc(100 * sizeof(float));
    compute_numerical_gradients_simple(nn, input, target, numerical_gradients, epsilon);
    
    // æ¯”è¾ƒæ¢¯åº¦
    bool is_correct = true;
    for (int i = 0; i < 100; i++) {
        if (fabsf(analytical_gradients[i] - numerical_gradients[i]) > tolerance) {
            is_correct = false;
            break;
        }
    }
    
    free(analytical_gradients);
    free(numerical_gradients);
    
    return is_correct;
}
```

**ç»ƒä¹ 3ï¼šåå‘ä¼ æ’­æ€§èƒ½æµ‹è¯•**

```c
// åå‘ä¼ æ’­æ€§èƒ½æµ‹è¯•
void test_backpropagation_performance() {
    printf("=== åå‘ä¼ æ’­æ€§èƒ½æµ‹è¯• ===\n");
    
    // åˆ›å»ºè¾ƒå¤§ç½‘ç»œ
    int layer_sizes[] = {50, 30, 20, 10, 1};
    NeuralNetwork* nn = nn_create(layer_sizes, 5, 0.01f);
    
    // ç”Ÿæˆæµ‹è¯•æ•°æ®
    const int num_samples = 1000;
    float* inputs = malloc(50 * num_samples * sizeof(float));
    float* targets = malloc(num_samples * sizeof(float));
    
    for (int i = 0; i < 50 * num_samples; i++) {
        inputs[i] = ((float)rand() / RAND_MAX - 0.5f) * 2.0f;
    }
    for (int i = 0; i < num_samples; i++) {
        targets[i] = ((float)rand() / RAND_MAX - 0.5f) * 2.0f;
    }
    
    // æµ‹è¯•å‰å‘ä¼ æ’­æ€§èƒ½
    clock_t start = clock();
    for (int i = 0; i < num_samples; i++) {
        float output[1];
        nn_forward(nn, &inputs[i * 50], output);
    }
    clock_t end = clock();
    double forward_time = ((double)(end - start)) / CLOCKS_PER_SEC;
    
    // æµ‹è¯•åå‘ä¼ æ’­æ€§èƒ½
    start = clock();
    for (int i = 0; i < num_samples; i++) {
        ForwardCache cache;
        cache.num_layers = nn->num_layers;
        
        float output[1];
        nn_forward_with_cache(nn, &inputs[i * 50], output, &cache);
        nn_backward(nn, &inputs[i * 50], &targets[i], &cache);
        
        // æ¸…ç†ç¼“å­˜
        for (int l = 0; l < nn->num_layers; l++) {
            free(cache.activations[l+1]);
            free(cache.z_values[l]);
        }
    }
    end = clock();
    double backward_time = ((double)(end - start)) / CLOCKS_PER_SEC;
    
    printf("å‰å‘ä¼ æ’­: %.4f ç§’ (%d ä¸ªæ ·æœ¬)\n", forward_time, num_samples);
    printf("åå‘ä¼ æ’­: %.4f ç§’ (%d ä¸ªæ ·æœ¬)\n", backward_time, num_samples);
    printf("å‰å‘/åå‘æ—¶é—´æ¯”: %.2f\n", forward_time / backward_time);
    
    free(inputs);
    free(targets);
    nn_free(nn);
    printf("\n");
}
```

**ç»ƒä¹ 4ï¼šæ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸æµ‹è¯•**

```c
// æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸æµ‹è¯•
void test_gradient_vanishing_exploding() {
    printf("=== æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸æµ‹è¯• ===\n");
    
    // åˆ›å»ºæ·±å±‚ç½‘ç»œ
    int layer_sizes[] = {2, 10, 10, 10, 10, 1};
    NeuralNetwork* nn = nn_create(layer_sizes, 6, 0.01f);
    
    // æµ‹è¯•æ•°æ®
    float input[] = {0.5f, -0.3f};
    float target[] = {0.8f};
    
    // æ‰§è¡Œå‰å‘å’Œåå‘ä¼ æ’­
    ForwardCache cache;
    cache.num_layers = nn->num_layers;
    
    float output[1];
    nn_forward_with_cache(nn, input, output, &cache);
    nn_backward(nn, input, target, &cache);
    
    // åˆ†æå„å±‚æ¢¯åº¦
    printf("å„å±‚æ¢¯åº¦åˆ†æ:\n");
    for (int l = 0; l < nn->num_layers; l++) {
        Layer* layer = &nn->layers[l];
        int weight_size = layer->output_size * layer->input_size;
        
        float max_grad = 0;
        float min_grad = 0;
        float avg_grad = 0;
        
        for (int i = 0; i < weight_size; i++) {
            float grad = layer->weight_gradients[i];
            if (grad > max_grad) max_grad = grad;
            if (grad < min_grad) min_grad = grad;
            avg_grad += grad;
        }
        avg_grad /= weight_size;
        
        printf("ç¬¬%då±‚: æœ€å¤§æ¢¯åº¦=%.4f, æœ€å°æ¢¯åº¦=%.4f, å¹³å‡æ¢¯åº¦=%.4f\n", 
               l+1, max_grad, min_grad, avg_grad);
        
        // æ£€æŸ¥æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸
        if (fabsf(max_grad) < 1e-6) {
            printf("  è­¦å‘Š: ç¬¬%då±‚å¯èƒ½å­˜åœ¨æ¢¯åº¦æ¶ˆå¤±\n", l+1);
        }
        if (fabsf(max_grad) > 10.0) {
            printf("  è­¦å‘Š: ç¬¬%då±‚å¯èƒ½å­˜åœ¨æ¢¯åº¦çˆ†ç‚¸\n", l+1);
        }
    }
    
    // æ¸…ç†
    for (int l = 0; l < nn->num_layers; l++) {
        free(cache.activations[l+1]);
        free(cache.z_values[l]);
    }
    
    nn_free(nn);
    printf("\n");
}
```

**ç»ƒä¹ 5ï¼šæ‰¹é‡åå‘ä¼ æ’­æµ‹è¯•**

```c
// æ‰¹é‡åå‘ä¼ æ’­æµ‹è¯•
void test_batch_backpropagation() {
    printf("=== æ‰¹é‡åå‘ä¼ æ’­æµ‹è¯• ===\n");
    
    // åˆ›å»ºç½‘ç»œ
    int layer_sizes[] = {3, 5, 2};
    NeuralNetwork* nn = nn_create(layer_sizes, 3, 0.01f);
    
    // æ‰¹é‡æ•°æ®
    const int batch_size = 4;
    float inputs[][3] = {
        {0.1f, 0.2f, 0.3f},
        {0.4f, 0.5f, 0.6f},
        {0.7f, 0.8f, 0.9f},
        {1.0f, 1.1f, 1.2f}
    };
    float targets[][2] = {
        {0.5f, 0.3f},
        {0.7f, 0.4f},
        {0.9f, 0.6f},
        {1.1f, 0.8f}
    };
    
    printf("æ‰¹é‡å¤§å°: %d\n", batch_size);
    
    // æ‰¹é‡å‰å‘ä¼ æ’­
    float* batch_outputs = malloc(batch_size * 2 * sizeof(float));
    for (int i = 0; i < batch_size; i++) {
        nn_forward(nn, inputs[i], &batch_outputs[i * 2]);
    }
    
    printf("æ‰¹é‡å‰å‘ä¼ æ’­å®Œæˆ\n");
    
    // æ‰¹é‡åå‘ä¼ æ’­
    for (int i = 0; i < batch_size; i++) {
        ForwardCache cache;
        cache.num_layers = nn->num_layers;
        
        float output[2];
        nn_forward_with_cache(nn, inputs[i], output, &cache);
        nn_backward(nn, inputs[i], targets[i], &cache);
        
        // æ¸…ç†ç¼“å­˜
        for (int l = 0; l < nn->num_layers; l++) {
            free(cache.activations[l+1]);
            free(cache.z_values[l]);
        }
    }
    
    printf("æ‰¹é‡åå‘ä¼ æ’­å®Œæˆ\n");
    
    // æ˜¾ç¤ºç»“æœ
    printf("æ‰¹é‡è¾“å‡º:\n");
    for (int i = 0; i < batch_size; i++) {
        printf("æ ·æœ¬%d: [%.4f, %.4f] -> [%.4f, %.4f]\n", 
               i, inputs[i][0], inputs[i][1], 
               batch_outputs[i*2], batch_outputs[i*2+1]);
    }
    
    free(batch_outputs);
    nn_free(nn);
    printf("\n");
}
```

**ç»ƒä¹ 6ï¼šæ•°å€¼ç¨³å®šæ€§æµ‹è¯•**

```c
// æ•°å€¼ç¨³å®šæ€§æµ‹è¯•
void test_numerical_stability() {
    printf("=== æ•°å€¼ç¨³å®šæ€§æµ‹è¯• ===\n");
    
    // åˆ›å»ºç½‘ç»œ
    int layer_sizes[] = {2, 3, 1};
    NeuralNetwork* nn = nn_create(layer_sizes, 3, 0.01f);
    
    // æµ‹è¯•æç«¯è¾“å…¥
    float extreme_inputs[][2] = {
        {1e6f, 1e6f},      // æå¤§å€¼
        {-1e6f, -1e6f},    // æå°å€¼
        {1e-6f, 1e-6f},    // æå°æ­£å€¼
        {0.0f, 0.0f},      // é›¶å€¼
        {1.0f, 1.0f}       // æ­£å¸¸å€¼
    };
    
    float target[] = {0.5f};
    
    printf("æµ‹è¯•æç«¯è¾“å…¥å€¼:\n");
    for (int i = 0; i < 5; i++) {
        ForwardCache cache;
        cache.num_layers = nn->num_layers;
        
        float output[1];
        
        // æ£€æŸ¥æ˜¯å¦äº§ç”ŸNaNæˆ–æ— ç©·å¤§
        bool has_nan = false;
        bool has_inf = false;
        
        nn_forward_with_cache(nn, extreme_inputs[i], output, &cache);
        nn_backward(nn, extreme_inputs[i], target, &cache);
        
        // æ£€æŸ¥è¾“å‡º
        if (isnan(output[0])) has_nan = true;
        if (isinf(output[0])) has_inf = true;
        
        // æ£€æŸ¥æ¢¯åº¦
        for (int l = 0; l < nn->num_layers; l++) {
            Layer* layer = &nn->layers[l];
            int weight_size = layer->output_size * layer->input_size;
            
            for (int j = 0; j < weight_size; j++) {
                if (isnan(layer->weight_gradients[j])) has_nan = true;
                if (isinf(layer->weight_gradients[j])) has_inf = true;
            }
        }
        
        printf("è¾“å…¥[%.1e, %.1e]: è¾“å‡º=%.4f, NaN=%s, Inf=%s\n", 
               extreme_inputs[i][0], extreme_inputs[i][1], output[0],
               has_nan ? "æ˜¯" : "å¦", has_inf ? "æ˜¯" : "å¦");
        
        // æ¸…ç†ç¼“å­˜
        for (int l = 0; l < nn->num_layers; l++) {
            free(cache.activations[l+1]);
            free(cache.z_values[l]);
        }
    }
    
    nn_free(nn);
    printf("\n");
}
```

**ç»ƒä¹ 7ï¼šå®Œæ•´è®­ç»ƒå¾ªç¯æµ‹è¯•**

```c
// å®Œæ•´è®­ç»ƒå¾ªç¯æµ‹è¯•
void test_complete_training_loop() {
    printf("=== å®Œæ•´è®­ç»ƒå¾ªç¯æµ‹è¯• ===\n");
    
    // åˆ›å»ºç½‘ç»œ
    int layer_sizes[] = {2, 4, 1};
    NeuralNetwork* nn = nn_create(layer_sizes, 3, 0.01f);
    
    // ç”Ÿæˆè®­ç»ƒæ•°æ®: y = 2*x1 + 3*x2 + 1
    const int num_samples = 100;
    float* inputs = malloc(num_samples * 2 * sizeof(float));
    float* targets = malloc(num_samples * sizeof(float));
    
    for (int i = 0; i < num_samples; i++) {
        float x1 = ((float)rand() / RAND_MAX - 0.5f) * 2.0f;
        float x2 = ((float)rand() / RAND_MAX - 0.5f) * 2.0f;
        
        inputs[i * 2] = x1;
        inputs[i * 2 + 1] = x2;
        targets[i] = 2.0f * x1 + 3.0f * x2 + 1.0f;
    }
    
    // è®­ç»ƒå¾ªç¯
    const int epochs = 50;
    const int batch_size = 10;
    
    printf("å¼€å§‹è®­ç»ƒ...\n");
    for (int epoch = 0; epoch < epochs; epoch++) {
        float total_loss = 0;
        
        for (int batch = 0; batch < num_samples; batch += batch_size) {
            int current_batch_size = (batch + batch_size <= num_samples) ? 
                                   batch_size : (num_samples - batch);
            
            for (int i = 0; i < current_batch_size; i++) {
                int sample_idx = batch + i;
                
                // å‰å‘ä¼ æ’­
                ForwardCache cache;
                cache.num_layers = nn->num_layers;
                
                float output[1];
                nn_forward_with_cache(nn, &inputs[sample_idx * 2], output, &cache);
                
                // è®¡ç®—æŸå¤±
                float loss = (output[0] - targets[sample_idx]) * (output[0] - targets[sample_idx]);
                total_loss += loss;
                
                // åå‘ä¼ æ’­
                nn_backward(nn, &inputs[sample_idx * 2], &targets[sample_idx], &cache);
                
                // æ›´æ–°å‚æ•°
                update_network_parameters(nn, nn->learning_rate);
                
                // æ¸…ç†ç¼“å­˜
                for (int l = 0; l < nn->num_layers; l++) {
                    free(cache.activations[l+1]);
                    free(cache.z_values[l]);
                }
            }
        }
        
        if (epoch % 10 == 0) {
            printf("Epoch %d, Average Loss: %.6f\n", epoch, total_loss / num_samples);
        }
    }
    
    printf("è®­ç»ƒå®Œæˆï¼\n");
    
    // æµ‹è¯•è®­ç»ƒç»“æœ
    float test_input[] = {1.0f, 1.0f};
    float test_output[1];
    nn_forward(nn, test_input, test_output);
    float expected = 2.0f * test_input[0] + 3.0f * test_input[1] + 1.0f;
    
    printf("æµ‹è¯•è¾“å…¥: [%.1f, %.1f]\n", test_input[0], test_input[1]);
    printf("é¢„æµ‹è¾“å‡º: %.4f\n", test_output[0]);
    printf("æœŸæœ›è¾“å‡º: %.4f\n", expected);
    printf("è¯¯å·®: %.4f\n", fabsf(test_output[0] - expected));
    
    free(inputs);
    free(targets);
    nn_free(nn);
    printf("\n");
}
```

**ç»ƒä¹ 8ï¼šä¸»æµ‹è¯•ç¨‹åº**

```c
// ä¸»æµ‹è¯•ç¨‹åº
int main() {
    printf("åå‘ä¼ æ’­ç®—æ³•æµ‹è¯•ç¨‹åº\n");
    printf("==================\n\n");
    
    // è®¾ç½®éšæœºç§å­
    srand(time(NULL));
    
    // è¿è¡Œæ‰€æœ‰æµ‹è¯•
    test_backpropagation_basic();
    test_gradient_checking();
    test_backpropagation_performance();
    test_gradient_vanishing_exploding();
    test_batch_backpropagation();
    test_numerical_stability();
    test_complete_training_loop();
    
    printf("æ‰€æœ‰æµ‹è¯•å®Œæˆï¼\n");
    return 0;
}
```

### ç¬¬5å‘¨ï¼šæ¢¯åº¦ä¸‹é™ä¼˜åŒ–ç®—æ³•

#### å­¦ä¹ å†…å®¹
- **æ‰¹é‡æ¢¯åº¦ä¸‹é™**
- **éšæœºæ¢¯åº¦ä¸‹é™**
- **å°æ‰¹é‡æ¢¯åº¦ä¸‹é™**

#### ç†è®ºçŸ¥è¯†

**1. æ¢¯åº¦ä¸‹é™å˜ä½“**
- **æ‰¹é‡æ¢¯åº¦ä¸‹é™(BGD)**: ä½¿ç”¨å…¨éƒ¨æ•°æ®è®¡ç®—æ¢¯åº¦
- **éšæœºæ¢¯åº¦ä¸‹é™(SGD)**: ä½¿ç”¨å•ä¸ªæ ·æœ¬è®¡ç®—æ¢¯åº¦
- **å°æ‰¹é‡æ¢¯åº¦ä¸‹é™(Mini-batch)**: ä½¿ç”¨éƒ¨åˆ†æ•°æ®è®¡ç®—æ¢¯åº¦

**2. ä¼˜åŒ–ç®—æ³•æ¯”è¾ƒ**
```
BGDä¼˜ç‚¹: æ¢¯åº¦ä¼°è®¡å‡†ç¡®
BGDç¼ºç‚¹: è®¡ç®—é‡å¤§ï¼Œå†…å­˜æ¶ˆè€—å¤§

SGDä¼˜ç‚¹: è®¡ç®—é‡å°ï¼Œå¯ä»¥é€ƒç¦»å±€éƒ¨æœ€ä¼˜
SGDç¼ºç‚¹: æ¢¯åº¦ä¼°è®¡ä¸å‡†ç¡®ï¼Œæ”¶æ•›ä¸ç¨³å®š

Mini-batchä¼˜ç‚¹: å¹³è¡¡äº†è®¡ç®—æ•ˆç‡å’Œæ¢¯åº¦å‡†ç¡®æ€§
```

**3. å­¦ä¹ ç‡è°ƒåº¦**
- **å›ºå®šå­¦ä¹ ç‡**: ç®€å•ä½†å¯èƒ½ä¸æ˜¯æœ€ä¼˜
- **å­¦ä¹ ç‡è¡°å‡**: Î± = Î±â‚€ / (1 + decay * epoch)
- **è‡ªé€‚åº”å­¦ä¹ ç‡**: Adamã€RMSprop

**4. æ¢¯åº¦ä¸‹é™çš„æ”¶æ•›æ€§åˆ†æ**

**æ”¶æ•›æ¡ä»¶**ï¼š
- **Lipschitzè¿ç»­**: ||âˆ‡f(x) - âˆ‡f(y)|| â‰¤ L||x - y||
- **å¼ºå‡¸æ€§**: f(y) â‰¥ f(x) + âˆ‡f(x)áµ€(y-x) + (Î¼/2)||y-x||Â²
- **å­¦ä¹ ç‡é€‰æ‹©**: Î± â‰¤ 2/L

**æ”¶æ•›é€Ÿåº¦**ï¼š
- **å‡¸å‡½æ•°**: O(1/T)æ”¶æ•›é€Ÿåº¦
- **å¼ºå‡¸å‡½æ•°**: O(exp(-Î¼T))æ”¶æ•›é€Ÿåº¦
- **éå‡¸å‡½æ•°**: ç†è®ºä¸Šéš¾ä»¥ä¿è¯å…¨å±€æ”¶æ•›

**5. éšæœºæ¢¯åº¦ä¸‹é™çš„æ–¹å·®åˆ†æ**

**æ–¹å·®å‡å°‘æŠ€æœ¯**ï¼š
- **SVRG**: éšæœºæ–¹å·®å‡å°‘æ¢¯åº¦ä¸‹é™
- **SAGA**: éšæœºå¹³å‡æ¢¯åº¦ä¸‹é™
- **SAG**: éšæœºå¹³å‡æ¢¯åº¦

**æ–¹å·®åˆ†æ**ï¼š
```
Var[âˆ‡ÌƒJ(Î¸)] = (1/n)Î£Var[âˆ‡L(Î¸, x_i, y_i)]
```

**6. å°æ‰¹é‡æ¢¯åº¦ä¸‹é™çš„æ‰¹é‡å¤§å°é€‰æ‹©**

**æ‰¹é‡å¤§å°å½±å“**ï¼š
- **å°æ‰¹é‡**: æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œæ›´å¤šçš„å™ªå£°
- **å¤§æ‰¹é‡**: æ›´ç¨³å®šçš„æ¢¯åº¦ï¼Œæ›´å¿«çš„æ”¶æ•›
- **æœ€ä¼˜æ‰¹é‡**: é€šå¸¸32-256ä¹‹é—´

**å†…å­˜è€ƒè™‘**ï¼š
```c
// å†…å­˜ä½¿ç”¨è®¡ç®—
int memory_usage = batch_size * (input_size + hidden_size + output_size) * sizeof(float);
```

**7. æ¢¯åº¦ä¸‹é™çš„ç†è®ºåŸºç¡€**

**ä¼˜åŒ–ç†è®º**ï¼š
- **å‡¸ä¼˜åŒ–**ï¼šç›®æ ‡å‡½æ•°æ˜¯å‡¸å‡½æ•°ï¼Œæœ‰å…¨å±€æœ€ä¼˜è§£
- **éå‡¸ä¼˜åŒ–**ï¼šç›®æ ‡å‡½æ•°éå‡¸ï¼Œå¯èƒ½æœ‰å¤šä¸ªå±€éƒ¨æœ€ä¼˜
- **çº¦æŸä¼˜åŒ–**ï¼šåœ¨çº¦æŸæ¡ä»¶ä¸‹å¯»æ‰¾æœ€ä¼˜è§£

**æ”¶æ•›ç†è®º**ï¼š
- **å•è°ƒæ”¶æ•›**ï¼šç›®æ ‡å‡½æ•°å€¼å•è°ƒé€’å‡
- **çº¿æ€§æ”¶æ•›**ï¼šè¯¯å·®ä»¥å‡ ä½•çº§æ•°å‡å°
- **æ¬¡çº¿æ€§æ”¶æ•›**ï¼šè¯¯å·®å‡å°é€Ÿåº¦é€æ¸å˜æ…¢

**8. æ¢¯åº¦ä¸‹é™çš„å˜ä½“ç®—æ³•**

**åŠ¨é‡æ–¹æ³•**ï¼š
- **ç‰©ç†ç±»æ¯”**ï¼šæ¨¡æ‹Ÿç‰©ä½“åœ¨é‡åŠ›åœºä¸­çš„è¿åŠ¨
- **æ•°å­¦è¡¨ç¤º**ï¼šv = Î²v + (1-Î²)âˆ‡J(Î¸), Î¸ = Î¸ - Î±v
- **ä¼˜åŠ¿**ï¼šåŠ é€Ÿæ”¶æ•›ï¼Œé€ƒç¦»å±€éƒ¨æœ€ä¼˜

**ç‰©ç†æ„ä¹‰çš„å…·ä½“ä¾‹å­**ï¼š
- **å°çƒæ»šåŠ¨**ï¼šæƒ³è±¡ä¸€ä¸ªå°çƒåœ¨å‡¹å‡¸ä¸å¹³çš„å±±å¡ä¸Šæ»šåŠ¨
  - é‡åŠ›æ–¹å‘ï¼šç›¸å½“äºæ¢¯åº¦æ–¹å‘ï¼ŒæŒ‡å‘æœ€é™¡ä¸‹é™æ–¹å‘
  - æƒ¯æ€§ä½œç”¨ï¼šå°çƒä¸ä¼šç«‹å³æ”¹å˜æ–¹å‘ï¼Œä¿æŒä¸€å®šçš„è¿åŠ¨è¶‹åŠ¿
  - å±€éƒ¨æœ€ä¼˜ï¼šå°çƒå¯èƒ½å¡åœ¨æŸä¸ªå°å‘é‡Œï¼ˆå±€éƒ¨æœ€å°å€¼ï¼‰
  - åŠ¨é‡å¸®åŠ©ï¼šå¦‚æœå°çƒæœ‰è¶³å¤Ÿçš„é€Ÿåº¦ï¼Œå¯ä»¥å†²å‡ºå°å‘

- **æ»‘é›ªç±»æ¯”**ï¼šæ»‘é›ªè€…ä»å±±é¡¶æ»‘ä¸‹
  - é‡åŠ›ï¼šå§‹ç»ˆæŒ‡å‘å±±ä¸‹ï¼ˆæ¢¯åº¦æ–¹å‘ï¼‰
  - æƒ¯æ€§ï¼šæ»‘é›ªè€…ä¸ä¼šç«‹å³æ”¹å˜æ–¹å‘
  - åœ°å½¢ï¼šå±±ä¸Šæœ‰å„ç§å‘æ´¼ï¼ˆå±€éƒ¨æœ€ä¼˜ï¼‰
  - é€Ÿåº¦ï¼šè¶³å¤Ÿçš„é€Ÿåº¦å¯ä»¥è¶Šè¿‡å°éšœç¢

**NesterovåŠ¨é‡**ï¼š
- **æ”¹è¿›**ï¼šåœ¨è®¡ç®—æ¢¯åº¦å‰å…ˆåº”ç”¨åŠ¨é‡
- **æ•°å­¦è¡¨ç¤º**ï¼šÎ¸Ìƒ = Î¸ + Î²v, v = Î²v + (1-Î²)âˆ‡J(Î¸Ìƒ)
- **ä¼˜åŠ¿**ï¼šæ¯”æ ‡å‡†åŠ¨é‡æ”¶æ•›æ›´å¿«

**NesterovåŠ¨é‡çš„ç›´è§‚ç†è§£**ï¼š
- **é¢„æµ‹æ€§**ï¼šä¸æ˜¯åœ¨å½“å‰ç‚¹è®¡ç®—æ¢¯åº¦ï¼Œè€Œæ˜¯é¢„æµ‹ä¸‹ä¸€æ­¥çš„ä½ç½®
- **å‰ç»æ€§**ï¼šåœ¨é¢„æµ‹ä½ç½®è®¡ç®—æ¢¯åº¦ï¼Œæ›´å‡†ç¡®åœ°ä¼°è®¡ä¸‹é™æ–¹å‘
- **å®é™…ä¾‹å­**ï¼šå°±åƒå¼€è½¦æ—¶ï¼Œä¸æ˜¯çœ‹å½“å‰æ–¹å‘ç›˜ä½ç½®ï¼Œè€Œæ˜¯çœ‹è½¦å³å°†åˆ°è¾¾çš„ä½ç½®

**9. è‡ªé€‚åº”å­¦ä¹ ç‡æ–¹æ³•**

**AdaGrad**ï¼š
- **åŸç†**ï¼šæ ¹æ®å†å²æ¢¯åº¦è°ƒæ•´å­¦ä¹ ç‡
- **æ›´æ–°è§„åˆ™**ï¼šÎ¸ = Î¸ - Î±/âˆš(G + Îµ) * âˆ‡J(Î¸)
- **é€‚ç”¨**ï¼šç¨€ç–æ¢¯åº¦é—®é¢˜

**RMSprop**ï¼š
- **åŸç†**ï¼šä½¿ç”¨ç§»åŠ¨å¹³å‡è°ƒæ•´å­¦ä¹ ç‡
- **æ›´æ–°è§„åˆ™**ï¼šv = Î²v + (1-Î²)(âˆ‡J(Î¸))Â², Î¸ = Î¸ - Î±/âˆš(v + Îµ) * âˆ‡J(Î¸)
- **ä¼˜åŠ¿**ï¼šå¤„ç†éå¹³ç¨³ç›®æ ‡å‡½æ•°

**Adam**ï¼š
- **åŸç†**ï¼šç»“åˆåŠ¨é‡å’Œè‡ªé€‚åº”å­¦ä¹ ç‡
- **æ›´æ–°è§„åˆ™**ï¼šç»“åˆåŠ¨é‡å’ŒRMSprop
- **ä¼˜åŠ¿**ï¼šæ·±åº¦å­¦ä¹ ä¸­çš„æ ‡å‡†é€‰æ‹©

**10. æ¢¯åº¦ä¸‹é™çš„æ•°å€¼ç¨³å®šæ€§**

**æ¢¯åº¦è£å‰ª**ï¼š
- **åŸç†**ï¼šé™åˆ¶æ¢¯åº¦èŒƒæ•°ä¸è¶…è¿‡é˜ˆå€¼
- **å®ç°**ï¼šif ||âˆ‡J(Î¸)|| > threshold, âˆ‡J(Î¸) = threshold * âˆ‡J(Î¸)/||âˆ‡J(Î¸)||
- **ä½œç”¨**ï¼šé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸

**æ¢¯åº¦ç¼©æ”¾**ï¼š
- **åŸç†**ï¼šå¯¹æ¢¯åº¦è¿›è¡Œç¼©æ”¾
- **å®ç°**ï¼šâˆ‡J(Î¸) = âˆ‡J(Î¸) / batch_size
- **ä½œç”¨**ï¼šä¿æŒä¸åŒæ‰¹é‡å¤§å°çš„ä¸€è‡´æ€§

**11. æ¢¯åº¦ä¸‹é™çš„å¹¶è¡ŒåŒ–**

**æ•°æ®å¹¶è¡Œ**ï¼š
- **åŸç†**ï¼šå°†æ•°æ®åˆ†é…åˆ°å¤šä¸ªå¤„ç†å™¨
- **å®ç°**ï¼šæ¯ä¸ªå¤„ç†å™¨å¤„ç†éƒ¨åˆ†æ•°æ®ï¼Œæ±‡æ€»æ¢¯åº¦
- **ä¼˜åŠ¿**ï¼šçº¿æ€§åŠ é€Ÿæ¯”

**æ¨¡å‹å¹¶è¡Œ**ï¼š
- **åŸç†**ï¼šå°†æ¨¡å‹åˆ†é…åˆ°å¤šä¸ªå¤„ç†å™¨
- **å®ç°**ï¼šä¸åŒå¤„ç†å™¨å¤„ç†ä¸åŒå±‚
- **ä¼˜åŠ¿**ï¼šå¤„ç†å¤§æ¨¡å‹

**12. æ¢¯åº¦ä¸‹é™çš„æ”¶æ•›è¯Šæ–­**

**æ”¶æ•›åˆ¤æ–­**ï¼š
- **æ¢¯åº¦èŒƒæ•°**ï¼š||âˆ‡J(Î¸)|| < Îµ
- **å‚æ•°å˜åŒ–**ï¼š||Î¸_t - Î¸_{t-1}|| < Îµ
- **æŸå¤±å˜åŒ–**ï¼š|J(Î¸_t) - J(Î¸_{t-1})| < Îµ

**å‘æ•£æ£€æµ‹**ï¼š
- **æŸå¤±çˆ†ç‚¸**ï¼šJ(Î¸) â†’ âˆ
- **æ¢¯åº¦çˆ†ç‚¸**ï¼š||âˆ‡J(Î¸)|| â†’ âˆ
- **å‚æ•°çˆ†ç‚¸**ï¼š||Î¸|| â†’ âˆ

**13. æ¢¯åº¦ä¸‹é™çš„è¶…å‚æ•°è°ƒä¼˜**

**å­¦ä¹ ç‡é€‰æ‹©**ï¼š
- **ç½‘æ ¼æœç´¢**ï¼šåœ¨é¢„å®šä¹‰èŒƒå›´å†…æœç´¢
- **éšæœºæœç´¢**ï¼šéšæœºé‡‡æ ·å­¦ä¹ ç‡
- **è´å¶æ–¯ä¼˜åŒ–**ï¼šä½¿ç”¨æ¦‚ç‡æ¨¡å‹æŒ‡å¯¼æœç´¢

**æ‰¹é‡å¤§å°é€‰æ‹©**ï¼š
- **å†…å­˜é™åˆ¶**ï¼šæ ¹æ®å¯ç”¨å†…å­˜é€‰æ‹©
- **è®¡ç®—æ•ˆç‡**ï¼šé€‰æ‹©GPUå†…å­˜åˆ©ç”¨ç‡æœ€é«˜çš„æ‰¹é‡å¤§å°
- **æ³›åŒ–æ€§èƒ½**ï¼šå°æ‰¹é‡é€šå¸¸æ³›åŒ–æ›´å¥½

**14. æ¢¯åº¦ä¸‹é™çš„ç”Ÿç‰©å­¦æ„ä¹‰**

**èµ«å¸ƒå­¦ä¹ **ï¼š
- **åŸç†**ï¼š"ä¸€èµ·æ¿€æ´»çš„ç¥ç»å…ƒè¿æ¥ä¼šå¢å¼º"
- **æ•°å­¦è¡¨ç¤º**ï¼šÎ”w = Î· * pre_activation * post_error
- **æ¢¯åº¦ä¸‹é™**ï¼šå®ç°äº†èµ«å¸ƒå­¦ä¹ çš„æ•°å­¦å½¢å¼

**çªè§¦å¯å¡‘æ€§**ï¼š
- **é•¿æ—¶ç¨‹å¢å¼º(LTP)**ï¼šçªè§¦å¼ºåº¦å¢åŠ 
- **é•¿æ—¶ç¨‹æŠ‘åˆ¶(LTD)**ï¼šçªè§¦å¼ºåº¦å‡å°‘
- **æ¢¯åº¦ä¸‹é™**ï¼šæ¨¡æ‹Ÿçªè§¦å¼ºåº¦çš„è°ƒæ•´

**15. æ¢¯åº¦ä¸‹é™çš„å†å²å‘å±•**

**æ—©æœŸå‘å±•**ï¼š
- **1847å¹´**ï¼šCauchyæå‡ºæ¢¯åº¦ä¸‹é™æ€æƒ³
- **1940å¹´ä»£**ï¼šæ¢¯åº¦ä¸‹é™åœ¨ä¼˜åŒ–ä¸­åº”ç”¨
- **1960å¹´ä»£**ï¼šéšæœºæ¢¯åº¦ä¸‹é™å¼•å…¥

**ç°ä»£å‘å±•**ï¼š
- **1980å¹´ä»£**ï¼šåŠ¨é‡æ–¹æ³•å¼•å…¥
- **1990å¹´ä»£**ï¼šè‡ªé€‚åº”æ–¹æ³•å‘å±•
- **2000å¹´ä»£**ï¼šæ·±åº¦å­¦ä¹ ä¼˜åŒ–ç®—æ³•

**æœ€æ–°è¶‹åŠ¿**ï¼š
- **è‡ªé€‚åº”ä¼˜åŒ–**ï¼šAdamã€AdaBeliefç­‰
- **äºŒé˜¶æ–¹æ³•**ï¼šK-FACã€Shampooç­‰
- **åˆ†å¸ƒå¼ä¼˜åŒ–**ï¼šå¼‚æ­¥SGDã€æ¨¡å‹å¹¶è¡Œç­‰

**7. æ¢¯åº¦ä¸‹é™çš„Cè¯­è¨€å®ç°**

**å®Œæ•´æ¢¯åº¦ä¸‹é™æ¡†æ¶**ï¼š
```c
// è®­ç»ƒæ•°æ®ç»“æ„
typedef struct {
    float* inputs;
    float* targets;
    int num_samples;
    int input_size;
    int output_size;
} TrainingData;

// æ‰¹é‡æ¢¯åº¦ä¸‹é™
void batch_gradient_descent(NeuralNetwork* nn, TrainingData* data, int epochs) {
    for (int epoch = 0; epoch < epochs; epoch++) {
        float total_loss = 0;
        
        // ä½¿ç”¨å…¨éƒ¨æ•°æ®
        for (int i = 0; i < data->num_samples; i++) {
            float* input = &data->inputs[i * data->input_size];
            float* target = &data->targets[i * data->output_size];
            
            // å‰å‘ä¼ æ’­
            float* output = malloc(data->output_size * sizeof(float));
            nn_forward(nn, input, output);
            
            // è®¡ç®—æŸå¤±
            float loss = mse_loss(output, target, data->output_size);
            total_loss += loss;
            
            // åå‘ä¼ æ’­
            ForwardCache cache;
            cache.num_layers = nn->num_layers;
            nn_forward_with_cache(nn, input, output, &cache);
            nn_backward(nn, input, target, &cache);
            
            // æ›´æ–°å‚æ•°
            update_network_parameters(nn, nn->learning_rate);
            
            free(output);
            
            // æ¸…ç†ç¼“å­˜
            for (int l = 0; l < nn->num_layers; l++) {
                free(cache.activations[l+1]);
                free(cache.z_values[l]);
            }
        }
        
        if (epoch % 10 == 0) {
            printf("Epoch %d, Average Loss: %.6f\n", epoch, total_loss / data->num_samples);
        }
    }
}

// éšæœºæ¢¯åº¦ä¸‹é™
void stochastic_gradient_descent(NeuralNetwork* nn, TrainingData* data, int epochs) {
    for (int epoch = 0; epoch < epochs; epoch++) {
        float total_loss = 0;
        
        // éšæœºæ‰“ä¹±æ•°æ®
        shuffle_data(data);
        
        for (int i = 0; i < data->num_samples; i++) {
            float* input = &data->inputs[i * data->input_size];
            float* target = &data->targets[i * data->output_size];
            
            // å‰å‘ä¼ æ’­
            float* output = malloc(data->output_size * sizeof(float));
            nn_forward(nn, input, output);
            
            // è®¡ç®—æŸå¤±
            float loss = mse_loss(output, target, data->output_size);
            total_loss += loss;
            
            // åå‘ä¼ æ’­
            ForwardCache cache;
            cache.num_layers = nn->num_layers;
            nn_forward_with_cache(nn, input, output, &cache);
            nn_backward(nn, input, target, &cache);
            
            // ç«‹å³æ›´æ–°å‚æ•°
            update_network_parameters(nn, nn->learning_rate);
            
            free(output);
            
            // æ¸…ç†ç¼“å­˜
            for (int l = 0; l < nn->num_layers; l++) {
                free(cache.activations[l+1]);
                free(cache.z_values[l]);
            }
        }
        
        if (epoch % 10 == 0) {
            printf("Epoch %d, Average Loss: %.6f\n", epoch, total_loss / data->num_samples);
        }
    }
}

// æ•°æ®æ‰“ä¹±å‡½æ•°
void shuffle_data(TrainingData* data) {
    for (int i = data->num_samples - 1; i > 0; i--) {
        int j = rand() % (i + 1);
        
        // äº¤æ¢è¾“å…¥
        for (int k = 0; k < data->input_size; k++) {
            float temp = data->inputs[i * data->input_size + k];
            data->inputs[i * data->input_size + k] = data->inputs[j * data->input_size + k];
            data->inputs[j * data->input_size + k] = temp;
        }
        
        // äº¤æ¢ç›®æ ‡
        for (int k = 0; k < data->output_size; k++) {
            float temp = data->targets[i * data->output_size + k];
            data->targets[i * data->output_size + k] = data->targets[j * data->output_size + k];
            data->targets[j * data->output_size + k] = temp;
        }
    }
}
```

#### å®è·µç»ƒä¹ 

**ç»ƒä¹ 1ï¼šæ¢¯åº¦ä¸‹é™ç®—æ³•å¯¹æ¯”**

```c
// æ¢¯åº¦ä¸‹é™ç®—æ³•å¯¹æ¯”æµ‹è¯•
void test_gradient_descent_algorithms() {
    printf("=== æ¢¯åº¦ä¸‹é™ç®—æ³•å¯¹æ¯”æµ‹è¯• ===\n");
    
    // åˆ›å»ºç½‘ç»œ
    int layer_sizes[] = {2, 4, 1};
    NeuralNetwork* nn_bgd = nn_create(layer_sizes, 3, 0.01f);
    NeuralNetwork* nn_sgd = nn_create(layer_sizes, 3, 0.01f);
    NeuralNetwork* nn_mbgd = nn_create(layer_sizes, 3, 0.01f);
    
    // ç”Ÿæˆè®­ç»ƒæ•°æ®
    const int num_samples = 100;
    TrainingData* data = malloc(sizeof(TrainingData));
    data->num_samples = num_samples;
    data->input_size = 2;
    data->output_size = 1;
    data->inputs = malloc(num_samples * 2 * sizeof(float));
    data->targets = malloc(num_samples * sizeof(float));
    
    for (int i = 0; i < num_samples; i++) {
        float x1 = ((float)rand() / RAND_MAX - 0.5f) * 2.0f;
        float x2 = ((float)rand() / RAND_MAX - 0.5f) * 2.0f;
        
        data->inputs[i * 2] = x1;
        data->inputs[i * 2 + 1] = x2;
        data->targets[i] = 2.0f * x1 + 3.0f * x2 + 1.0f;
    }
    
    // æµ‹è¯•æ‰¹é‡æ¢¯åº¦ä¸‹é™
    printf("æµ‹è¯•æ‰¹é‡æ¢¯åº¦ä¸‹é™...\n");
    clock_t start = clock();
    batch_gradient_descent(nn_bgd, data, 50);
    clock_t end = clock();
    double bgd_time = ((double)(end - start)) / CLOCKS_PER_SEC;
    printf("BGDè®­ç»ƒæ—¶é—´: %.4f ç§’\n", bgd_time);
    
    // æµ‹è¯•éšæœºæ¢¯åº¦ä¸‹é™
    printf("æµ‹è¯•éšæœºæ¢¯åº¦ä¸‹é™...\n");
    start = clock();
    stochastic_gradient_descent(nn_sgd, data, 50);
    end = clock();
    double sgd_time = ((double)(end - start)) / CLOCKS_PER_SEC;
    printf("SGDè®­ç»ƒæ—¶é—´: %.4f ç§’\n", sgd_time);
    
    // æµ‹è¯•å°æ‰¹é‡æ¢¯åº¦ä¸‹é™
    printf("æµ‹è¯•å°æ‰¹é‡æ¢¯åº¦ä¸‹é™...\n");
    start = clock();
    mini_batch_gradient_descent(nn_mbgd, data, 10, 50);
    end = clock();
    double mbgd_time = ((double)(end - start)) / CLOCKS_PER_SEC;
    printf("MBGDè®­ç»ƒæ—¶é—´: %.4f ç§’\n", mbgd_time);
    
    // æµ‹è¯•è®­ç»ƒç»“æœ
    float test_input[] = {1.0f, 1.0f};
    float expected = 2.0f * test_input[0] + 3.0f * test_input[1] + 1.0f;
    
    float output_bgd[1], output_sgd[1], output_mbgd[1];
    nn_forward(nn_bgd, test_input, output_bgd);
    nn_forward(nn_sgd, test_input, output_sgd);
    nn_forward(nn_mbgd, test_input, output_mbgd);
    
    printf("\næµ‹è¯•ç»“æœå¯¹æ¯”:\n");
    printf("æœŸæœ›è¾“å‡º: %.4f\n", expected);
    printf("BGDè¾“å‡º: %.4f (è¯¯å·®: %.4f)\n", output_bgd[0], fabsf(output_bgd[0] - expected));
    printf("SGDè¾“å‡º: %.4f (è¯¯å·®: %.4f)\n", output_sgd[0], fabsf(output_sgd[0] - expected));
    printf("MBGDè¾“å‡º: %.4f (è¯¯å·®: %.4f)\n", output_mbgd[0], fabsf(output_mbgd[0] - expected));
    
    // æ¸…ç†
    nn_free(nn_bgd);
    nn_free(nn_sgd);
    nn_free(nn_mbgd);
    free(data->inputs);
    free(data->targets);
    free(data);
    
    printf("\n");
}

// å°æ‰¹é‡æ¢¯åº¦ä¸‹é™å®ç°
void mini_batch_gradient_descent(NeuralNetwork* nn, TrainingData* data, int batch_size, int epochs) {
    int num_batches = data->num_samples / batch_size;
    
    for (int epoch = 0; epoch < epochs; epoch++) {
        float total_loss = 0;
        
        // éšæœºæ‰“ä¹±æ•°æ®
        shuffle_data(data);
        
        for (int batch = 0; batch < num_batches; batch++) {
            // è·å–å½“å‰æ‰¹æ¬¡æ•°æ®
            int start_idx = batch * batch_size;
            float* batch_input = &data->inputs[start_idx * data->input_size];
            float* batch_target = &data->targets[start_idx * data->output_size];
            
            // æ‰¹é‡å‰å‘ä¼ æ’­
            float* batch_predictions = malloc(data->output_size * batch_size * sizeof(float));
            for (int i = 0; i < batch_size; i++) {
                nn_forward(nn, &batch_input[i * data->input_size], &batch_predictions[i * data->output_size]);
            }
            
            // è®¡ç®—æ‰¹é‡æŸå¤±
            float loss = mse_loss(batch_predictions, batch_target, data->output_size * batch_size);
            total_loss += loss;
            
            // æ‰¹é‡åå‘ä¼ æ’­
            for (int i = 0; i < batch_size; i++) {
                ForwardCache cache;
                cache.num_layers = nn->num_layers;
                
                float output[1];
                nn_forward_with_cache(nn, &batch_input[i * data->input_size], output, &cache);
                nn_backward(nn, &batch_input[i * data->input_size], &batch_target[i * data->output_size], &cache);
                
                // æ¸…ç†ç¼“å­˜
                for (int l = 0; l < nn->num_layers; l++) {
                    free(cache.activations[l+1]);
                    free(cache.z_values[l]);
                }
            }
            
            // æ›´æ–°å‚æ•°
            update_network_parameters(nn, nn->learning_rate);
            
            free(batch_predictions);
        }
        
        if (epoch % 10 == 0) {
            printf("Epoch %d, Average Loss: %.6f\n", epoch, total_loss / num_batches);
        }
    }
}
```

**ç»ƒä¹ 2ï¼šå­¦ä¹ ç‡è°ƒåº¦æµ‹è¯•**

```c
// å­¦ä¹ ç‡è°ƒåº¦æµ‹è¯•
void test_learning_rate_scheduling() {
    printf("=== å­¦ä¹ ç‡è°ƒåº¦æµ‹è¯• ===\n");
    
    // åˆ›å»ºç½‘ç»œ
    int layer_sizes[] = {2, 4, 1};
    NeuralNetwork* nn = nn_create(layer_sizes, 3, 0.1f);
    
    // ç”Ÿæˆè®­ç»ƒæ•°æ®
    const int num_samples = 50;
    TrainingData* data = malloc(sizeof(TrainingData));
    data->num_samples = num_samples;
    data->input_size = 2;
    data->output_size = 1;
    data->inputs = malloc(num_samples * 2 * sizeof(float));
    data->targets = malloc(num_samples * sizeof(float));
    
    for (int i = 0; i < num_samples; i++) {
        float x1 = ((float)rand() / RAND_MAX - 0.5f) * 2.0f;
        float x2 = ((float)rand() / RAND_MAX - 0.5f) * 2.0f;
        
        data->inputs[i * 2] = x1;
        data->inputs[i * 2 + 1] = x2;
        data->targets[i] = 2.0f * x1 + 3.0f * x2 + 1.0f;
    }
    
    // æµ‹è¯•å›ºå®šå­¦ä¹ ç‡
    printf("æµ‹è¯•å›ºå®šå­¦ä¹ ç‡...\n");
    float fixed_lr = 0.01f;
    nn->learning_rate = fixed_lr;
    
    for (int epoch = 0; epoch < 30; epoch++) {
        float total_loss = 0;
        
        for (int i = 0; i < num_samples; i++) {
            float* input = &data->inputs[i * 2];
            float* target = &data->targets[i];
            
            float output[1];
            nn_forward(nn, input, output);
            
            float loss = mse_loss(output, target, 1);
            total_loss += loss;
            
            ForwardCache cache;
            cache.num_layers = nn->num_layers;
            nn_forward_with_cache(nn, input, output, &cache);
            nn_backward(nn, input, target, &cache);
            update_network_parameters(nn, fixed_lr);
            
            for (int l = 0; l < nn->num_layers; l++) {
                free(cache.activations[l+1]);
                free(cache.z_values[l]);
            }
        }
        
        if (epoch % 10 == 0) {
            printf("Epoch %d, LR=%.4f, Loss=%.6f\n", epoch, fixed_lr, total_loss / num_samples);
        }
    }
    
    // æµ‹è¯•å­¦ä¹ ç‡è¡°å‡
    printf("\næµ‹è¯•å­¦ä¹ ç‡è¡°å‡...\n");
    NeuralNetwork* nn_decay = nn_create(layer_sizes, 3, 0.1f);
    float initial_lr = 0.1f;
    float decay_rate = 0.95f;
    
    for (int epoch = 0; epoch < 30; epoch++) {
        float current_lr = initial_lr * powf(decay_rate, epoch);
        nn_decay->learning_rate = current_lr;
        
        float total_loss = 0;
        
        for (int i = 0; i < num_samples; i++) {
            float* input = &data->inputs[i * 2];
            float* target = &data->targets[i];
            
            float output[1];
            nn_forward(nn_decay, input, output);
            
            float loss = mse_loss(output, target, 1);
            total_loss += loss;
            
            ForwardCache cache;
            cache.num_layers = nn_decay->num_layers;
            nn_forward_with_cache(nn_decay, input, output, &cache);
            nn_backward(nn_decay, input, target, &cache);
            update_network_parameters(nn_decay, current_lr);
            
            for (int l = 0; l < nn_decay->num_layers; l++) {
                free(cache.activations[l+1]);
                free(cache.z_values[l]);
            }
        }
        
        if (epoch % 10 == 0) {
            printf("Epoch %d, LR=%.4f, Loss=%.6f\n", epoch, current_lr, total_loss / num_samples);
        }
    }
    
    // æ¸…ç†
    nn_free(nn);
    nn_free(nn_decay);
    free(data->inputs);
    free(data->targets);
    free(data);
    
    printf("\n");
}
```

**ç»ƒä¹ 3ï¼šæ‰¹é‡å¤§å°å½±å“æµ‹è¯•**

```c
// æ‰¹é‡å¤§å°å½±å“æµ‹è¯•
void test_batch_size_impact() {
    printf("=== æ‰¹é‡å¤§å°å½±å“æµ‹è¯• ===\n");
    
    // åˆ›å»ºç½‘ç»œ
    int layer_sizes[] = {2, 4, 1};
    NeuralNetwork* nn = nn_create(layer_sizes, 3, 0.01f);
    
    // ç”Ÿæˆè®­ç»ƒæ•°æ®
    const int num_samples = 100;
    TrainingData* data = malloc(sizeof(TrainingData));
    data->num_samples = num_samples;
    data->input_size = 2;
    data->output_size = 1;
    data->inputs = malloc(num_samples * 2 * sizeof(float));
    data->targets = malloc(num_samples * sizeof(float));
    
    for (int i = 0; i < num_samples; i++) {
        float x1 = ((float)rand() / RAND_MAX - 0.5f) * 2.0f;
        float x2 = ((float)rand() / RAND_MAX - 0.5f) * 2.0f;
        
        data->inputs[i * 2] = x1;
        data->inputs[i * 2 + 1] = x2;
        data->targets[i] = 2.0f * x1 + 3.0f * x2 + 1.0f;
    }
    
    // æµ‹è¯•ä¸åŒæ‰¹é‡å¤§å°
    int batch_sizes[] = {1, 5, 10, 20, 50};
    int num_batch_sizes = sizeof(batch_sizes) / sizeof(batch_sizes[0]);
    
    for (int b = 0; b < num_batch_sizes; b++) {
        int batch_size = batch_sizes[b];
        printf("æµ‹è¯•æ‰¹é‡å¤§å°: %d\n", batch_size);
        
        // é‡æ–°åˆ›å»ºç½‘ç»œ
        NeuralNetwork* nn_test = nn_create(layer_sizes, 3, 0.01f);
        
        clock_t start = clock();
        mini_batch_gradient_descent(nn_test, data, batch_size, 20);
        clock_t end = clock();
        double training_time = ((double)(end - start)) / CLOCKS_PER_SEC;
        
        // æµ‹è¯•ç»“æœ
        float test_input[] = {1.0f, 1.0f};
        float expected = 2.0f * test_input[0] + 3.0f * test_input[1] + 1.0f;
        float output[1];
        nn_forward(nn_test, test_input, output);
        float error = fabsf(output[0] - expected);
        
        printf("  è®­ç»ƒæ—¶é—´: %.4f ç§’\n", training_time);
        printf("  é¢„æµ‹è¯¯å·®: %.4f\n", error);
        
        nn_free(nn_test);
    }
    
    // æ¸…ç†
    free(data->inputs);
    free(data->targets);
    free(data);
    
    printf("\n");
}
```

**ç»ƒä¹ 4ï¼šæ”¶æ•›æ€§åˆ†ææµ‹è¯•**

```c
// æ”¶æ•›æ€§åˆ†ææµ‹è¯•
void test_convergence_analysis() {
    printf("=== æ”¶æ•›æ€§åˆ†ææµ‹è¯• ===\n");
    
    // åˆ›å»ºç½‘ç»œ
    int layer_sizes[] = {2, 4, 1};
    NeuralNetwork* nn = nn_create(layer_sizes, 3, 0.01f);
    
    // ç”Ÿæˆè®­ç»ƒæ•°æ®
    const int num_samples = 50;
    TrainingData* data = malloc(sizeof(TrainingData));
    data->num_samples = num_samples;
    data->input_size = 2;
    data->output_size = 1;
    data->inputs = malloc(num_samples * 2 * sizeof(float));
    data->targets = malloc(num_samples * sizeof(float));
    
    for (int i = 0; i < num_samples; i++) {
        float x1 = ((float)rand() / RAND_MAX - 0.5f) * 2.0f;
        float x2 = ((float)rand() / RAND_MAX - 0.5f) * 2.0f;
        
        data->inputs[i * 2] = x1;
        data->inputs[i * 2 + 1] = x2;
        data->targets[i] = 2.0f * x1 + 3.0f * x2 + 1.0f;
    }
    
    // è®°å½•æŸå¤±å˜åŒ–
    const int epochs = 100;
    float* losses = malloc(epochs * sizeof(float));
    
    printf("è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±å˜åŒ–:\n");
    for (int epoch = 0; epoch < epochs; epoch++) {
        float total_loss = 0;
        
        for (int i = 0; i < num_samples; i++) {
            float* input = &data->inputs[i * 2];
            float* target = &data->targets[i];
            
            float output[1];
            nn_forward(nn, input, output);
            
            float loss = mse_loss(output, target, 1);
            total_loss += loss;
            
            ForwardCache cache;
            cache.num_layers = nn->num_layers;
            nn_forward_with_cache(nn, input, output, &cache);
            nn_backward(nn, input, target, &cache);
            update_network_parameters(nn, nn->learning_rate);
            
            for (int l = 0; l < nn->num_layers; l++) {
                free(cache.activations[l+1]);
                free(cache.z_values[l]);
            }
        }
        
        float avg_loss = total_loss / num_samples;
        losses[epoch] = avg_loss;
        
        if (epoch % 20 == 0) {
            printf("Epoch %d: Loss = %.6f\n", epoch, avg_loss);
        }
    }
    
    // åˆ†ææ”¶æ•›æ€§
    printf("\næ”¶æ•›æ€§åˆ†æ:\n");
    float initial_loss = losses[0];
    float final_loss = losses[epochs-1];
    float convergence_rate = (initial_loss - final_loss) / initial_loss;
    
    printf("åˆå§‹æŸå¤±: %.6f\n", initial_loss);
    printf("æœ€ç»ˆæŸå¤±: %.6f\n", final_loss);
    printf("æ”¶æ•›ç‡: %.2f%%\n", convergence_rate * 100);
    
    // æ£€æŸ¥æ˜¯å¦æ”¶æ•›
    bool is_converged = false;
    for (int i = epochs - 10; i < epochs; i++) {
        if (fabsf(losses[i] - losses[i-1]) < 1e-6) {
            is_converged = true;
            break;
        }
    }
    
    if (is_converged) {
        printf("âœ“ ç®—æ³•å·²æ”¶æ•›\n");
    } else {
        printf("âœ— ç®—æ³•æœªæ”¶æ•›\n");
    }
    
    // æ¸…ç†
    free(losses);
    nn_free(nn);
    free(data->inputs);
    free(data->targets);
    free(data);
    
    printf("\n");
}
```

**ç»ƒä¹ 5ï¼šä¸»æµ‹è¯•ç¨‹åº**

```c
// ä¸»æµ‹è¯•ç¨‹åº
int main() {
    printf("æ¢¯åº¦ä¸‹é™ä¼˜åŒ–ç®—æ³•æµ‹è¯•ç¨‹åº\n");
    printf("======================\n\n");
    
    // è®¾ç½®éšæœºç§å­
    srand(time(NULL));
    
    // è¿è¡Œæ‰€æœ‰æµ‹è¯•
    test_gradient_descent_algorithms();
    test_learning_rate_scheduling();
    test_batch_size_impact();
    test_convergence_analysis();
    
    printf("æ‰€æœ‰æµ‹è¯•å®Œæˆï¼\n");
    return 0;
}
```

### ç¬¬6å‘¨ï¼šé«˜çº§ä¼˜åŒ–æŠ€æœ¯

#### å­¦ä¹ å†…å®¹
- **åŠ¨é‡ä¼˜åŒ–**
- **Adamä¼˜åŒ–å™¨**
- **æ­£åˆ™åŒ–æŠ€æœ¯**

#### ç†è®ºçŸ¥è¯†

**1. åŠ¨é‡ä¼˜åŒ–**
```
v = Î²v + (1-Î²)âˆ‡J(Î¸)
Î¸ = Î¸ - Î±v
å…¶ä¸­Î²æ˜¯åŠ¨é‡ç³»æ•°(é€šå¸¸0.9)
```

**2. Adamä¼˜åŒ–å™¨**
```
m = Î²â‚m + (1-Î²â‚)âˆ‡J(Î¸)  // ä¸€é˜¶çŸ©ä¼°è®¡
v = Î²â‚‚v + (1-Î²â‚‚)(âˆ‡J(Î¸))Â²  // äºŒé˜¶çŸ©ä¼°è®¡
mÌ‚ = m / (1-Î²â‚áµ—)
vÌ‚ = v / (1-Î²â‚‚áµ—)
Î¸ = Î¸ - Î± * mÌ‚ / (âˆšvÌ‚ + Îµ)
```

**3. æ­£åˆ™åŒ–æŠ€æœ¯**
- **L1æ­£åˆ™åŒ–**: L = Lâ‚€ + Î»Î£|w|
- **L2æ­£åˆ™åŒ–**: L = Lâ‚€ + Î»Î£wÂ²
- **Dropout**: éšæœºä¸¢å¼ƒç¥ç»å…ƒ

**4. é«˜çº§ä¼˜åŒ–ç®—æ³•çš„ç†è®ºåˆ†æ**

**åŠ¨é‡ä¼˜åŒ–çš„ç‰©ç†æ„ä¹‰**ï¼š
- æ¨¡æ‹Ÿç‰©ç†ä¸­çš„åŠ¨é‡æ¦‚å¿µ
- å¸®åŠ©é€ƒç¦»å±€éƒ¨æœ€ä¼˜å’Œéç‚¹
- åŠ é€Ÿæ”¶æ•›è¿‡ç¨‹

**Adamä¼˜åŒ–å™¨çš„ä¼˜åŠ¿**ï¼š
- è‡ªé€‚åº”å­¦ä¹ ç‡
- åå·®ä¿®æ­£
- ç»“åˆåŠ¨é‡å’ŒRMSpropçš„ä¼˜ç‚¹

**æ­£åˆ™åŒ–çš„ä½œç”¨æœºåˆ¶**ï¼š
- **L1æ­£åˆ™åŒ–**: äº§ç”Ÿç¨€ç–è§£ï¼Œç‰¹å¾é€‰æ‹©
- **L2æ­£åˆ™åŒ–**: é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œæƒé‡è¡°å‡
- **Dropout**: é˜²æ­¢ç¥ç»å…ƒå…±é€‚åº”

**5. åŠ¨é‡ä¼˜åŒ–çš„ç†è®ºåŸºç¡€**

**ç‰©ç†ç±»æ¯”**ï¼š
- **ç‰›é¡¿ç¬¬äºŒå®šå¾‹**ï¼šF = maï¼ŒåŠ›äº§ç”ŸåŠ é€Ÿåº¦
- **åŠ¨é‡å®ˆæ’**ï¼šp = mvï¼ŒåŠ¨é‡åœ¨ç¢°æ’ä¸­å®ˆæ’
- **ä¼˜åŒ–ç±»æ¯”**ï¼šæ¢¯åº¦ç›¸å½“äºåŠ›ï¼ŒåŠ¨é‡å¸®åŠ©é€ƒç¦»å±€éƒ¨æœ€ä¼˜

**æ•°å­¦åˆ†æ**ï¼š
- **æ”¶æ•›é€Ÿåº¦**ï¼šåŠ¨é‡æ–¹æ³•é€šå¸¸æ¯”æ ‡å‡†SGDæ”¶æ•›æ›´å¿«
- **ç¨³å®šæ€§**ï¼šåŠ¨é‡å‡å°‘æ¢¯åº¦å™ªå£°çš„å½±å“
- **é€ƒç¦»å±€éƒ¨æœ€ä¼˜**ï¼šåŠ¨é‡å¸®åŠ©ç®—æ³•é€ƒç¦»æµ…çš„å±€éƒ¨æœ€ä¼˜

**6. Adamä¼˜åŒ–å™¨çš„æ•°å­¦åŸç†**

**è‡ªé€‚åº”å­¦ä¹ ç‡**ï¼š
- **åŸç†**ï¼šæ ¹æ®å†å²æ¢¯åº¦è°ƒæ•´æ¯ä¸ªå‚æ•°çš„å­¦ä¹ ç‡
- **ä¼˜åŠ¿**ï¼šä¸åŒå‚æ•°ä½¿ç”¨ä¸åŒçš„å­¦ä¹ ç‡
- **å®ç°**ï¼šä½¿ç”¨æ¢¯åº¦çš„ç§»åŠ¨å¹³å‡å’Œå¹³æ–¹

**åå·®ä¿®æ­£**ï¼š
- **é—®é¢˜**ï¼šåˆå§‹æ—¶åˆ»çš„ç§»åŠ¨å¹³å‡æœ‰åå·®
- **è§£å†³**ï¼šä½¿ç”¨åå·®ä¿®æ­£é¡¹
- **å…¬å¼**ï¼šmÌ‚ = m / (1-Î²â‚áµ—), vÌ‚ = v / (1-Î²â‚‚áµ—)

**7. æ­£åˆ™åŒ–æŠ€æœ¯çš„ç†è®ºåˆ†æ**

**L1æ­£åˆ™åŒ–ï¼ˆLassoï¼‰**ï¼š
- **æ•°å­¦å½¢å¼**ï¼šL = Lâ‚€ + Î»Î£|w|
- **å‡ ä½•è§£é‡Š**ï¼šåœ¨L1çƒé¢ä¸Šå¯»æ‰¾æœ€ä¼˜è§£
- **ç¨€ç–æ€§**ï¼šäº§ç”Ÿç¨€ç–è§£ï¼Œè‡ªåŠ¨ç‰¹å¾é€‰æ‹©
- **é€‚ç”¨åœºæ™¯**ï¼šç‰¹å¾é€‰æ‹©ï¼Œå‹ç¼©æ„ŸçŸ¥

**L1æ­£åˆ™åŒ–çš„å…·ä½“ä¾‹å­**ï¼š
- **åŸºå› è¡¨è¾¾åˆ†æ**ï¼šä»æ•°åƒä¸ªåŸºå› ä¸­æ‰¾å‡ºä¸ç–¾ç—…ç›¸å…³çš„åŸºå› 
  - è¾“å…¥ï¼šæ•°åƒä¸ªåŸºå› çš„è¡¨è¾¾æ°´å¹³
  - è¾“å‡ºï¼šç–¾ç—…é£é™©é¢„æµ‹
  - L1æ­£åˆ™åŒ–ï¼šè‡ªåŠ¨é€‰æ‹©æœ€é‡è¦çš„å‡ ä¸ªåŸºå› 
  - ç»“æœï¼šåªæœ‰å°‘æ•°åŸºå› çš„æƒé‡ä¸ä¸ºé›¶ï¼Œå…¶ä»–åŸºå› æƒé‡ä¸ºé›¶
  - å®é™…æ„ä¹‰ï¼šé™ä½æ£€æµ‹æˆæœ¬ï¼Œæé«˜å¯è§£é‡Šæ€§

- **å›¾åƒå‹ç¼©**ï¼šå‹ç¼©å›¾åƒæ•°æ®
  - è¾“å…¥ï¼šå›¾åƒåƒç´ å€¼
  - ç›®æ ‡ï¼šç”¨æœ€å°‘çš„ç³»æ•°é‡å»ºå›¾åƒ
  - L1æ­£åˆ™åŒ–ï¼šé¼“åŠ±ç¨€ç–è¡¨ç¤º
  - ç»“æœï¼šå¤§éƒ¨åˆ†ç³»æ•°ä¸ºé›¶ï¼Œåªæœ‰å°‘æ•°é‡è¦ç³»æ•°

**L2æ­£åˆ™åŒ–ï¼ˆRidgeï¼‰**ï¼š
- **æ•°å­¦å½¢å¼**ï¼šL = Lâ‚€ + Î»Î£wÂ²
- **å‡ ä½•è§£é‡Š**ï¼šåœ¨L2çƒé¢ä¸Šå¯»æ‰¾æœ€ä¼˜è§£
- **æƒé‡è¡°å‡**ï¼šé˜²æ­¢æƒé‡è¿‡å¤§
- **é€‚ç”¨åœºæ™¯**ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œæé«˜æ³›åŒ–èƒ½åŠ›

**L2æ­£åˆ™åŒ–çš„å…·ä½“ä¾‹å­**ï¼š
- **æˆ¿ä»·é¢„æµ‹**ï¼šé˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆè®­ç»ƒæ•°æ®
  - é—®é¢˜ï¼šæ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šè¡¨ç°å¾ˆå¥½ï¼Œä½†æµ‹è¯•é›†ä¸Šè¡¨ç°å·®
  - åŸå› ï¼šæ¨¡å‹å­¦ä¹ äº†è®­ç»ƒæ•°æ®çš„å™ªå£°
  - L2æ­£åˆ™åŒ–ï¼šé™åˆ¶æƒé‡å¤§å°ï¼Œé˜²æ­¢æ¨¡å‹è¿‡äºå¤æ‚
  - æ•ˆæœï¼šæ¨¡å‹æ³›åŒ–èƒ½åŠ›æé«˜ï¼Œæµ‹è¯•é›†æ€§èƒ½æ”¹å–„

- **è¯­éŸ³è¯†åˆ«**ï¼šå¤„ç†å™ªå£°æ•°æ®
  - è¾“å…¥ï¼šå¸¦å™ªå£°çš„è¯­éŸ³ä¿¡å·
  - ç›®æ ‡ï¼šè¯†åˆ«è¯­éŸ³å†…å®¹
  - L2æ­£åˆ™åŒ–ï¼šä½¿æ¨¡å‹å¯¹å™ªå£°æ›´é²æ£’
  - ç»“æœï¼šæ¨¡å‹ä¸ä¼šè¿‡åº¦æ‹Ÿåˆå™ªå£°

**Dropoutæ­£åˆ™åŒ–çš„å…·ä½“ä¾‹å­**ï¼š
- **å›¾åƒåˆ†ç±»**ï¼šé˜²æ­¢ç¥ç»å…ƒè¿‡åº¦ä¾èµ–ç‰¹å®šç‰¹å¾
  - é—®é¢˜ï¼šæŸäº›ç¥ç»å…ƒåªå­¦ä¹ ç‰¹å®šç‰¹å¾ï¼Œä¸å…¶ä»–ç¥ç»å…ƒä¸åˆä½œ
  - ä¾‹å­ï¼šä¸€ä¸ªç¥ç»å…ƒåªè¯†åˆ«"çŒ«è€³æœµ"ï¼Œå¦ä¸€ä¸ªåªè¯†åˆ«"çŒ«å°¾å·´"
  - Dropoutï¼šéšæœºè®©ä¸€äº›ç¥ç»å…ƒ"ä¼‘æ¯"
  - æ•ˆæœï¼šå…¶ä»–ç¥ç»å…ƒå­¦ä¼šæ›´å…¨é¢çš„ç‰¹å¾è¡¨ç¤º

- **æ–‡æœ¬åˆ†ç±»**ï¼šæé«˜æ¨¡å‹é²æ£’æ€§
  - è¾“å…¥ï¼šæ–‡æœ¬å†…å®¹
  - ç›®æ ‡ï¼šåˆ†ç±»æ–‡æœ¬ç±»å‹
  - Dropoutï¼šè®­ç»ƒæ—¶éšæœºä¸¢å¼ƒä¸€äº›ç¥ç»å…ƒ
  - ç»“æœï¼šæ¨¡å‹ä¸ä¼šè¿‡åº¦ä¾èµ–æŸäº›ç‰¹å®šè¯æ±‡

**æ­£åˆ™åŒ–çš„å®é™…æ„ä¹‰**ï¼š
- **é˜²æ­¢è¿‡æ‹Ÿåˆ**ï¼šå°±åƒå­¦ç”Ÿä¸èƒ½åªèƒŒä¾‹é¢˜ï¼Œè¦ç†è§£åŸç†
  - è¿‡æ‹Ÿåˆï¼šå­¦ç”Ÿåªè®°ä½äº†ä¾‹é¢˜çš„ç­”æ¡ˆ
  - æ­£åˆ™åŒ–ï¼šè¦æ±‚å­¦ç”Ÿç†è§£è§£é¢˜æ€è·¯
- **æé«˜æ³›åŒ–èƒ½åŠ›**ï¼šå°±åƒåŒ»ç”Ÿä¸èƒ½åªè®°ä½ç—‡çŠ¶ï¼Œè¦ç†è§£ç—…ç†
  - è®­ç»ƒæ•°æ®ï¼šå·²çŸ¥ç—…ä¾‹çš„ç—‡çŠ¶å’Œè¯Šæ–­
  - æ­£åˆ™åŒ–ï¼šè®©æ¨¡å‹å­¦ä¹ ç–¾ç—…çš„æœ¬è´¨ç‰¹å¾
- **æ¨¡å‹ç®€åŒ–**ï¼šå°±åƒå·¥ç¨‹å¸ˆä¸èƒ½è®¾è®¡è¿‡äºå¤æ‚çš„ç³»ç»Ÿ
  - å¤æ‚æ¨¡å‹ï¼šå®¹æ˜“å‡ºé”™ï¼Œéš¾ä»¥ç»´æŠ¤
  - æ­£åˆ™åŒ–ï¼šé¼“åŠ±ç®€å•æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆ

**8. é«˜çº§ä¼˜åŒ–ç®—æ³•çš„æ”¶æ•›ç†è®º**

**åŠ¨é‡æ–¹æ³•çš„æ”¶æ•›æ€§**ï¼š
- **å‡¸å‡½æ•°**ï¼šO(1/âˆšT)æ”¶æ•›é€Ÿåº¦
- **å¼ºå‡¸å‡½æ•°**ï¼šçº¿æ€§æ”¶æ•›
- **éå‡¸å‡½æ•°**ï¼šç†è®ºä¸Šéš¾ä»¥ä¿è¯å…¨å±€æ”¶æ•›

**Adamçš„æ”¶æ•›æ€§**ï¼š
- **ç†è®ºä¿è¯**ï¼šåœ¨å‡¸å‡½æ•°ä¸Šæ”¶æ•›
- **å®é™…è¡¨ç°**ï¼šåœ¨æ·±åº¦å­¦ä¹ ä¸­è¡¨ç°è‰¯å¥½
- **è¶…å‚æ•°æ•æ„Ÿæ€§**ï¼šå¯¹Î²â‚ã€Î²â‚‚çš„é€‰æ‹©æ•æ„Ÿ

**9. æ­£åˆ™åŒ–çš„å‡ ä½•è§£é‡Š**

**L1æ­£åˆ™åŒ–çš„å‡ ä½•**ï¼š
- **çº¦æŸåŒºåŸŸ**ï¼šè±å½¢ï¼ˆL1çƒï¼‰
- **æœ€ä¼˜è§£**ï¼šé€šå¸¸åœ¨é¡¶ç‚¹å¤„
- **ç¨€ç–æ€§**ï¼šé¡¶ç‚¹å¯¹åº”ç¨€ç–è§£

**L2æ­£åˆ™åŒ–çš„å‡ ä½•**ï¼š
- **çº¦æŸåŒºåŸŸ**ï¼šåœ†å½¢ï¼ˆL2çƒï¼‰
- **æœ€ä¼˜è§£**ï¼šé€šå¸¸åœ¨å†…éƒ¨
- **å¹³æ»‘æ€§**ï¼šäº§ç”Ÿå¹³æ»‘çš„è§£

**10. é«˜çº§ä¼˜åŒ–ç®—æ³•çš„è¶…å‚æ•°è°ƒä¼˜**

**åŠ¨é‡ç³»æ•°Î²**ï¼š
- **å…¸å‹å€¼**ï¼š0.9
- **å½±å“**ï¼šÎ²è¶Šå¤§ï¼ŒåŠ¨é‡è¶Šå¼º
- **è°ƒä¼˜**ï¼šæ ¹æ®é—®é¢˜ç‰¹æ€§è°ƒæ•´

**Adamè¶…å‚æ•°**ï¼š
- **Î²â‚**ï¼šä¸€é˜¶çŸ©ä¼°è®¡çš„è¡°å‡ç‡ï¼ˆ0.9ï¼‰
- **Î²â‚‚**ï¼šäºŒé˜¶çŸ©ä¼°è®¡çš„è¡°å‡ç‡ï¼ˆ0.999ï¼‰
- **Îµ**ï¼šæ•°å€¼ç¨³å®šæ€§å¸¸æ•°ï¼ˆ1e-8ï¼‰

**æ­£åˆ™åŒ–å¼ºåº¦Î»**ï¼š
- **L1æ­£åˆ™åŒ–**ï¼šÎ»æ§åˆ¶ç¨€ç–æ€§
- **L2æ­£åˆ™åŒ–**ï¼šÎ»æ§åˆ¶æƒé‡è¡°å‡å¼ºåº¦
- **Dropout**ï¼špæ§åˆ¶ä¸¢å¼ƒæ¦‚ç‡

**11. é«˜çº§ä¼˜åŒ–ç®—æ³•çš„æ•°å€¼ç¨³å®šæ€§**

**æ¢¯åº¦è£å‰ª**ï¼š
- **åŸç†**ï¼šé™åˆ¶æ¢¯åº¦èŒƒæ•°
- **å®ç°**ï¼šif ||âˆ‡J(Î¸)|| > threshold, âˆ‡J(Î¸) = threshold * âˆ‡J(Î¸)/||âˆ‡J(Î¸)||
- **ä½œç”¨**ï¼šé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸

**æ•°å€¼ç²¾åº¦**ï¼š
- **å•ç²¾åº¦**ï¼š32ä½æµ®ç‚¹æ•°ï¼Œè®¡ç®—å¿«ä½†ç²¾åº¦ä½
- **åŒç²¾åº¦**ï¼š64ä½æµ®ç‚¹æ•°ï¼Œç²¾åº¦é«˜ä½†è®¡ç®—æ…¢
- **æ··åˆç²¾åº¦**ï¼šç»“åˆä¸¤ç§ç²¾åº¦çš„ä¼˜åŠ¿

**12. é«˜çº§ä¼˜åŒ–ç®—æ³•çš„å¹¶è¡ŒåŒ–**

**æ•°æ®å¹¶è¡Œ**ï¼š
- **åŸç†**ï¼šå°†æ•°æ®åˆ†é…åˆ°å¤šä¸ªå¤„ç†å™¨
- **å®ç°**ï¼šæ¯ä¸ªå¤„ç†å™¨å¤„ç†éƒ¨åˆ†æ•°æ®
- **ä¼˜åŠ¿**ï¼šçº¿æ€§åŠ é€Ÿæ¯”

**æ¨¡å‹å¹¶è¡Œ**ï¼š
- **åŸç†**ï¼šå°†æ¨¡å‹åˆ†é…åˆ°å¤šä¸ªå¤„ç†å™¨
- **å®ç°**ï¼šä¸åŒå¤„ç†å™¨å¤„ç†ä¸åŒå±‚
- **ä¼˜åŠ¿**ï¼šå¤„ç†å¤§æ¨¡å‹

**13. é«˜çº§ä¼˜åŒ–ç®—æ³•çš„ç”Ÿç‰©å­¦å¯å‘**

**èµ«å¸ƒå­¦ä¹ **ï¼š
- **åŸç†**ï¼š"ä¸€èµ·æ¿€æ´»çš„ç¥ç»å…ƒè¿æ¥ä¼šå¢å¼º"
- **æ•°å­¦è¡¨ç¤º**ï¼šÎ”w = Î· * pre_activation * post_error
- **åŠ¨é‡ç±»æ¯”**ï¼šæ¨¡æ‹Ÿçªè§¦å¼ºåº¦çš„ç´¯ç§¯æ•ˆåº”

**çªè§¦å¯å¡‘æ€§**ï¼š
- **é•¿æ—¶ç¨‹å¢å¼º(LTP)**ï¼šçªè§¦å¼ºåº¦å¢åŠ 
- **é•¿æ—¶ç¨‹æŠ‘åˆ¶(LTD)**ï¼šçªè§¦å¼ºåº¦å‡å°‘
- **è‡ªé€‚åº”å­¦ä¹ **ï¼šæ¨¡æ‹Ÿçªè§¦çš„è‡ªé€‚åº”è°ƒæ•´

**14. é«˜çº§ä¼˜åŒ–ç®—æ³•çš„å†å²å‘å±•**

**æ—©æœŸå‘å±•**ï¼š
- **1980å¹´ä»£**ï¼šåŠ¨é‡æ–¹æ³•å¼•å…¥
- **1990å¹´ä»£**ï¼šè‡ªé€‚åº”æ–¹æ³•å‘å±•
- **2000å¹´ä»£**ï¼šæ·±åº¦å­¦ä¹ ä¼˜åŒ–ç®—æ³•

**ç°ä»£å‘å±•**ï¼š
- **2010å¹´ä»£**ï¼šAdamæˆä¸ºæ ‡å‡†
- **2015å¹´å**ï¼šå„ç§Adamå˜ä½“
- **2020å¹´ä»£**ï¼šäºŒé˜¶æ–¹æ³•å¤å…´

**æœ€æ–°è¶‹åŠ¿**ï¼š
- **è‡ªé€‚åº”ä¼˜åŒ–**ï¼šAdamã€AdaBeliefç­‰
- **äºŒé˜¶æ–¹æ³•**ï¼šK-FACã€Shampooç­‰
- **åˆ†å¸ƒå¼ä¼˜åŒ–**ï¼šå¼‚æ­¥SGDã€æ¨¡å‹å¹¶è¡Œç­‰

**15. é«˜çº§ä¼˜åŒ–ç®—æ³•çš„é€‰æ‹©æŒ‡å—**

**é—®é¢˜ç‰¹æ€§**ï¼š
- **å‡¸ä¼˜åŒ–**ï¼šæ ‡å‡†æ¢¯åº¦ä¸‹é™æˆ–ç‰›é¡¿æ³•
- **éå‡¸ä¼˜åŒ–**ï¼šAdamæˆ–åŠ¨é‡SGD
- **ç¨€ç–é—®é¢˜**ï¼šL1æ­£åˆ™åŒ–æˆ–AdaGrad

**è®¡ç®—èµ„æº**ï¼š
- **å†…å­˜å—é™**ï¼šSGDæˆ–å°æ‰¹é‡Adam
- **è®¡ç®—å—é™**ï¼šç®€å•æ¢¯åº¦ä¸‹é™
- **é€šä¿¡å—é™**ï¼šæœ¬åœ°ä¼˜åŒ–æ–¹æ³•

**æ”¶æ•›è¦æ±‚**ï¼š
- **å¿«é€Ÿæ”¶æ•›**ï¼šAdamæˆ–åŠ¨é‡æ–¹æ³•
- **ç¨³å®šæ”¶æ•›**ï¼šL2æ­£åˆ™åŒ–æˆ–æ¢¯åº¦è£å‰ª
- **ç²¾ç¡®æ”¶æ•›**ï¼šäºŒé˜¶æ–¹æ³•æˆ–ç²¾ç¡®çº¿æœç´¢

**5. é«˜çº§ä¼˜åŒ–ç®—æ³•çš„Cè¯­è¨€å®ç°**

**åŠ¨é‡ä¼˜åŒ–å™¨å®ç°**ï¼š
```c
// åŠ¨é‡ä¼˜åŒ–å™¨
typedef struct {
    float learning_rate;
    float momentum;
    float* velocity;
    int num_parameters;
} MomentumOptimizer;

MomentumOptimizer* momentum_create(float learning_rate, float momentum, int num_parameters) {
    MomentumOptimizer* optimizer = malloc(sizeof(MomentumOptimizer));
    optimizer->learning_rate = learning_rate;
    optimizer->momentum = momentum;
    optimizer->num_parameters = num_parameters;
    optimizer->velocity = calloc(num_parameters, sizeof(float));
    return optimizer;
}

void momentum_update(MomentumOptimizer* optimizer, float* parameters, float* gradients) {
    for (int i = 0; i < optimizer->num_parameters; i++) {
        optimizer->velocity[i] = optimizer->momentum * optimizer->velocity[i] + 
                                 optimizer->learning_rate * gradients[i];
        parameters[i] -= optimizer->velocity[i];
    }
}

void momentum_free(MomentumOptimizer* optimizer) {
    free(optimizer->velocity);
    free(optimizer);
}
```

**RMSpropä¼˜åŒ–å™¨å®ç°**ï¼š
```c
// RMSpropä¼˜åŒ–å™¨
typedef struct {
    float learning_rate;
    float decay_rate;
    float epsilon;
    float* v;  // ç§»åŠ¨å¹³å‡çš„æ¢¯åº¦å¹³æ–¹
    int num_parameters;
} RMSpropOptimizer;

RMSpropOptimizer* rmsprop_create(float learning_rate, int num_parameters) {
    RMSpropOptimizer* optimizer = malloc(sizeof(RMSpropOptimizer));
    optimizer->learning_rate = learning_rate;
    optimizer->decay_rate = 0.9f;
    optimizer->epsilon = 1e-8f;
    optimizer->num_parameters = num_parameters;
    optimizer->v = calloc(num_parameters, sizeof(float));
    return optimizer;
}

void rmsprop_update(RMSpropOptimizer* optimizer, float* parameters, float* gradients) {
    for (int i = 0; i < optimizer->num_parameters; i++) {
        optimizer->v[i] = optimizer->decay_rate * optimizer->v[i] + 
                          (1.0f - optimizer->decay_rate) * gradients[i] * gradients[i];
        parameters[i] -= optimizer->learning_rate * gradients[i] / 
                        (sqrtf(optimizer->v[i]) + optimizer->epsilon);
    }
}

void rmsprop_free(RMSpropOptimizer* optimizer) {
    free(optimizer->v);
    free(optimizer);
}
```

**æ­£åˆ™åŒ–å®ç°**ï¼š
```c
// L2æ­£åˆ™åŒ–
void l2_regularization(NeuralNetwork* nn, float lambda) {
    for (int l = 0; l < nn->num_layers; l++) {
        Layer* layer = &nn->layers[l];
        int weight_size = layer->output_size * layer->input_size;
        
        for (int i = 0; i < weight_size; i++) {
            layer->weights[i] -= lambda * layer->weights[i];
        }
    }
}

// L1æ­£åˆ™åŒ–
void l1_regularization(NeuralNetwork* nn, float lambda) {
    for (int l = 0; l < nn->num_layers; l++) {
        Layer* layer = &nn->layers[l];
        int weight_size = layer->output_size * layer->input_size;
        
        for (int i = 0; i < weight_size; i++) {
            if (layer->weights[i] > 0) {
                layer->weights[i] -= lambda;
            } else if (layer->weights[i] < 0) {
                layer->weights[i] += lambda;
            }
        }
    }
}

// Dropoutå®ç°
void apply_dropout(float* activations, int size, float dropout_rate) {
    for (int i = 0; i < size; i++) {
        if ((float)rand() / RAND_MAX < dropout_rate) {
            activations[i] = 0.0f;
        } else {
            activations[i] /= (1.0f - dropout_rate);  // ç¼©æ”¾ä¿æŒæœŸæœ›å€¼ä¸å˜
        }
    }
}
```

**6. ä¼˜åŒ–ç®—æ³•æ€§èƒ½å¯¹æ¯”**

**ç†è®ºå¯¹æ¯”**ï¼š
- **SGD**: ç®€å•ä½†æ”¶æ•›æ…¢
- **Momentum**: åŠ é€Ÿæ”¶æ•›ï¼Œé€ƒç¦»å±€éƒ¨æœ€ä¼˜
- **RMSprop**: è‡ªé€‚åº”å­¦ä¹ ç‡ï¼Œé€‚åˆéå‡¸ä¼˜åŒ–
- **Adam**: ç»“åˆåŠ¨é‡å’Œè‡ªé€‚åº”å­¦ä¹ ç‡ï¼Œæœ€å¸¸ç”¨

**å®é™…åº”ç”¨è€ƒè™‘**ï¼š
- **å†…å­˜ä½¿ç”¨**: Adaméœ€è¦å­˜å‚¨æ›´å¤šçŠ¶æ€
- **è®¡ç®—å¤æ‚åº¦**: å„ç®—æ³•è®¡ç®—é‡ç›¸è¿‘
- **è¶…å‚æ•°æ•æ„Ÿæ€§**: Adamç›¸å¯¹ä¸æ•æ„Ÿ

#### å®è·µç»ƒä¹ 

**ç»ƒä¹ 1ï¼šé«˜çº§ä¼˜åŒ–ç®—æ³•å¯¹æ¯”**

```c
// é«˜çº§ä¼˜åŒ–ç®—æ³•å¯¹æ¯”æµ‹è¯•
void test_advanced_optimizers() {
    printf("=== é«˜çº§ä¼˜åŒ–ç®—æ³•å¯¹æ¯”æµ‹è¯• ===\n");
    
    // åˆ›å»ºç½‘ç»œ
    int layer_sizes[] = {2, 4, 1};
    NeuralNetwork* nn_sgd = nn_create(layer_sizes, 3, 0.01f);
    NeuralNetwork* nn_momentum = nn_create(layer_sizes, 3, 0.01f);
    NeuralNetwork* nn_rmsprop = nn_create(layer_sizes, 3, 0.01f);
    NeuralNetwork* nn_adam = nn_create(layer_sizes, 3, 0.01f);
    
    // ç”Ÿæˆè®­ç»ƒæ•°æ®
    const int num_samples = 100;
    TrainingData* data = malloc(sizeof(TrainingData));
    data->num_samples = num_samples;
    data->input_size = 2;
    data->output_size = 1;
    data->inputs = malloc(num_samples * 2 * sizeof(float));
    data->targets = malloc(num_samples * sizeof(float));
    
    for (int i = 0; i < num_samples; i++) {
        float x1 = ((float)rand() / RAND_MAX - 0.5f) * 2.0f;
        float x2 = ((float)rand() / RAND_MAX - 0.5f) * 2.0f;
        
        data->inputs[i * 2] = x1;
        data->inputs[i * 2 + 1] = x2;
        data->targets[i] = 2.0f * x1 + 3.0f * x2 + 1.0f;
    }
    
    // åˆ›å»ºä¼˜åŒ–å™¨
    MomentumOptimizer* momentum = momentum_create(0.01f, 0.9f, 100);
    RMSpropOptimizer* rmsprop = rmsprop_create(0.01f, 100);
    Adam* adam = adam_create(0.01f, 100);
    
    // æµ‹è¯•SGD
    printf("æµ‹è¯•SGD...\n");
    clock_t start = clock();
    train_with_sgd(nn_sgd, data, 50);
    clock_t end = clock();
    double sgd_time = ((double)(end - start)) / CLOCKS_PER_SEC;
    
    // æµ‹è¯•Momentum
    printf("æµ‹è¯•Momentum...\n");
    start = clock();
    train_with_momentum(nn_momentum, data, momentum, 50);
    end = clock();
    double momentum_time = ((double)(end - start)) / CLOCKS_PER_SEC;
    
    // æµ‹è¯•RMSprop
    printf("æµ‹è¯•RMSprop...\n");
    start = clock();
    train_with_rmsprop(nn_rmsprop, data, rmsprop, 50);
    end = clock();
    double rmsprop_time = ((double)(end - start)) / CLOCKS_PER_SEC;
    
    // æµ‹è¯•Adam
    printf("æµ‹è¯•Adam...\n");
    start = clock();
    train_with_adam(nn_adam, data, adam, 50);
    end = clock();
    double adam_time = ((double)(end - start)) / CLOCKS_PER_SEC;
    
    // æµ‹è¯•ç»“æœå¯¹æ¯”
    float test_input[] = {1.0f, 1.0f};
    float expected = 2.0f * test_input[0] + 3.0f * test_input[1] + 1.0f;
    
    float output_sgd[1], output_momentum[1], output_rmsprop[1], output_adam[1];
    nn_forward(nn_sgd, test_input, output_sgd);
    nn_forward(nn_momentum, test_input, output_momentum);
    nn_forward(nn_rmsprop, test_input, output_rmsprop);
    nn_forward(nn_adam, test_input, output_adam);
    
    printf("\næµ‹è¯•ç»“æœå¯¹æ¯”:\n");
    printf("æœŸæœ›è¾“å‡º: %.4f\n", expected);
    printf("SGDè¾“å‡º: %.4f (è¯¯å·®: %.4f, æ—¶é—´: %.4fs)\n", 
           output_sgd[0], fabsf(output_sgd[0] - expected), sgd_time);
    printf("Momentumè¾“å‡º: %.4f (è¯¯å·®: %.4f, æ—¶é—´: %.4fs)\n", 
           output_momentum[0], fabsf(output_momentum[0] - expected), momentum_time);
    printf("RMSpropè¾“å‡º: %.4f (è¯¯å·®: %.4f, æ—¶é—´: %.4fs)\n", 
           output_rmsprop[0], fabsf(output_rmsprop[0] - expected), rmsprop_time);
    printf("Adamè¾“å‡º: %.4f (è¯¯å·®: %.4f, æ—¶é—´: %.4fs)\n", 
           output_adam[0], fabsf(output_adam[0] - expected), adam_time);
    
    // æ¸…ç†
    nn_free(nn_sgd);
    nn_free(nn_momentum);
    nn_free(nn_rmsprop);
    nn_free(nn_adam);
    momentum_free(momentum);
    rmsprop_free(rmsprop);
    adam_free(adam);
    free(data->inputs);
    free(data->targets);
    free(data);
    
    printf("\n");
}

// ä½¿ç”¨ä¸åŒä¼˜åŒ–å™¨çš„è®­ç»ƒå‡½æ•°
void train_with_sgd(NeuralNetwork* nn, TrainingData* data, int epochs) {
    for (int epoch = 0; epoch < epochs; epoch++) {
        float total_loss = 0;
        
        for (int i = 0; i < data->num_samples; i++) {
            float* input = &data->inputs[i * data->input_size];
            float* target = &data->targets[i * data->output_size];
            
            float output[1];
            nn_forward(nn, input, output);
            
            float loss = mse_loss(output, target, 1);
            total_loss += loss;
            
            ForwardCache cache;
            cache.num_layers = nn->num_layers;
            nn_forward_with_cache(nn, input, output, &cache);
            nn_backward(nn, input, target, &cache);
            update_network_parameters(nn, nn->learning_rate);
            
            for (int l = 0; l < nn->num_layers; l++) {
                free(cache.activations[l+1]);
                free(cache.z_values[l]);
            }
        }
        
        if (epoch % 10 == 0) {
            printf("SGD Epoch %d, Loss: %.6f\n", epoch, total_loss / data->num_samples);
        }
    }
}

void train_with_momentum(NeuralNetwork* nn, TrainingData* data, MomentumOptimizer* optimizer, int epochs) {
    for (int epoch = 0; epoch < epochs; epoch++) {
        float total_loss = 0;
        
        for (int i = 0; i < data->num_samples; i++) {
            float* input = &data->inputs[i * data->input_size];
            float* target = &data->targets[i * data->output_size];
            
            float output[1];
            nn_forward(nn, input, output);
            
            float loss = mse_loss(output, target, 1);
            total_loss += loss;
            
            ForwardCache cache;
            cache.num_layers = nn->num_layers;
            nn_forward_with_cache(nn, input, output, &cache);
            nn_backward(nn, input, target, &cache);
            
            // æå–æ¢¯åº¦å¹¶æ›´æ–°
            float* gradients = extract_gradients(nn);
            momentum_update(optimizer, (float*)nn, gradients);
            
            for (int l = 0; l < nn->num_layers; l++) {
                free(cache.activations[l+1]);
                free(cache.z_values[l]);
            }
            free(gradients);
        }
        
        if (epoch % 10 == 0) {
            printf("Momentum Epoch %d, Loss: %.6f\n", epoch, total_loss / data->num_samples);
        }
    }
}
```

**ç»ƒä¹ 2ï¼šæ­£åˆ™åŒ–æŠ€æœ¯æµ‹è¯•**

```c
// æ­£åˆ™åŒ–æŠ€æœ¯æµ‹è¯•
void test_regularization_techniques() {
    printf("=== æ­£åˆ™åŒ–æŠ€æœ¯æµ‹è¯• ===\n");
    
    // åˆ›å»ºç½‘ç»œ
    int layer_sizes[] = {2, 8, 1};  // ä½¿ç”¨è¾ƒå¤§ç½‘ç»œæµ‹è¯•è¿‡æ‹Ÿåˆ
    NeuralNetwork* nn_no_reg = nn_create(layer_sizes, 3, 0.01f);
    NeuralNetwork* nn_l2_reg = nn_create(layer_sizes, 3, 0.01f);
    NeuralNetwork* nn_l1_reg = nn_create(layer_sizes, 3, 0.01f);
    NeuralNetwork* nn_dropout = nn_create(layer_sizes, 3, 0.01f);
    
    // ç”Ÿæˆå°‘é‡è®­ç»ƒæ•°æ®ï¼ˆå®¹æ˜“è¿‡æ‹Ÿåˆï¼‰
    const int num_samples = 20;
    TrainingData* data = malloc(sizeof(TrainingData));
    data->num_samples = num_samples;
    data->input_size = 2;
    data->output_size = 1;
    data->inputs = malloc(num_samples * 2 * sizeof(float));
    data->targets = malloc(num_samples * sizeof(float));
    
    for (int i = 0; i < num_samples; i++) {
        float x1 = ((float)rand() / RAND_MAX - 0.5f) * 2.0f;
        float x2 = ((float)rand() / RAND_MAX - 0.5f) * 2.0f;
        
        data->inputs[i * 2] = x1;
        data->inputs[i * 2 + 1] = x2;
        data->targets[i] = 2.0f * x1 + 3.0f * x2 + 1.0f;
    }
    
    // è®­ç»ƒä¸åŒæ­£åˆ™åŒ–æ–¹æ³•
    printf("è®­ç»ƒæ— æ­£åˆ™åŒ–ç½‘ç»œ...\n");
    train_with_regularization(nn_no_reg, data, 100, 0.0f, 0.0f, 0.0f);
    
    printf("è®­ç»ƒL2æ­£åˆ™åŒ–ç½‘ç»œ...\n");
    train_with_regularization(nn_l2_reg, data, 100, 0.01f, 0.0f, 0.0f);
    
    printf("è®­ç»ƒL1æ­£åˆ™åŒ–ç½‘ç»œ...\n");
    train_with_regularization(nn_l1_reg, data, 100, 0.0f, 0.01f, 0.0f);
    
    printf("è®­ç»ƒDropoutç½‘ç»œ...\n");
    train_with_regularization(nn_dropout, data, 100, 0.0f, 0.0f, 0.3f);
    
    // æµ‹è¯•æ³›åŒ–èƒ½åŠ›
    printf("\næ³›åŒ–èƒ½åŠ›æµ‹è¯•:\n");
    float test_inputs[][2] = {{1.0f, 1.0f}, {0.5f, -0.3f}, {-1.0f, 0.8f}};
    float expected_outputs[] = {6.0f, 1.1f, 0.4f};
    
    for (int i = 0; i < 3; i++) {
        float output_no_reg[1], output_l2[1], output_l1[1], output_dropout[1];
        
        nn_forward(nn_no_reg, test_inputs[i], output_no_reg);
        nn_forward(nn_l2_reg, test_inputs[i], output_l2);
        nn_forward(nn_l1_reg, test_inputs[i], output_l1);
        nn_forward(nn_dropout, test_inputs[i], output_dropout);
        
        printf("æµ‹è¯•è¾“å…¥[%.1f, %.1f], æœŸæœ›: %.1f\n", 
               test_inputs[i][0], test_inputs[i][1], expected_outputs[i]);
        printf("  æ— æ­£åˆ™åŒ–: %.4f (è¯¯å·®: %.4f)\n", 
               output_no_reg[0], fabsf(output_no_reg[0] - expected_outputs[i]));
        printf("  L2æ­£åˆ™åŒ–: %.4f (è¯¯å·®: %.4f)\n", 
               output_l2[0], fabsf(output_l2[0] - expected_outputs[i]));
        printf("  L1æ­£åˆ™åŒ–: %.4f (è¯¯å·®: %.4f)\n", 
               output_l1[0], fabsf(output_l1[0] - expected_outputs[i]));
        printf("  Dropout: %.4f (è¯¯å·®: %.4f)\n", 
               output_dropout[0], fabsf(output_dropout[0] - expected_outputs[i]));
    }
    
    // æ¸…ç†
    nn_free(nn_no_reg);
    nn_free(nn_l2_reg);
    nn_free(nn_l1_reg);
    nn_free(nn_dropout);
    free(data->inputs);
    free(data->targets);
    free(data);
    
    printf("\n");
}

// å¸¦æ­£åˆ™åŒ–çš„è®­ç»ƒå‡½æ•°
void train_with_regularization(NeuralNetwork* nn, TrainingData* data, int epochs, 
                              float l2_lambda, float l1_lambda, float dropout_rate) {
    for (int epoch = 0; epoch < epochs; epoch++) {
        float total_loss = 0;
        
        for (int i = 0; i < data->num_samples; i++) {
            float* input = &data->inputs[i * data->input_size];
            float* target = &data->targets[i * data->output_size];
            
            // å‰å‘ä¼ æ’­ï¼ˆå¸¦Dropoutï¼‰
            ForwardCache cache;
            cache.num_layers = nn->num_layers;
            float output[1];
            nn_forward_with_cache(nn, input, output, &cache);
            
            // åº”ç”¨Dropoutåˆ°éšè—å±‚
            if (dropout_rate > 0) {
                for (int l = 1; l < nn->num_layers; l++) {
                    apply_dropout(cache.activations[l], nn->layers[l-1].output_size, dropout_rate);
                }
            }
            
            float loss = mse_loss(output, target, 1);
            total_loss += loss;
            
            // åå‘ä¼ æ’­
            nn_backward(nn, input, target, &cache);
            
            // åº”ç”¨æ­£åˆ™åŒ–
            if (l2_lambda > 0) {
                l2_regularization(nn, l2_lambda);
            }
            if (l1_lambda > 0) {
                l1_regularization(nn, l1_lambda);
            }
            
            update_network_parameters(nn, nn->learning_rate);
            
            for (int l = 0; l < nn->num_layers; l++) {
                free(cache.activations[l+1]);
                free(cache.z_values[l]);
            }
        }
        
        if (epoch % 20 == 0) {
            printf("Epoch %d, Loss: %.6f\n", epoch, total_loss / data->num_samples);
        }
    }
}
```

**ç»ƒä¹ 3ï¼šä¼˜åŒ–ç®—æ³•è¶…å‚æ•°æ•æ„Ÿæ€§æµ‹è¯•**

```c
// ä¼˜åŒ–ç®—æ³•è¶…å‚æ•°æ•æ„Ÿæ€§æµ‹è¯•
void test_hyperparameter_sensitivity() {
    printf("=== è¶…å‚æ•°æ•æ„Ÿæ€§æµ‹è¯• ===\n");
    
    // åˆ›å»ºç½‘ç»œ
    int layer_sizes[] = {2, 4, 1};
    NeuralNetwork* nn = nn_create(layer_sizes, 3, 0.01f);
    
    // ç”Ÿæˆè®­ç»ƒæ•°æ®
    const int num_samples = 50;
    TrainingData* data = malloc(sizeof(TrainingData));
    data->num_samples = num_samples;
    data->input_size = 2;
    data->output_size = 1;
    data->inputs = malloc(num_samples * 2 * sizeof(float));
    data->targets = malloc(num_samples * sizeof(float));
    
    for (int i = 0; i < num_samples; i++) {
        float x1 = ((float)rand() / RAND_MAX - 0.5f) * 2.0f;
        float x2 = ((float)rand() / RAND_MAX - 0.5f) * 2.0f;
        
        data->inputs[i * 2] = x1;
        data->inputs[i * 2 + 1] = x2;
        data->targets[i] = 2.0f * x1 + 3.0f * x2 + 1.0f;
    }
    
    // æµ‹è¯•ä¸åŒå­¦ä¹ ç‡
    float learning_rates[] = {0.001f, 0.01f, 0.1f, 0.5f};
    int num_lrs = sizeof(learning_rates) / sizeof(learning_rates[0]);
    
    printf("å­¦ä¹ ç‡æ•æ„Ÿæ€§æµ‹è¯•:\n");
    for (int i = 0; i < num_lrs; i++) {
        NeuralNetwork* nn_test = nn_create(layer_sizes, 3, learning_rates[i]);
        
        float final_loss = train_and_evaluate(nn_test, data, 30);
        printf("å­¦ä¹ ç‡ %.3f: æœ€ç»ˆæŸå¤± %.6f\n", learning_rates[i], final_loss);
        
        nn_free(nn_test);
    }
    
    // æµ‹è¯•ä¸åŒåŠ¨é‡ç³»æ•°
    printf("\nåŠ¨é‡ç³»æ•°æ•æ„Ÿæ€§æµ‹è¯•:\n");
    float momentum_values[] = {0.5f, 0.7f, 0.9f, 0.95f};
    int num_momentums = sizeof(momentum_values) / sizeof(momentum_values[0]);
    
    for (int i = 0; i < num_momentums; i++) {
        NeuralNetwork* nn_test = nn_create(layer_sizes, 3, 0.01f);
        MomentumOptimizer* momentum = momentum_create(0.01f, momentum_values[i], 100);
        
        float final_loss = train_with_momentum_and_evaluate(nn_test, data, momentum, 30);
        printf("åŠ¨é‡ç³»æ•° %.2f: æœ€ç»ˆæŸå¤± %.6f\n", momentum_values[i], final_loss);
        
        momentum_free(momentum);
        nn_free(nn_test);
    }
    
    // æµ‹è¯•ä¸åŒæ­£åˆ™åŒ–å¼ºåº¦
    printf("\næ­£åˆ™åŒ–å¼ºåº¦æ•æ„Ÿæ€§æµ‹è¯•:\n");
    float l2_lambdas[] = {0.0f, 0.001f, 0.01f, 0.1f};
    int num_lambdas = sizeof(l2_lambdas) / sizeof(l2_lambdas[0]);
    
    for (int i = 0; i < num_lambdas; i++) {
        NeuralNetwork* nn_test = nn_create(layer_sizes, 3, 0.01f);
        
        float final_loss = train_with_l2_and_evaluate(nn_test, data, l2_lambdas[i], 30);
        printf("L2æ­£åˆ™åŒ–å¼ºåº¦ %.3f: æœ€ç»ˆæŸå¤± %.6f\n", l2_lambdas[i], final_loss);
        
        nn_free(nn_test);
    }
    
    // æ¸…ç†
    free(data->inputs);
    free(data->targets);
    free(data);
    
    printf("\n");
}

// è®­ç»ƒå¹¶è¯„ä¼°å‡½æ•°
float train_and_evaluate(NeuralNetwork* nn, TrainingData* data, int epochs) {
    for (int epoch = 0; epoch < epochs; epoch++) {
        float total_loss = 0;
        
        for (int i = 0; i < data->num_samples; i++) {
            float* input = &data->inputs[i * data->input_size];
            float* target = &data->targets[i * data->output_size];
            
            float output[1];
            nn_forward(nn, input, output);
            
            float loss = mse_loss(output, target, 1);
            total_loss += loss;
            
            ForwardCache cache;
            cache.num_layers = nn->num_layers;
            nn_forward_with_cache(nn, input, output, &cache);
            nn_backward(nn, input, target, &cache);
            update_network_parameters(nn, nn->learning_rate);
            
            for (int l = 0; l < nn->num_layers; l++) {
                free(cache.activations[l+1]);
                free(cache.z_values[l]);
            }
        }
    }
    
    // è®¡ç®—æœ€ç»ˆæŸå¤±
    float final_loss = 0;
    for (int i = 0; i < data->num_samples; i++) {
        float* input = &data->inputs[i * data->input_size];
        float* target = &data->targets[i * data->output_size];
        
        float output[1];
        nn_forward(nn, input, output);
        
        float loss = mse_loss(output, target, 1);
        final_loss += loss;
    }
    
    return final_loss / data->num_samples;
}
```

**ç»ƒä¹ 4ï¼šä¸»æµ‹è¯•ç¨‹åº**

```c
// ä¸»æµ‹è¯•ç¨‹åº
int main() {
    printf("é«˜çº§ä¼˜åŒ–æŠ€æœ¯æµ‹è¯•ç¨‹åº\n");
    printf("==================\n\n");
    
    // è®¾ç½®éšæœºç§å­
    srand(time(NULL));
    
    // è¿è¡Œæ‰€æœ‰æµ‹è¯•
    test_advanced_optimizers();
    test_regularization_techniques();
    test_hyperparameter_sensitivity();
    
    printf("æ‰€æœ‰æµ‹è¯•å®Œæˆï¼\n");
    return 0;
}
```

---

## ğŸ’» é˜¶æ®µä¸‰ï¼šä»é›¶å®ç°ç®—æ³•ï¼ˆ3å‘¨ï¼‰

### ç¬¬7å‘¨ï¼šæ•°æ®ç»“æ„è®¾è®¡

#### å­¦ä¹ å†…å®¹
- **ç¥ç»ç½‘ç»œæ•°æ®ç»“æ„**
- **çŸ©é˜µè¿ç®—åº“**
- **å†…å­˜ç®¡ç†**

#### ç†è®ºçŸ¥è¯†

**1. æ•°æ®ç»“æ„è®¾è®¡åŸåˆ™**

**æ¨¡å—åŒ–è®¾è®¡**
- **å•ä¸€èŒè´£**: æ¯ä¸ªç»“æ„ä½“åªè´Ÿè´£ä¸€ä¸ªåŠŸèƒ½
- **æ¥å£æ¸…æ™°**: å®šä¹‰æ˜ç¡®çš„å‡½æ•°æ¥å£
- **ä¾èµ–æœ€å°åŒ–**: å‡å°‘æ¨¡å—é—´çš„è€¦åˆ

**å†…å­˜æ•ˆç‡ä¼˜åŒ–**
- **å†…å­˜å¯¹é½**: åˆ©ç”¨CPUç¼“å­˜è¡Œå¯¹é½
- **å†…å­˜æ± **: å‡å°‘é¢‘ç¹çš„å†…å­˜åˆ†é…/é‡Šæ”¾
- **æ•°æ®å‹ç¼©**: ä½¿ç”¨é€‚å½“çš„æ•°æ®ç±»å‹

**è®¡ç®—æ•ˆç‡ä¼˜åŒ–**
- **ç¼“å­˜å‹å¥½**: æ•°æ®è®¿é—®æ¨¡å¼ç¬¦åˆCPUç¼“å­˜ç‰¹æ€§
- **å‘é‡åŒ–**: åˆ©ç”¨SIMDæŒ‡ä»¤é›†åŠ é€Ÿè®¡ç®—
- **å¹¶è¡ŒåŒ–**: å¤šçº¿ç¨‹å¤„ç†å¤§è§„æ¨¡æ•°æ®

**2. çŸ©é˜µè¿ç®—ä¼˜åŒ–ç­–ç•¥**

**å†…å­˜è®¿é—®æ¨¡å¼ä¼˜åŒ–**
```
// è¡Œä¸»åºè®¿é—®ï¼ˆæ¨èï¼‰
for (int i = 0; i < rows; i++) {
    for (int j = 0; j < cols; j++) {
        result[i * cols + j] = a[i * cols + j] + b[i * cols + j];
    }
}

// åˆ—ä¸»åºè®¿é—®ï¼ˆä¸æ¨èï¼Œç¼“å­˜ä¸å‹å¥½ï¼‰
for (int j = 0; j < cols; j++) {
    for (int i = 0; i < rows; i++) {
        result[i * cols + j] = a[i * cols + j] + b[i * cols + j];
    }
}
```

**SIMDå‘é‡åŒ–**
- **SSE/AVXæŒ‡ä»¤**: åŒæ—¶å¤„ç†å¤šä¸ªfloatæ•°æ®
- **è‡ªåŠ¨å‘é‡åŒ–**: ç¼–è¯‘å™¨ä¼˜åŒ–
- **æ‰‹åŠ¨å‘é‡åŒ–**: æ˜¾å¼ä½¿ç”¨SIMDæŒ‡ä»¤

**3. å†…å­˜ç®¡ç†ç­–ç•¥**

**å†…å­˜æ± è®¾è®¡**
- **é¢„åˆ†é…**: é¿å…è¿è¡Œæ—¶é¢‘ç¹åˆ†é…
- **åˆ†å±‚ç®¡ç†**: ä¸åŒå¤§å°çš„å†…å­˜å—
- **ç¢ç‰‡æ•´ç†**: å®šæœŸæ•´ç†å†…å­˜ç¢ç‰‡

**æ™ºèƒ½æŒ‡é’ˆæ¨¡å¼**
- **å¼•ç”¨è®¡æ•°**: è‡ªåŠ¨ç®¡ç†å†…å­˜ç”Ÿå‘½å‘¨æœŸ
- **RAII**: èµ„æºè·å–å³åˆå§‹åŒ–
- **å¼‚å¸¸å®‰å…¨**: ç¡®ä¿èµ„æºæ­£ç¡®é‡Šæ”¾

#### Cè¯­è¨€å®ç°

**1. å®Œæ•´çš„ç¥ç»ç½‘ç»œæ•°æ®ç»“æ„**

```c
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <math.h>
#include <time.h>

// æ¿€æ´»å‡½æ•°ç±»å‹å®šä¹‰
typedef float (*activation_func_t)(float);
typedef float (*activation_derivative_t)(float);

// ç¥ç»å…ƒç»“æ„
typedef struct {
    float* weights;
    float bias;
    float last_activation;
    float last_input;
    int input_size;
} Neuron;

// å±‚ç»“æ„
typedef struct {
    Neuron* neurons;
    int num_neurons;
    int input_size;
    activation_func_t activation;
    activation_derivative_t activation_derivative;
    float* last_outputs;  // ç¼“å­˜å‰å‘ä¼ æ’­ç»“æœ
    float* last_inputs;   // ç¼“å­˜è¾“å…¥
} Layer;

// ç¥ç»ç½‘ç»œç»“æ„
typedef struct {
    Layer* layers;
    int num_layers;
    float learning_rate;
    int* layer_sizes;
    int total_parameters;
    float* parameter_buffer;  // å‚æ•°ç¼“å†²åŒº
    float* gradient_buffer;   // æ¢¯åº¦ç¼“å†²åŒº
} NeuralNetwork;

// è®­ç»ƒæ•°æ®ç»“æ„
typedef struct {
    float* inputs;
    float* targets;
    int num_samples;
    int input_size;
    int output_size;
    int current_batch_start;
    int current_batch_size;
} TrainingData;

// çŸ©é˜µç»“æ„
typedef struct {
    float* data;
    int rows;
    int cols;
    int capacity;  // é¢„åˆ†é…å®¹é‡
} Matrix;

// å†…å­˜æ± ç»“æ„
typedef struct {
    void** blocks;
    int* block_sizes;
    int num_blocks;
    int capacity;
    void* pool_start;
    size_t pool_size;
} MemoryPool;
```

**2. å†…å­˜æ± å®ç°**

```c
// å†…å­˜æ± åˆ›å»º
MemoryPool* memory_pool_create(size_t pool_size) {
    MemoryPool* pool = malloc(sizeof(MemoryPool));
    pool->pool_size = pool_size;
    pool->pool_start = malloc(pool_size);
    pool->blocks = malloc(100 * sizeof(void*));  // æœ€å¤š100ä¸ªå—
    pool->block_sizes = malloc(100 * sizeof(int));
    pool->num_blocks = 0;
    pool->capacity = 100;
    
    return pool;
}

// ä»å†…å­˜æ± åˆ†é…
void* memory_pool_alloc(MemoryPool* pool, size_t size) {
    // ç®€å•çš„é¦–æ¬¡é€‚é…ç®—æ³•
    size_t offset = 0;
    for (int i = 0; i < pool->num_blocks; i++) {
        offset += pool->block_sizes[i];
    }
    
    if (offset + size <= pool->pool_size) {
        void* block = (char*)pool->pool_start + offset;
        pool->blocks[pool->num_blocks] = block;
        pool->block_sizes[pool->num_blocks] = size;
        pool->num_blocks++;
        return block;
    }
    
    return NULL;  // å†…å­˜ä¸è¶³
}

// é‡Šæ”¾å†…å­˜æ± 
void memory_pool_free(MemoryPool* pool) {
    free(pool->pool_start);
    free(pool->blocks);
    free(pool->block_sizes);
    free(pool);
}
```

**3. çŸ©é˜µè¿ç®—åº“å®ç°**

```c
// çŸ©é˜µåˆ›å»º
Matrix* matrix_create(int rows, int cols) {
    Matrix* mat = malloc(sizeof(Matrix));
    mat->rows = rows;
    mat->cols = cols;
    mat->capacity = rows * cols;
    mat->data = calloc(rows * cols, sizeof(float));
    return mat;
}

// çŸ©é˜µé‡Šæ”¾
void matrix_free(Matrix* mat) {
    if (mat) {
        free(mat->data);
        free(mat);
    }
}

// çŸ©é˜µä¹˜æ³•ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
void matrix_multiply(Matrix* result, Matrix* a, Matrix* b) {
    if (a->cols != b->rows) {
        printf("çŸ©é˜µç»´åº¦ä¸åŒ¹é…: %dx%d * %dx%d\n", 
               a->rows, a->cols, b->rows, b->cols);
        return;
    }
    
    // æ¸…é›¶ç»“æœçŸ©é˜µ
    memset(result->data, 0, result->rows * result->cols * sizeof(float));
    
    // ä¼˜åŒ–çš„çŸ©é˜µä¹˜æ³•ï¼ˆè¡Œä¸»åºè®¿é—®ï¼‰
    for (int i = 0; i < a->rows; i++) {
        for (int k = 0; k < a->cols; k++) {
            float a_ik = a->data[i * a->cols + k];
            for (int j = 0; j < b->cols; j++) {
                result->data[i * result->cols + j] += 
                    a_ik * b->data[k * b->cols + j];
            }
        }
    }
}

// çŸ©é˜µåŠ æ³•
void matrix_add(Matrix* result, Matrix* a, Matrix* b) {
    if (a->rows != b->rows || a->cols != b->cols) {
        printf("çŸ©é˜µç»´åº¦ä¸åŒ¹é…\n");
        return;
    }
    
    int size = a->rows * a->cols;
    for (int i = 0; i < size; i++) {
        result->data[i] = a->data[i] + b->data[i];
    }
}

// çŸ©é˜µè½¬ç½®
void matrix_transpose(Matrix* result, Matrix* src) {
    for (int i = 0; i < src->rows; i++) {
        for (int j = 0; j < src->cols; j++) {
            result->data[j * result->cols + i] = src->data[i * src->cols + j];
        }
    }
}

// çŸ©é˜µå…ƒç´ çº§ä¹˜æ³•
void matrix_elementwise_multiply(Matrix* result, Matrix* a, Matrix* b) {
    int size = a->rows * a->cols;
    for (int i = 0; i < size; i++) {
        result->data[i] = a->data[i] * b->data[i];
    }
}

// çŸ©é˜µæ ‡é‡ä¹˜æ³•
void matrix_scalar_multiply(Matrix* result, Matrix* mat, float scalar) {
    int size = mat->rows * mat->cols;
    for (int i = 0; i < size; i++) {
        result->data[i] = mat->data[i] * scalar;
    }
}
```

**4. ç¥ç»ç½‘ç»œæ•°æ®ç»“æ„å®ç°**

```c
// ç¥ç»å…ƒåˆ›å»º
Neuron* neuron_create(int input_size) {
    Neuron* neuron = malloc(sizeof(Neuron));
    neuron->input_size = input_size;
    neuron->weights = malloc(input_size * sizeof(float));
    neuron->bias = 0.0f;
    
    // éšæœºåˆå§‹åŒ–æƒé‡
    for (int i = 0; i < input_size; i++) {
        neuron->weights[i] = ((float)rand() / RAND_MAX - 0.5f) * 2.0f;
    }
    
    return neuron;
}

// ç¥ç»å…ƒé‡Šæ”¾
void neuron_free(Neuron* neuron) {
    if (neuron) {
        free(neuron->weights);
        free(neuron);
    }
}

// å±‚åˆ›å»º
Layer* layer_create(int input_size, int num_neurons, 
                   activation_func_t activation,
                   activation_derivative_t activation_derivative) {
    Layer* layer = malloc(sizeof(Layer));
    layer->input_size = input_size;
    layer->num_neurons = num_neurons;
    layer->activation = activation;
    layer->activation_derivative = activation_derivative;
    
    // åˆ›å»ºç¥ç»å…ƒ
    layer->neurons = malloc(num_neurons * sizeof(Neuron));
    for (int i = 0; i < num_neurons; i++) {
        layer->neurons[i] = *neuron_create(input_size);
    }
    
    // åˆ†é…ç¼“å­˜ç©ºé—´
    layer->last_outputs = malloc(num_neurons * sizeof(float));
    layer->last_inputs = malloc(input_size * sizeof(float));
    
    return layer;
}

// å±‚é‡Šæ”¾
void layer_free(Layer* layer) {
    if (layer) {
        for (int i = 0; i < layer->num_neurons; i++) {
            neuron_free(&layer->neurons[i]);
        }
        free(layer->neurons);
        free(layer->last_outputs);
        free(layer->last_inputs);
        free(layer);
    }
}

// ç¥ç»ç½‘ç»œåˆ›å»ºï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
NeuralNetwork* nn_create_optimized(int* layer_sizes, int num_layers, 
                                  float learning_rate, MemoryPool* pool) {
    NeuralNetwork* nn;
    if (pool) {
        nn = memory_pool_alloc(pool, sizeof(NeuralNetwork));
    } else {
        nn = malloc(sizeof(NeuralNetwork));
    }
    
    nn->num_layers = num_layers - 1;
    nn->learning_rate = learning_rate;
    nn->layer_sizes = malloc(num_layers * sizeof(int));
    
    // å¤åˆ¶å±‚å¤§å°ä¿¡æ¯
    memcpy(nn->layer_sizes, layer_sizes, num_layers * sizeof(int));
    
    // è®¡ç®—æ€»å‚æ•°æ•°é‡
    nn->total_parameters = 0;
    for (int i = 0; i < nn->num_layers; i++) {
        nn->total_parameters += layer_sizes[i] * layer_sizes[i + 1] + layer_sizes[i + 1];
    }
    
    // åˆ†é…å‚æ•°å’Œæ¢¯åº¦ç¼“å†²åŒº
    if (pool) {
        nn->parameter_buffer = memory_pool_alloc(pool, nn->total_parameters * sizeof(float));
        nn->gradient_buffer = memory_pool_alloc(pool, nn->total_parameters * sizeof(float));
    } else {
        nn->parameter_buffer = malloc(nn->total_parameters * sizeof(float));
        nn->gradient_buffer = malloc(nn->total_parameters * sizeof(float));
    }
    
    // åˆå§‹åŒ–å±‚
    nn->layers = malloc(nn->num_layers * sizeof(Layer));
    int param_offset = 0;
    
    for (int i = 0; i < nn->num_layers; i++) {
        Layer* layer = &nn->layers[i];
        layer->input_size = layer_sizes[i];
        layer->num_neurons = layer_sizes[i + 1];
        
        // è®¾ç½®æ¿€æ´»å‡½æ•°
        if (i == nn->num_layers - 1) {
            layer->activation = linear_activation;
            layer->activation_derivative = linear_derivative;
        } else {
            layer->activation = relu;
            layer->activation_derivative = relu_derivative;
        }
        
        // åˆ†é…ç¥ç»å…ƒ
        layer->neurons = malloc(layer->num_neurons * sizeof(Neuron));
        for (int j = 0; j < layer->num_neurons; j++) {
            Neuron* neuron = &layer->neurons[j];
            neuron->input_size = layer->input_size;
            neuron->weights = &nn->parameter_buffer[param_offset];
            neuron->bias = nn->parameter_buffer[param_offset + layer->input_size];
            
            // åˆå§‹åŒ–æƒé‡
            for (int k = 0; k < layer->input_size; k++) {
                neuron->weights[k] = ((float)rand() / RAND_MAX - 0.5f) * 2.0f * 
                                   sqrtf(2.0f / layer->input_size);
            }
            
            param_offset += layer->input_size + 1;
        }
        
        // åˆ†é…ç¼“å­˜ç©ºé—´
        layer->last_outputs = malloc(layer->num_neurons * sizeof(float));
        layer->last_inputs = malloc(layer->input_size * sizeof(float));
    }
    
    return nn;
}
```

#### å®è·µç»ƒä¹ 

**ç»ƒä¹ 1ï¼šæ•°æ®ç»“æ„æµ‹è¯•**

```c
// æµ‹è¯•ç¥ç»ç½‘ç»œæ•°æ®ç»“æ„
void test_neural_network_structure() {
    printf("=== æµ‹è¯•ç¥ç»ç½‘ç»œæ•°æ®ç»“æ„ ===\n");
    
    // åˆ›å»ºå†…å­˜æ± 
    MemoryPool* pool = memory_pool_create(1024 * 1024);  // 1MB
    
    // å®šä¹‰ç½‘ç»œç»“æ„
    int layer_sizes[] = {2, 3, 1};
    int num_layers = 3;
    
    // åˆ›å»ºç¥ç»ç½‘ç»œ
    NeuralNetwork* nn = nn_create_optimized(layer_sizes, num_layers, 0.01f, pool);
    
    printf("ç½‘ç»œå±‚æ•°: %d\n", nn->num_layers);
    printf("æ€»å‚æ•°æ•°é‡: %d\n", nn->total_parameters);
    printf("å­¦ä¹ ç‡: %f\n", nn->learning_rate);
    
    // æµ‹è¯•å‚æ•°ç¼“å†²åŒº
    printf("å‚æ•°ç¼“å†²åŒºå¤§å°: %d bytes\n", nn->total_parameters * sizeof(float));
    printf("æ¢¯åº¦ç¼“å†²åŒºå¤§å°: %d bytes\n", nn->total_parameters * sizeof(float));
    
    // éªŒè¯å±‚ç»“æ„
    for (int i = 0; i < nn->num_layers; i++) {
        Layer* layer = &nn->layers[i];
        printf("å±‚ %d: è¾“å…¥å¤§å°=%d, ç¥ç»å…ƒæ•°é‡=%d\n", 
               i, layer->input_size, layer->num_neurons);
    }
    
    // æ¸…ç†
    memory_pool_free(pool);
    printf("æµ‹è¯•å®Œæˆ\n\n");
}

// æµ‹è¯•çŸ©é˜µè¿ç®—
void test_matrix_operations() {
    printf("=== æµ‹è¯•çŸ©é˜µè¿ç®— ===\n");
    
    // åˆ›å»ºæµ‹è¯•çŸ©é˜µ
    Matrix* a = matrix_create(2, 3);
    Matrix* b = matrix_create(3, 2);
    Matrix* c = matrix_create(2, 2);
    
    // åˆå§‹åŒ–çŸ©é˜µA
    a->data[0] = 1; a->data[1] = 2; a->data[2] = 3;
    a->data[3] = 4; a->data[4] = 5; a->data[5] = 6;
    
    // åˆå§‹åŒ–çŸ©é˜µB
    b->data[0] = 1; b->data[1] = 4;
    b->data[2] = 2; b->data[3] = 5;
    b->data[4] = 3; b->data[5] = 6;
    
    printf("çŸ©é˜µA (2x3):\n");
    for (int i = 0; i < 2; i++) {
        for (int j = 0; j < 3; j++) {
            printf("%.1f ", a->data[i * 3 + j]);
        }
        printf("\n");
    }
    
    printf("çŸ©é˜µB (3x2):\n");
    for (int i = 0; i < 3; i++) {
        for (int j = 0; j < 2; j++) {
            printf("%.1f ", b->data[i * 2 + j]);
        }
        printf("\n");
    }
    
    // æµ‹è¯•çŸ©é˜µä¹˜æ³•
    matrix_multiply(c, a, b);
    printf("A * B (2x2):\n");
    for (int i = 0; i < 2; i++) {
        for (int j = 0; j < 2; j++) {
            printf("%.1f ", c->data[i * 2 + j]);
        }
        printf("\n");
    }
    
    // æµ‹è¯•çŸ©é˜µåŠ æ³•
    Matrix* d = matrix_create(2, 2);
    d->data[0] = 1; d->data[1] = 2;
    d->data[2] = 3; d->data[3] = 4;
    
    matrix_add(c, c, d);
    printf("C + D:\n");
    for (int i = 0; i < 2; i++) {
        for (int j = 0; j < 2; j++) {
            printf("%.1f ", c->data[i * 2 + j]);
        }
        printf("\n");
    }
    
    // æ¸…ç†
    matrix_free(a);
    matrix_free(b);
    matrix_free(c);
    matrix_free(d);
    printf("çŸ©é˜µè¿ç®—æµ‹è¯•å®Œæˆ\n\n");
}
```

**ç»ƒä¹ 2ï¼šå†…å­˜ç®¡ç†æµ‹è¯•**

```c
// æµ‹è¯•å†…å­˜æ± æ€§èƒ½
void test_memory_pool_performance() {
    printf("=== æµ‹è¯•å†…å­˜æ± æ€§èƒ½ ===\n");
    
    const int num_allocations = 1000;
    const size_t allocation_size = 1024;  // 1KB
    
    // æµ‹è¯•æ™®é€šmalloc/free
    clock_t start = clock();
    void** ptrs1 = malloc(num_allocations * sizeof(void*));
    
    for (int i = 0; i < num_allocations; i++) {
        ptrs1[i] = malloc(allocation_size);
    }
    
    for (int i = 0; i < num_allocations; i++) {
        free(ptrs1[i]);
    }
    free(ptrs1);
    
    clock_t end = clock();
    double malloc_time = ((double)(end - start)) / CLOCKS_PER_SEC;
    printf("æ™®é€šmalloc/freeæ—¶é—´: %.6fç§’\n", malloc_time);
    
    // æµ‹è¯•å†…å­˜æ± 
    start = clock();
    MemoryPool* pool = memory_pool_create(num_allocations * allocation_size);
    void** ptrs2 = malloc(num_allocations * sizeof(void*));
    
    for (int i = 0; i < num_allocations; i++) {
        ptrs2[i] = memory_pool_alloc(pool, allocation_size);
    }
    
    memory_pool_free(pool);
    free(ptrs2);
    
    end = clock();
    double pool_time = ((double)(end - start)) / CLOCKS_PER_SEC;
    printf("å†…å­˜æ± æ—¶é—´: %.6fç§’\n", pool_time);
    printf("æ€§èƒ½æå‡: %.2fx\n", malloc_time / pool_time);
    printf("å†…å­˜æ± æµ‹è¯•å®Œæˆ\n\n");
}

// æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ
void test_memory_usage() {
    printf("=== æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ ===\n");
    
    // åˆ›å»ºä¸åŒå¤§å°çš„ç¥ç»ç½‘ç»œ
    int small_layers[] = {2, 3, 1};
    int medium_layers[] = {10, 20, 10, 1};
    int large_layers[] = {100, 200, 100, 50, 1};
    
    // è®¡ç®—å‚æ•°æ•°é‡
    int small_params = 0;
    for (int i = 0; i < 2; i++) {
        small_params += small_layers[i] * small_layers[i + 1] + small_layers[i + 1];
    }
    
    int medium_params = 0;
    for (int i = 0; i < 3; i++) {
        medium_params += medium_layers[i] * medium_layers[i + 1] + medium_layers[i + 1];
    }
    
    int large_params = 0;
    for (int i = 0; i < 4; i++) {
        large_params += large_layers[i] * large_layers[i + 1] + large_layers[i + 1];
    }
    
    printf("å°å‹ç½‘ç»œå‚æ•°: %d (%.2f KB)\n", small_params, small_params * 4.0f / 1024);
    printf("ä¸­å‹ç½‘ç»œå‚æ•°: %d (%.2f KB)\n", medium_params, medium_params * 4.0f / 1024);
    printf("å¤§å‹ç½‘ç»œå‚æ•°: %d (%.2f KB)\n", large_params, large_params * 4.0f / 1024);
    
    // æµ‹è¯•å®é™…å†…å­˜åˆ†é…
    MemoryPool* pool = memory_pool_create(1024 * 1024);  // 1MB
    
    NeuralNetwork* small_nn = nn_create_optimized(small_layers, 3, 0.01f, pool);
    NeuralNetwork* medium_nn = nn_create_optimized(medium_layers, 4, 0.01f, pool);
    NeuralNetwork* large_nn = nn_create_optimized(large_layers, 5, 0.01f, pool);
    
    printf("å®é™…åˆ†é…çš„ç½‘ç»œ:\n");
    printf("å°å‹ç½‘ç»œ: %d å‚æ•°\n", small_nn->total_parameters);
    printf("ä¸­å‹ç½‘ç»œ: %d å‚æ•°\n", medium_nn->total_parameters);
    printf("å¤§å‹ç½‘ç»œ: %d å‚æ•°\n", large_nn->total_parameters);
    
    memory_pool_free(pool);
    printf("å†…å­˜ä½¿ç”¨æµ‹è¯•å®Œæˆ\n\n");
}
```

**ç»ƒä¹ 3ï¼šæ€§èƒ½ä¼˜åŒ–æµ‹è¯•**

```c
// æµ‹è¯•çŸ©é˜µä¹˜æ³•æ€§èƒ½
void test_matrix_performance() {
    printf("=== æµ‹è¯•çŸ©é˜µä¹˜æ³•æ€§èƒ½ ===\n");
    
    const int size = 100;
    Matrix* a = matrix_create(size, size);
    Matrix* b = matrix_create(size, size);
    Matrix* c = matrix_create(size, size);
    
    // åˆå§‹åŒ–éšæœºæ•°æ®
    srand(time(NULL));
    for (int i = 0; i < size * size; i++) {
        a->data[i] = ((float)rand() / RAND_MAX - 0.5f) * 2.0f;
        b->data[i] = ((float)rand() / RAND_MAX - 0.5f) * 2.0f;
    }
    
    // æµ‹è¯•çŸ©é˜µä¹˜æ³•æ€§èƒ½
    clock_t start = clock();
    matrix_multiply(c, a, b);
    clock_t end = clock();
    
    double time_taken = ((double)(end - start)) / CLOCKS_PER_SEC;
    printf("çŸ©é˜µä¹˜æ³• (%dx%d): %.6fç§’\n", size, size, time_taken);
    printf("æ€§èƒ½: %.2f MFLOPS\n", (2.0 * size * size * size) / (time_taken * 1e6));
    
    // æµ‹è¯•ä¸åŒå¤§å°çš„æ€§èƒ½
    int sizes[] = {50, 100, 200};
    for (int i = 0; i < 3; i++) {
        int test_size = sizes[i];
        Matrix* test_a = matrix_create(test_size, test_size);
        Matrix* test_b = matrix_create(test_size, test_size);
        Matrix* test_c = matrix_create(test_size, test_size);
        
        // åˆå§‹åŒ–
        for (int j = 0; j < test_size * test_size; j++) {
            test_a->data[j] = ((float)rand() / RAND_MAX - 0.5f) * 2.0f;
            test_b->data[j] = ((float)rand() / RAND_MAX - 0.5f) * 2.0f;
        }
        
        start = clock();
        matrix_multiply(test_c, test_a, test_b);
        end = clock();
        
        time_taken = ((double)(end - start)) / CLOCKS_PER_SEC;
        printf("çŸ©é˜µä¹˜æ³• (%dx%d): %.6fç§’\n", test_size, test_size, time_taken);
        
        matrix_free(test_a);
        matrix_free(test_b);
        matrix_free(test_c);
    }
    
    matrix_free(a);
    matrix_free(b);
    matrix_free(c);
    printf("çŸ©é˜µæ€§èƒ½æµ‹è¯•å®Œæˆ\n\n");
}

// æµ‹è¯•ç¼“å­˜å‹å¥½æ€§
void test_cache_friendliness() {
    printf("=== æµ‹è¯•ç¼“å­˜å‹å¥½æ€§ ===\n");
    
    const int size = 1000;
    float* data = malloc(size * size * sizeof(float));
    
    // åˆå§‹åŒ–æ•°æ®
    for (int i = 0; i < size * size; i++) {
        data[i] = (float)i;
    }
    
    // æµ‹è¯•è¡Œä¸»åºè®¿é—®
    clock_t start = clock();
    float sum1 = 0;
    for (int i = 0; i < size; i++) {
        for (int j = 0; j < size; j++) {
            sum1 += data[i * size + j];
        }
    }
    clock_t end = clock();
    double row_time = ((double)(end - start)) / CLOCKS_PER_SEC;
    printf("è¡Œä¸»åºè®¿é—®: %.6fç§’, æ€»å’Œ: %.0f\n", row_time, sum1);
    
    // æµ‹è¯•åˆ—ä¸»åºè®¿é—®
    start = clock();
    float sum2 = 0;
    for (int j = 0; j < size; j++) {
        for (int i = 0; i < size; i++) {
            sum2 += data[i * size + j];
        }
    }
    end = clock();
    double col_time = ((double)(end - start)) / CLOCKS_PER_SEC;
    printf("åˆ—ä¸»åºè®¿é—®: %.6fç§’, æ€»å’Œ: %.0f\n", col_time, sum2);
    
    printf("æ€§èƒ½å·®å¼‚: %.2fx\n", col_time / row_time);
    
    free(data);
    printf("ç¼“å­˜å‹å¥½æ€§æµ‹è¯•å®Œæˆ\n\n");
}
```

**ç»ƒä¹ 4ï¼šå®Œæ•´çš„æ•°æ®ç»“æ„æµ‹è¯•ç¨‹åº**

```c
// ä¸»æµ‹è¯•å‡½æ•°
int main() {
    printf("=== ç¬¬7å‘¨ï¼šæ•°æ®ç»“æ„è®¾è®¡æµ‹è¯• ===\n\n");
    
    // è®¾ç½®éšæœºç§å­
    srand(time(NULL));
    
    // è¿è¡Œæ‰€æœ‰æµ‹è¯•
    test_neural_network_structure();
    test_matrix_operations();
    test_memory_pool_performance();
    test_memory_usage();
    test_matrix_performance();
    test_cache_friendliness();
    
    printf("=== æ‰€æœ‰æµ‹è¯•å®Œæˆ ===\n");
    return 0;
}
```

**åµŒå…¥å¼ä¼˜åŒ–ç‰ˆæœ¬**

```c
// åµŒå…¥å¼ç¯å¢ƒä¸‹çš„ä¼˜åŒ–ç‰ˆæœ¬
typedef struct {
    float* data;
    int rows;
    int cols;
    int is_static;  // æ˜¯å¦ä¸ºé™æ€åˆ†é…
} MatrixStatic;

// é™æ€çŸ©é˜µåˆ›å»ºï¼ˆé¿å…åŠ¨æ€åˆ†é…ï¼‰
MatrixStatic* matrix_create_static(float* buffer, int rows, int cols) {
    MatrixStatic* mat = malloc(sizeof(MatrixStatic));
    mat->data = buffer;
    mat->rows = rows;
    mat->cols = cols;
    mat->is_static = 1;
    return mat;
}

// ä¼˜åŒ–çš„çŸ©é˜µä¹˜æ³•ï¼ˆé¿å…åŠ¨æ€åˆ†é…ï¼‰
void matrix_multiply_static(float* result, float* a, float* b, 
                           int a_rows, int a_cols, int b_cols) {
    // æ¸…é›¶ç»“æœ
    memset(result, 0, a_rows * b_cols * sizeof(float));
    
    // ä¼˜åŒ–çš„ä¸‰é‡å¾ªç¯
    for (int i = 0; i < a_rows; i++) {
        for (int k = 0; k < a_cols; k++) {
            float a_ik = a[i * a_cols + k];
            for (int j = 0; j < b_cols; j++) {
                result[i * b_cols + j] += a_ik * b[k * b_cols + j];
            }
        }
    }
}

// å†…å­˜ä½¿ç”¨ç›‘æ§
typedef struct {
    size_t total_allocated;
    size_t peak_usage;
    int allocation_count;
} MemoryMonitor;

MemoryMonitor* memory_monitor_create() {
    MemoryMonitor* monitor = malloc(sizeof(MemoryMonitor));
    monitor->total_allocated = 0;
    monitor->peak_usage = 0;
    monitor->allocation_count = 0;
    return monitor;
}

void* monitored_malloc(MemoryMonitor* monitor, size_t size) {
    void* ptr = malloc(size);
    if (ptr) {
        monitor->total_allocated += size;
        monitor->allocation_count++;
        if (monitor->total_allocated > monitor->peak_usage) {
            monitor->peak_usage = monitor->total_allocated;
        }
    }
    return ptr;
}

void monitored_free(MemoryMonitor* monitor, void* ptr) {
    free(ptr);
    // æ³¨æ„ï¼šè¿™é‡Œç®€åŒ–äº†ï¼Œå®é™…åº”è¯¥è·Ÿè¸ªæ¯ä¸ªåˆ†é…çš„å¤§å°
}

void print_memory_stats(MemoryMonitor* monitor) {
    printf("å†…å­˜ä½¿ç”¨ç»Ÿè®¡:\n");
    printf("æ€»åˆ†é…æ¬¡æ•°: %d\n", monitor->allocation_count);
    printf("å³°å€¼å†…å­˜ä½¿ç”¨: %.2f KB\n", monitor->peak_usage / 1024.0f);
    printf("å½“å‰å†…å­˜ä½¿ç”¨: %.2f KB\n", monitor->total_allocated / 1024.0f);
}
```

### ç¬¬8å‘¨ï¼šæ ¸å¿ƒç®—æ³•å®ç°

#### å­¦ä¹ å†…å®¹
- **å‰å‘ä¼ æ’­å®ç°**
- **åå‘ä¼ æ’­å®ç°**
- **è®­ç»ƒå¾ªç¯å®ç°**

#### ç†è®ºçŸ¥è¯†

**1. ç®—æ³•å®ç°è¦ç‚¹**

**æ•°å€¼ç¨³å®šæ€§**
- **æ¢¯åº¦è£å‰ª**: é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
- **æƒé‡åˆå§‹åŒ–**: ä½¿ç”¨åˆé€‚çš„åˆå§‹åŒ–ç­–ç•¥
- **æ¿€æ´»å‡½æ•°é€‰æ‹©**: é¿å…é¥±å’ŒåŒºåŸŸ
- **æŸå¤±å‡½æ•°é€‰æ‹©**: æ•°å€¼ç¨³å®šçš„æŸå¤±å‡½æ•°

**å†…å­˜ç®¡ç†ç­–ç•¥**
- **ç¼“å­˜ç®¡ç†**: å­˜å‚¨ä¸­é—´è®¡ç®—ç»“æœ
- **å†…å­˜æ± **: å‡å°‘é¢‘ç¹åˆ†é…/é‡Šæ”¾
- **æ‰¹é‡å¤„ç†**: ä¼˜åŒ–å†…å­˜ä½¿ç”¨æ¨¡å¼
- **åƒåœ¾å›æ”¶**: åŠæ—¶é‡Šæ”¾ä¸´æ—¶å˜é‡

**é”™è¯¯å¤„ç†æœºåˆ¶**
- **å‚æ•°éªŒè¯**: æ£€æŸ¥è¾“å…¥å‚æ•°çš„æœ‰æ•ˆæ€§
- **è¾¹ç•Œæ£€æŸ¥**: é˜²æ­¢æ•°ç»„è¶Šç•Œ
- **å¼‚å¸¸å¤„ç†**: ä¼˜é›…å¤„ç†é”™è¯¯æƒ…å†µ
- **æ—¥å¿—è®°å½•**: è®°å½•å…³é”®æ“ä½œå’Œé”™è¯¯

**æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯**
- **å‘é‡åŒ–è®¡ç®—**: åˆ©ç”¨SIMDæŒ‡ä»¤
- **å¹¶è¡ŒåŒ–**: å¤šçº¿ç¨‹å¤„ç†
- **ç¼“å­˜ä¼˜åŒ–**: æé«˜æ•°æ®å±€éƒ¨æ€§
- **ç®—æ³•ä¼˜åŒ–**: å‡å°‘è®¡ç®—å¤æ‚åº¦

**2. è°ƒè¯•æŠ€å·§**

**æ¢¯åº¦æ£€æŸ¥åŸç†**
```
æ•°å€¼æ¢¯åº¦ â‰ˆ è§£ææ¢¯åº¦
âˆ‚L/âˆ‚Î¸ â‰ˆ (L(Î¸+Îµ) - L(Î¸-Îµ)) / (2Îµ)
```

**æŸå¤±ç›‘æ§æŒ‡æ ‡**
- **è®­ç»ƒæŸå¤±**: è§‚å¯Ÿæ”¶æ•›è¶‹åŠ¿
- **éªŒè¯æŸå¤±**: æ£€æµ‹è¿‡æ‹Ÿåˆ
- **æ¢¯åº¦èŒƒæ•°**: ç›‘æ§æ¢¯åº¦å¤§å°
- **æƒé‡åˆ†å¸ƒ**: è§‚å¯Ÿå‚æ•°å˜åŒ–

**3. ç®—æ³•å®ç°ç­–ç•¥**

**å‰å‘ä¼ æ’­ä¼˜åŒ–**
- **æ‰¹é‡å¤„ç†**: åŒæ—¶å¤„ç†å¤šä¸ªæ ·æœ¬
- **ç¼“å­˜ä¼˜åŒ–**: å­˜å‚¨ä¸­é—´ç»“æœ
- **å‘é‡åŒ–**: åˆ©ç”¨SIMDæŒ‡ä»¤åŠ é€Ÿ
- **å†…å­˜é¢„åˆ†é…**: é¿å…è¿è¡Œæ—¶åˆ†é…

**åå‘ä¼ æ’­ä¼˜åŒ–**
- **é“¾å¼æ³•åˆ™**: é«˜æ•ˆè®¡ç®—æ¢¯åº¦
- **æ¢¯åº¦ç´¯ç§¯**: å¤„ç†å¤§æ‰¹é‡æ•°æ®
- **ç¨€ç–æ¢¯åº¦**: å¤„ç†ç¨€ç–è¿æ¥
- **æ¢¯åº¦å‹ç¼©**: å‡å°‘é€šä¿¡å¼€é”€

#### Cè¯­è¨€å®ç°

**1. å®Œæ•´çš„å‰å‘ä¼ æ’­å®ç°**

```c
// å‰å‘ä¼ æ’­ç¼“å­˜ç»“æ„
typedef struct {
    float* layer_inputs;    // æ¯å±‚çš„è¾“å…¥
    float* layer_outputs;   // æ¯å±‚çš„è¾“å‡º
    float* layer_activations; // æ¯å±‚çš„æ¿€æ´»å€¼
    int max_layer_size;     // æœ€å¤§å±‚å¤§å°
    int num_layers;         // å±‚æ•°
} ForwardCache;

// åˆ›å»ºå‰å‘ä¼ æ’­ç¼“å­˜
ForwardCache* create_forward_cache(NeuralNetwork* nn) {
    ForwardCache* cache = malloc(sizeof(ForwardCache));
    cache->num_layers = nn->num_layers;
    
    // è®¡ç®—æœ€å¤§å±‚å¤§å°
    cache->max_layer_size = 0;
    for (int i = 0; i < nn->num_layers; i++) {
        int layer_size = nn->layers[i].num_neurons;
        if (layer_size > cache->max_layer_size) {
            cache->max_layer_size = layer_size;
        }
    }
    
    // åˆ†é…ç¼“å­˜ç©ºé—´
    cache->layer_inputs = malloc(cache->max_layer_size * sizeof(float));
    cache->layer_outputs = malloc(cache->max_layer_size * sizeof(float));
    cache->layer_activations = malloc(cache->max_layer_size * sizeof(float));
    
    return cache;
}

// é‡Šæ”¾å‰å‘ä¼ æ’­ç¼“å­˜
void free_forward_cache(ForwardCache* cache) {
    if (cache) {
        free(cache->layer_inputs);
        free(cache->layer_outputs);
        free(cache->layer_activations);
        free(cache);
    }
}

// å•å±‚å‰å‘ä¼ æ’­ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
void layer_forward_optimized(Layer* layer, float* input, float* output, 
                            ForwardCache* cache) {
    // ç¼“å­˜è¾“å…¥
    memcpy(cache->layer_inputs, input, layer->input_size * sizeof(float));
    
    // è®¡ç®—çº¿æ€§ç»„åˆå’Œæ¿€æ´»
    for (int i = 0; i < layer->num_neurons; i++) {
        float sum = layer->neurons[i].bias;
        
        // å‘é‡åŒ–è®¡ç®—ï¼ˆå¦‚æœæ”¯æŒï¼‰
        for (int j = 0; j < layer->input_size; j++) {
            sum += layer->neurons[i].weights[j] * input[j];
        }
        
        // ç¼“å­˜æ¿€æ´»å‰çš„å€¼
        cache->layer_activations[i] = sum;
        
        // åº”ç”¨æ¿€æ´»å‡½æ•°
        output[i] = layer->activation(sum);
    }
    
    // ç¼“å­˜è¾“å‡º
    memcpy(cache->layer_outputs, output, layer->num_neurons * sizeof(float));
}

// å®Œæ•´çš„å‰å‘ä¼ æ’­
void nn_forward_optimized(NeuralNetwork* nn, float* input, float* output, 
                         ForwardCache* cache) {
    float* current_input = input;
    float* current_output = cache->layer_outputs;
    
    // é€å±‚å‰å‘ä¼ æ’­
    for (int i = 0; i < nn->num_layers; i++) {
        Layer* layer = &nn->layers[i];
        
        // æœ€åä¸€å±‚çš„è¾“å‡ºç›´æ¥å†™å…¥output
        if (i == nn->num_layers - 1) {
            layer_forward_optimized(layer, current_input, output, cache);
        } else {
            layer_forward_optimized(layer, current_input, current_output, cache);
            current_input = current_output;
        }
    }
}
```

**2. å®Œæ•´çš„åå‘ä¼ æ’­å®ç°**

```c
// åå‘ä¼ æ’­ç¼“å­˜ç»“æ„
typedef struct {
    float* layer_gradients;     // æ¯å±‚çš„æ¢¯åº¦
    float* weight_gradients;    // æƒé‡æ¢¯åº¦
    float* bias_gradients;      // åç½®æ¢¯åº¦
    float* input_gradients;     // è¾“å…¥æ¢¯åº¦
    int max_layer_size;         // æœ€å¤§å±‚å¤§å°
    int num_layers;             // å±‚æ•°
} BackwardCache;

// åˆ›å»ºåå‘ä¼ æ’­ç¼“å­˜
BackwardCache* create_backward_cache(NeuralNetwork* nn) {
    BackwardCache* cache = malloc(sizeof(BackwardCache));
    cache->num_layers = nn->num_layers;
    
    // è®¡ç®—æœ€å¤§å±‚å¤§å°
    cache->max_layer_size = 0;
    for (int i = 0; i < nn->num_layers; i++) {
        int layer_size = nn->layers[i].num_neurons;
        if (layer_size > cache->max_layer_size) {
            cache->max_layer_size = layer_size;
        }
    }
    
    // åˆ†é…ç¼“å­˜ç©ºé—´
    cache->layer_gradients = malloc(cache->max_layer_size * sizeof(float));
    cache->weight_gradients = malloc(cache->max_layer_size * cache->max_layer_size * sizeof(float));
    cache->bias_gradients = malloc(cache->max_layer_size * sizeof(float));
    cache->input_gradients = malloc(cache->max_layer_size * sizeof(float));
    
    return cache;
}

// é‡Šæ”¾åå‘ä¼ æ’­ç¼“å­˜
void free_backward_cache(BackwardCache* cache) {
    if (cache) {
        free(cache->layer_gradients);
        free(cache->weight_gradients);
        free(cache->bias_gradients);
        free(cache->input_gradients);
        free(cache);
    }
}

// è®¡ç®—è¾“å‡ºå±‚è¯¯å·®
void compute_output_error(float* output, float* target, float* error, 
                         int output_size, activation_derivative_t activation_derivative) {
    for (int i = 0; i < output_size; i++) {
        float output_error = output[i] - target[i];
        error[i] = output_error * activation_derivative(output[i]);
    }
}

// è®¡ç®—éšè—å±‚è¯¯å·®
void compute_hidden_error(float* next_layer_error, float* weights, 
                         float* current_activation, float* error,
                         int current_size, int next_size, 
                         activation_derivative_t activation_derivative) {
    // æ¸…é›¶å½“å‰å±‚è¯¯å·®
    memset(error, 0, current_size * sizeof(float));
    
    // è®¡ç®—è¯¯å·®ä¼ æ’­
    for (int i = 0; i < current_size; i++) {
        float sum = 0;
        for (int j = 0; j < next_size; j++) {
            sum += next_layer_error[j] * weights[j * current_size + i];
        }
        error[i] = sum * activation_derivative(current_activation[i]);
    }
}

// è®¡ç®—æƒé‡æ¢¯åº¦
void compute_weight_gradients(float* input, float* error, float* weight_gradients,
                             int input_size, int output_size) {
    for (int i = 0; i < output_size; i++) {
        for (int j = 0; j < input_size; j++) {
            weight_gradients[i * input_size + j] = error[i] * input[j];
        }
    }
}

// å•å±‚åå‘ä¼ æ’­
void layer_backward_optimized(Layer* layer, float* input, float* error,
                             BackwardCache* cache, ForwardCache* forward_cache) {
    // è®¡ç®—æƒé‡æ¢¯åº¦
    compute_weight_gradients(input, error, cache->weight_gradients,
                           layer->input_size, layer->num_neurons);
    
    // è®¡ç®—åç½®æ¢¯åº¦
    memcpy(cache->bias_gradients, error, layer->num_neurons * sizeof(float));
    
    // è®¡ç®—è¾“å…¥æ¢¯åº¦ï¼ˆç”¨äºå‰ä¸€å±‚ï¼‰
    compute_hidden_error(error, layer->neurons[0].weights, 
                        forward_cache->layer_activations, cache->input_gradients,
                        layer->input_size, layer->num_neurons,
                        layer->activation_derivative);
}

// å®Œæ•´çš„åå‘ä¼ æ’­
void nn_backward_optimized(NeuralNetwork* nn, float* input, float* target,
                          ForwardCache* forward_cache, BackwardCache* backward_cache) {
    // è®¡ç®—è¾“å‡ºå±‚è¯¯å·®
    float* output = malloc(nn->layers[nn->num_layers - 1].num_neurons * sizeof(float));
    nn_forward_optimized(nn, input, output, forward_cache);
    
    float* error = malloc(nn->layers[nn->num_layers - 1].num_neurons * sizeof(float));
    compute_output_error(output, target, error, 
                        nn->layers[nn->num_layers - 1].num_neurons,
                        nn->layers[nn->num_layers - 1].activation_derivative);
    
    // ä»è¾“å‡ºå±‚å¼€å§‹åå‘ä¼ æ’­
    for (int i = nn->num_layers - 1; i >= 0; i--) {
        Layer* layer = &nn->layers[i];
        
        // è·å–å½“å‰å±‚çš„è¾“å…¥
        float* layer_input = (i == 0) ? input : 
                            forward_cache->layer_outputs;
        
        // æ‰§è¡Œåå‘ä¼ æ’­
        layer_backward_optimized(layer, layer_input, error, 
                                backward_cache, forward_cache);
        
        // æ›´æ–°å‚æ•°
        update_layer_parameters(layer, backward_cache->weight_gradients,
                              backward_cache->bias_gradients, nn->learning_rate);
        
        // å‡†å¤‡ä¸‹ä¸€å±‚çš„è¯¯å·®
        if (i > 0) {
            float* temp = error;
            error = backward_cache->input_gradients;
            backward_cache->input_gradients = temp;
        }
    }
    
    free(output);
    free(error);
}
```

**3. è®­ç»ƒå¾ªç¯å®ç°**

```c
// è®­ç»ƒé…ç½®ç»“æ„
typedef struct {
    int epochs;
    int batch_size;
    float learning_rate;
    float validation_split;
    int early_stopping_patience;
    float min_delta;
} TrainingConfig;

// è®­ç»ƒçŠ¶æ€ç»“æ„
typedef struct {
    float* training_losses;
    float* validation_losses;
    int current_epoch;
    int best_epoch;
    float best_validation_loss;
    int patience_counter;
} TrainingState;

// åˆ›å»ºè®­ç»ƒçŠ¶æ€
TrainingState* create_training_state(int epochs) {
    TrainingState* state = malloc(sizeof(TrainingState));
    state->training_losses = malloc(epochs * sizeof(float));
    state->validation_losses = malloc(epochs * sizeof(float));
    state->current_epoch = 0;
    state->best_epoch = 0;
    state->best_validation_loss = INFINITY;
    state->patience_counter = 0;
    return state;
}

// é‡Šæ”¾è®­ç»ƒçŠ¶æ€
void free_training_state(TrainingState* state) {
    if (state) {
        free(state->training_losses);
        free(state->validation_losses);
        free(state);
    }
}

// æ•°æ®æ‰“ä¹±
void shuffle_data(TrainingData* data) {
    for (int i = data->num_samples - 1; i > 0; i--) {
        int j = rand() % (i + 1);
        
        // äº¤æ¢è¾“å…¥
        for (int k = 0; k < data->input_size; k++) {
            float temp = data->inputs[i * data->input_size + k];
            data->inputs[i * data->input_size + k] = data->inputs[j * data->input_size + k];
            data->inputs[j * data->input_size + k] = temp;
        }
        
        // äº¤æ¢ç›®æ ‡
        for (int k = 0; k < data->output_size; k++) {
            float temp = data->targets[i * data->output_size + k];
            data->targets[i * data->output_size + k] = data->targets[j * data->output_size + k];
            data->targets[j * data->output_size + k] = temp;
        }
    }
}

// è·å–æ‰¹æ¬¡æ•°æ®
void get_batch_data(TrainingData* data, int batch_start, int batch_size,
                   float* batch_input, float* batch_target) {
    for (int i = 0; i < batch_size; i++) {
        int sample_idx = batch_start + i;
        
        // å¤åˆ¶è¾“å…¥
        for (int j = 0; j < data->input_size; j++) {
            batch_input[i * data->input_size + j] = 
                data->inputs[sample_idx * data->input_size + j];
        }
        
        // å¤åˆ¶ç›®æ ‡
        for (int j = 0; j < data->output_size; j++) {
            batch_target[i * data->output_size + j] = 
                data->targets[sample_idx * data->output_size + j];
        }
    }
}

// å®Œæ•´çš„è®­ç»ƒå¾ªç¯
void train_neural_network(NeuralNetwork* nn, TrainingData* data, 
                         TrainingConfig* config, TrainingState* state) {
    // åˆ›å»ºç¼“å­˜
    ForwardCache* forward_cache = create_forward_cache(nn);
    BackwardCache* backward_cache = create_backward_cache(nn);
    
    // è®¡ç®—è®­ç»ƒå’ŒéªŒè¯æ ·æœ¬æ•°é‡
    int validation_samples = (int)(data->num_samples * config->validation_split);
    int training_samples = data->num_samples - validation_samples;
    int num_batches = training_samples / config->batch_size;
    
    printf("å¼€å§‹è®­ç»ƒ...\n");
    printf("è®­ç»ƒæ ·æœ¬: %d, éªŒè¯æ ·æœ¬: %d, æ‰¹æ¬¡æ•°é‡: %d\n", 
           training_samples, validation_samples, num_batches);
    
    for (int epoch = 0; epoch < config->epochs; epoch++) {
        state->current_epoch = epoch;
        
        // æ‰“ä¹±è®­ç»ƒæ•°æ®
        shuffle_data(data);
        
        float total_training_loss = 0;
        
        // è®­ç»ƒé˜¶æ®µ
        for (int batch = 0; batch < num_batches; batch++) {
            int batch_start = batch * config->batch_size;
            
            // è·å–æ‰¹æ¬¡æ•°æ®
            float* batch_input = malloc(config->batch_size * data->input_size * sizeof(float));
            float* batch_target = malloc(config->batch_size * data->output_size * sizeof(float));
            get_batch_data(data, batch_start, config->batch_size, batch_input, batch_target);
            
            // å‰å‘ä¼ æ’­
            float* predictions = malloc(config->batch_size * data->output_size * sizeof(float));
            nn_forward_optimized(nn, batch_input, predictions, forward_cache);
            
            // è®¡ç®—æŸå¤±
            float batch_loss = 0;
            for (int i = 0; i < config->batch_size * data->output_size; i++) {
                float diff = predictions[i] - batch_target[i];
                batch_loss += diff * diff;
            }
            batch_loss /= (config->batch_size * data->output_size);
            total_training_loss += batch_loss;
            
            // åå‘ä¼ æ’­
            nn_backward_optimized(nn, batch_input, batch_target, 
                                forward_cache, backward_cache);
            
            // æ¸…ç†æ‰¹æ¬¡å†…å­˜
            free(batch_input);
            free(batch_target);
            free(predictions);
        }
        
        // è®¡ç®—å¹³å‡è®­ç»ƒæŸå¤±
        state->training_losses[epoch] = total_training_loss / num_batches;
        
        // éªŒè¯é˜¶æ®µ
        float total_validation_loss = 0;
        int validation_batches = validation_samples / config->batch_size;
        
        for (int batch = 0; batch < validation_batches; batch++) {
            int batch_start = training_samples + batch * config->batch_size;
            
            // è·å–éªŒè¯æ‰¹æ¬¡æ•°æ®
            float* batch_input = malloc(config->batch_size * data->input_size * sizeof(float));
            float* batch_target = malloc(config->batch_size * data->output_size * sizeof(float));
            get_batch_data(data, batch_start, config->batch_size, batch_input, batch_target);
            
            // å‰å‘ä¼ æ’­ï¼ˆä¸æ›´æ–°å‚æ•°ï¼‰
            float* predictions = malloc(config->batch_size * data->output_size * sizeof(float));
            nn_forward_optimized(nn, batch_input, predictions, forward_cache);
            
            // è®¡ç®—éªŒè¯æŸå¤±
            float batch_loss = 0;
            for (int i = 0; i < config->batch_size * data->output_size; i++) {
                float diff = predictions[i] - batch_target[i];
                batch_loss += diff * diff;
            }
            batch_loss /= (config->batch_size * data->output_size);
            total_validation_loss += batch_loss;
            
            // æ¸…ç†éªŒè¯æ‰¹æ¬¡å†…å­˜
            free(batch_input);
            free(batch_target);
            free(predictions);
        }
        
        state->validation_losses[epoch] = total_validation_loss / validation_batches;
        
        // æ—©åœæ£€æŸ¥
        if (state->validation_losses[epoch] < state->best_validation_loss - config->min_delta) {
            state->best_validation_loss = state->validation_losses[epoch];
            state->best_epoch = epoch;
            state->patience_counter = 0;
        } else {
            state->patience_counter++;
        }
        
        // æ‰“å°è®­ç»ƒè¿›åº¦
        if (epoch % 10 == 0) {
            printf("Epoch %d/%d - è®­ç»ƒæŸå¤±: %.6f, éªŒè¯æŸå¤±: %.6f\n", 
                   epoch + 1, config->epochs, 
                   state->training_losses[epoch], 
                   state->validation_losses[epoch]);
        }
        
        // æ—©åœ
        if (state->patience_counter >= config->early_stopping_patience) {
            printf("æ—©åœè§¦å‘ï¼Œæœ€ä½³epoch: %d\n", state->best_epoch + 1);
            break;
        }
    }
    
    // æ¸…ç†ç¼“å­˜
    free_forward_cache(forward_cache);
    free_backward_cache(backward_cache);
    
    printf("è®­ç»ƒå®Œæˆï¼\n");
}
```

#### å®è·µç»ƒä¹ 

**ç»ƒä¹ 1ï¼šå‰å‘ä¼ æ’­æµ‹è¯•**

```c
// æµ‹è¯•å‰å‘ä¼ æ’­åŠŸèƒ½
void test_forward_propagation() {
    printf("=== æµ‹è¯•å‰å‘ä¼ æ’­ ===\n");
    
    // åˆ›å»ºç®€å•ç½‘ç»œ
    int layer_sizes[] = {2, 3, 1};
    NeuralNetwork* nn = nn_create_optimized(layer_sizes, 3, 0.01f, NULL);
    
    // åˆ›å»ºå‰å‘ä¼ æ’­ç¼“å­˜
    ForwardCache* cache = create_forward_cache(nn);
    
    // æµ‹è¯•è¾“å…¥
    float input[] = {0.5f, 0.3f};
    float output[1];
    
    // æ‰§è¡Œå‰å‘ä¼ æ’­
    nn_forward_optimized(nn, input, output, cache);
    
    printf("è¾“å…¥: [%.2f, %.2f]\n", input[0], input[1]);
    printf("è¾“å‡º: %.6f\n", output[0]);
    
    // æµ‹è¯•å¤šå±‚å‰å‘ä¼ æ’­
    printf("\næµ‹è¯•å¤šå±‚å‰å‘ä¼ æ’­:\n");
    for (int i = 0; i < nn->num_layers; i++) {
        Layer* layer = &nn->layers[i];
        printf("å±‚ %d: è¾“å…¥å¤§å°=%d, ç¥ç»å…ƒæ•°é‡=%d\n", 
               i, layer->input_size, layer->num_neurons);
        
        // æ˜¾ç¤ºæƒé‡ç»Ÿè®¡
        float min_w = INFINITY, max_w = -INFINITY, sum_w = 0;
        for (int j = 0; j < layer->num_neurons; j++) {
            for (int k = 0; k < layer->input_size; k++) {
                float w = layer->neurons[j].weights[k];
                if (w < min_w) min_w = w;
                if (w > max_w) max_w = w;
                sum_w += w;
            }
        }
        printf("  æƒé‡èŒƒå›´: [%.4f, %.4f], å¹³å‡å€¼: %.4f\n", 
               min_w, max_w, sum_w / (layer->num_neurons * layer->input_size));
    }
    
    // æ¸…ç†
    free_forward_cache(cache);
    printf("å‰å‘ä¼ æ’­æµ‹è¯•å®Œæˆ\n\n");
}

// æµ‹è¯•æ‰¹é‡å‰å‘ä¼ æ’­
void test_batch_forward_propagation() {
    printf("=== æµ‹è¯•æ‰¹é‡å‰å‘ä¼ æ’­ ===\n");
    
    // åˆ›å»ºç½‘ç»œ
    int layer_sizes[] = {2, 4, 1};
    NeuralNetwork* nn = nn_create_optimized(layer_sizes, 3, 0.01f, NULL);
    ForwardCache* cache = create_forward_cache(nn);
    
    // åˆ›å»ºæ‰¹é‡æ•°æ®
    const int batch_size = 4;
    float batch_input[] = {
        0.1f, 0.2f,  // æ ·æœ¬1
        0.3f, 0.4f,  // æ ·æœ¬2
        0.5f, 0.6f,  // æ ·æœ¬3
        0.7f, 0.8f   // æ ·æœ¬4
    };
    float batch_output[batch_size];
    
    // æ‰§è¡Œæ‰¹é‡å‰å‘ä¼ æ’­
    clock_t start = clock();
    for (int i = 0; i < 1000; i++) {  // é‡å¤1000æ¬¡æµ‹è¯•æ€§èƒ½
        for (int j = 0; j < batch_size; j++) {
            float* sample_input = &batch_input[j * 2];
            float* sample_output = &batch_output[j];
            nn_forward_optimized(nn, sample_input, sample_output, cache);
        }
    }
    clock_t end = clock();
    
    double time_taken = ((double)(end - start)) / CLOCKS_PER_SEC;
    printf("æ‰¹é‡å‰å‘ä¼ æ’­æ€§èƒ½: %.6fç§’ (1000æ¬¡è¿­ä»£)\n", time_taken);
    printf("å¹³å‡æ¯æ¬¡å‰å‘ä¼ æ’­: %.6fç§’\n", time_taken / (1000 * batch_size));
    
    // æ˜¾ç¤ºç»“æœ
    printf("æ‰¹é‡è¾“å‡ºç»“æœ:\n");
    for (int i = 0; i < batch_size; i++) {
        printf("æ ·æœ¬ %d: %.6f\n", i + 1, batch_output[i]);
    }
    
    free_forward_cache(cache);
    printf("æ‰¹é‡å‰å‘ä¼ æ’­æµ‹è¯•å®Œæˆ\n\n");
}
```

**ç»ƒä¹ 2ï¼šåå‘ä¼ æ’­æµ‹è¯•**

```c
// æµ‹è¯•åå‘ä¼ æ’­åŠŸèƒ½
void test_backward_propagation() {
    printf("=== æµ‹è¯•åå‘ä¼ æ’­ ===\n");
    
    // åˆ›å»ºç®€å•ç½‘ç»œ
    int layer_sizes[] = {2, 3, 1};
    NeuralNetwork* nn = nn_create_optimized(layer_sizes, 3, 0.01f, NULL);
    
    // åˆ›å»ºç¼“å­˜
    ForwardCache* forward_cache = create_forward_cache(nn);
    BackwardCache* backward_cache = create_backward_cache(nn);
    
    // æµ‹è¯•æ•°æ®
    float input[] = {0.5f, 0.3f};
    float target[] = {0.8f};
    
    // è®°å½•åˆå§‹å‚æ•°
    float initial_weights[6];
    memcpy(initial_weights, nn->layers[0].neurons[0].weights, 6 * sizeof(float));
    
    // æ‰§è¡Œå‰å‘ä¼ æ’­
    float output[1];
    nn_forward_optimized(nn, input, output, forward_cache);
    printf("å‰å‘ä¼ æ’­è¾“å‡º: %.6f\n", output[0]);
    
    // æ‰§è¡Œåå‘ä¼ æ’­
    nn_backward_optimized(nn, input, target, forward_cache, backward_cache);
    
    // æ£€æŸ¥å‚æ•°æ˜¯å¦æ›´æ–°
    float updated_weights[6];
    memcpy(updated_weights, nn->layers[0].neurons[0].weights, 6 * sizeof(float));
    
    printf("å‚æ•°æ›´æ–°æ£€æŸ¥:\n");
    for (int i = 0; i < 6; i++) {
        float diff = updated_weights[i] - initial_weights[i];
        printf("æƒé‡ %d: %.6f -> %.6f (å˜åŒ–: %.6f)\n", 
               i, initial_weights[i], updated_weights[i], diff);
    }
    
    // æ¸…ç†
    free_forward_cache(forward_cache);
    free_backward_cache(backward_cache);
    printf("åå‘ä¼ æ’­æµ‹è¯•å®Œæˆ\n\n");
}

// æµ‹è¯•æ¢¯åº¦æ£€æŸ¥
void test_gradient_checking() {
    printf("=== æµ‹è¯•æ¢¯åº¦æ£€æŸ¥ ===\n");
    
    // åˆ›å»ºç½‘ç»œ
    int layer_sizes[] = {2, 3, 1};
    NeuralNetwork* nn = nn_create_optimized(layer_sizes, 3, 0.01f, NULL);
    
    // æµ‹è¯•æ•°æ®
    float input[] = {0.5f, 0.3f};
    float target[] = {0.8f};
    
    // è®¡ç®—æ•°å€¼æ¢¯åº¦
    const float epsilon = 1e-6f;
    float numerical_gradients[6];
    
    for (int i = 0; i < 6; i++) {
        // ä¿å­˜åŸå§‹æƒé‡
        float original_weight = nn->layers[0].neurons[0].weights[i];
        
        // è®¡ç®— f(Î¸ + Îµ)
        nn->layers[0].neurons[0].weights[i] = original_weight + epsilon;
        float output_plus[1];
        nn_forward_optimized(nn, input, output_plus, NULL);
        float loss_plus = (output_plus[0] - target[0]) * (output_plus[0] - target[0]);
        
        // è®¡ç®— f(Î¸ - Îµ)
        nn->layers[0].neurons[0].weights[i] = original_weight - epsilon;
        float output_minus[1];
        nn_forward_optimized(nn, input, output_minus, NULL);
        float loss_minus = (output_minus[0] - target[0]) * (output_minus[0] - target[0]);
        
        // æ¢å¤åŸå§‹æƒé‡
        nn->layers[0].neurons[0].weights[i] = original_weight;
        
        // è®¡ç®—æ•°å€¼æ¢¯åº¦
        numerical_gradients[i] = (loss_plus - loss_minus) / (2.0f * epsilon);
    }
    
    // è®¡ç®—è§£ææ¢¯åº¦
    ForwardCache* forward_cache = create_forward_cache(nn);
    BackwardCache* backward_cache = create_backward_cache(nn);
    
    nn_forward_optimized(nn, input, output_plus, forward_cache);
    nn_backward_optimized(nn, input, target, forward_cache, backward_cache);
    
    // æ¯”è¾ƒæ¢¯åº¦
    printf("æ¢¯åº¦æ¯”è¾ƒ (æ•°å€¼ vs è§£æ):\n");
    for (int i = 0; i < 6; i++) {
        float analytical_gradient = backward_cache->weight_gradients[i];
        float relative_error = fabs(numerical_gradients[i] - analytical_gradient) / 
                             (fabs(numerical_gradients[i]) + fabs(analytical_gradient) + 1e-8f);
        
        printf("æƒé‡ %d: æ•°å€¼=%.6f, è§£æ=%.6f, ç›¸å¯¹è¯¯å·®=%.6f\n", 
               i, numerical_gradients[i], analytical_gradient, relative_error);
        
        if (relative_error > 1e-3f) {
            printf("è­¦å‘Š: æ¢¯åº¦æ£€æŸ¥å¤±è´¥ï¼\n");
        }
    }
    
    free_forward_cache(forward_cache);
    free_backward_cache(backward_cache);
    printf("æ¢¯åº¦æ£€æŸ¥æµ‹è¯•å®Œæˆ\n\n");
}
```

**ç»ƒä¹ 3ï¼šè®­ç»ƒå¾ªç¯æµ‹è¯•**

```c
// åˆ›å»ºæµ‹è¯•æ•°æ®
TrainingData* create_test_data(int num_samples) {
    TrainingData* data = malloc(sizeof(TrainingData));
    data->num_samples = num_samples;
    data->input_size = 2;
    data->output_size = 1;
    
    data->inputs = malloc(num_samples * data->input_size * sizeof(float));
    data->targets = malloc(num_samples * data->output_size * sizeof(float));
    
    // ç”Ÿæˆç®€å•çš„XORæ•°æ®
    for (int i = 0; i < num_samples; i++) {
        float x1 = ((float)rand() / RAND_MAX - 0.5f) * 2.0f;
        float x2 = ((float)rand() / RAND_MAX - 0.5f) * 2.0f;
        
        data->inputs[i * 2] = x1;
        data->inputs[i * 2 + 1] = x2;
        
        // ç®€å•çš„éçº¿æ€§å‡½æ•°: y = sin(x1) * cos(x2)
        data->targets[i] = sinf(x1) * cosf(x2);
    }
    
    return data;
}

// æµ‹è¯•å®Œæ•´è®­ç»ƒå¾ªç¯
void test_training_loop() {
    printf("=== æµ‹è¯•å®Œæ•´è®­ç»ƒå¾ªç¯ ===\n");
    
    // åˆ›å»ºç½‘ç»œ
    int layer_sizes[] = {2, 8, 4, 1};
    NeuralNetwork* nn = nn_create_optimized(layer_sizes, 4, 0.01f, NULL);
    
    // åˆ›å»ºè®­ç»ƒæ•°æ®
    TrainingData* data = create_test_data(1000);
    
    // åˆ›å»ºè®­ç»ƒé…ç½®
    TrainingConfig config = {
        .epochs = 100,
        .batch_size = 32,
        .learning_rate = 0.01f,
        .validation_split = 0.2f,
        .early_stopping_patience = 10,
        .min_delta = 1e-6f
    };
    
    // åˆ›å»ºè®­ç»ƒçŠ¶æ€
    TrainingState* state = create_training_state(config.epochs);
    
    // æ‰§è¡Œè®­ç»ƒ
    train_neural_network(nn, data, &config, state);
    
    // æ˜¾ç¤ºè®­ç»ƒç»“æœ
    printf("\nè®­ç»ƒç»“æœæ€»ç»“:\n");
    printf("æœ€ä½³epoch: %d\n", state->best_epoch + 1);
    printf("æœ€ä½³éªŒè¯æŸå¤±: %.6f\n", state->best_validation_loss);
    printf("æœ€ç»ˆè®­ç»ƒæŸå¤±: %.6f\n", state->training_losses[state->current_epoch]);
    printf("æœ€ç»ˆéªŒè¯æŸå¤±: %.6f\n", state->validation_losses[state->current_epoch]);
    
    // æµ‹è¯•è®­ç»ƒåçš„ç½‘ç»œ
    printf("\næµ‹è¯•è®­ç»ƒåçš„ç½‘ç»œ:\n");
    float test_inputs[][2] = {{0.5f, 0.3f}, {-0.2f, 0.8f}, {0.1f, -0.4f}};
    float expected_outputs[] = {sinf(0.5f) * cosf(0.3f), 
                               sinf(-0.2f) * cosf(0.8f), 
                               sinf(0.1f) * cosf(-0.4f)};
    
    ForwardCache* cache = create_forward_cache(nn);
    for (int i = 0; i < 3; i++) {
        float output[1];
        nn_forward_optimized(nn, test_inputs[i], output, cache);
        float error = fabs(output[0] - expected_outputs[i]);
        printf("æµ‹è¯• %d: è¾“å…¥[%.2f, %.2f], é¢„æµ‹=%.6f, æœŸæœ›=%.6f, è¯¯å·®=%.6f\n", 
               i + 1, test_inputs[i][0], test_inputs[i][1], 
               output[0], expected_outputs[i], error);
    }
    
    // æ¸…ç†
    free_forward_cache(cache);
    free_training_state(state);
    free(data->inputs);
    free(data->targets);
    free(data);
    printf("è®­ç»ƒå¾ªç¯æµ‹è¯•å®Œæˆ\n\n");
}
```

**ç»ƒä¹ 4ï¼šæ€§èƒ½ä¼˜åŒ–æµ‹è¯•**

```c
// æµ‹è¯•ä¸åŒç½‘ç»œå¤§å°çš„æ€§èƒ½
void test_performance_scaling() {
    printf("=== æµ‹è¯•æ€§èƒ½æ‰©å±•æ€§ ===\n");
    
    int network_sizes[] = {10, 50, 100, 200};
    int layer_configs[][4] = {
        {2, 10, 1},      // å°ç½‘ç»œ
        {2, 50, 1},      // ä¸­ç­‰ç½‘ç»œ
        {2, 100, 1},     // å¤§ç½‘ç»œ
        {2, 200, 1}      // è¶…å¤§ç½‘ç»œ
    };
    
    for (int i = 0; i < 4; i++) {
        printf("\næµ‹è¯•ç½‘ç»œå¤§å° %d:\n", network_sizes[i]);
        
        // åˆ›å»ºç½‘ç»œ
        NeuralNetwork* nn = nn_create_optimized(layer_configs[i], 3, 0.01f, NULL);
        ForwardCache* forward_cache = create_forward_cache(nn);
        BackwardCache* backward_cache = create_backward_cache(nn);
        
        // åˆ›å»ºæµ‹è¯•æ•°æ®
        TrainingData* data = create_test_data(100);
        
        // æµ‹è¯•å‰å‘ä¼ æ’­æ€§èƒ½
        clock_t start = clock();
        for (int j = 0; j < 1000; j++) {
            for (int k = 0; k < data->num_samples; k++) {
                float* input = &data->inputs[k * data->input_size];
                float output[1];
                nn_forward_optimized(nn, input, output, forward_cache);
            }
        }
        clock_t end = clock();
        double forward_time = ((double)(end - start)) / CLOCKS_PER_SEC;
        
        // æµ‹è¯•åå‘ä¼ æ’­æ€§èƒ½
        start = clock();
        for (int j = 0; j < 100; j++) {
            for (int k = 0; k < data->num_samples; k++) {
                float* input = &data->inputs[k * data->input_size];
                float* target = &data->targets[k * data->output_size];
                nn_backward_optimized(nn, input, target, forward_cache, backward_cache);
            }
        }
        end = clock();
        double backward_time = ((double)(end - start)) / CLOCKS_PER_SEC;
        
        printf("å‚æ•°æ•°é‡: %d\n", nn->total_parameters);
        printf("å‰å‘ä¼ æ’­æ—¶é—´: %.6fç§’\n", forward_time);
        printf("åå‘ä¼ æ’­æ—¶é—´: %.6fç§’\n", backward_time);
        printf("æ€»è®¡ç®—æ—¶é—´: %.6fç§’\n", forward_time + backward_time);
        
        // æ¸…ç†
        free_forward_cache(forward_cache);
        free_backward_cache(backward_cache);
        free(data->inputs);
        free(data->targets);
        free(data);
    }
    
    printf("æ€§èƒ½æ‰©å±•æ€§æµ‹è¯•å®Œæˆ\n\n");
}

// æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ
void test_memory_usage() {
    printf("=== æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ ===\n");
    
    // åˆ›å»ºä¸åŒå¤§å°çš„ç½‘ç»œ
    int layer_configs[][5] = {
        {2, 10, 1},           // å°ç½‘ç»œ
        {2, 50, 25, 1},       // ä¸­ç­‰ç½‘ç»œ
        {2, 100, 50, 25, 1}   // å¤§ç½‘ç»œ
    };
    
    for (int i = 0; i < 3; i++) {
        int num_layers = (i == 0) ? 3 : (i == 1) ? 4 : 5;
        
        // åˆ›å»ºç½‘ç»œ
        NeuralNetwork* nn = nn_create_optimized(layer_configs[i], num_layers, 0.01f, NULL);
        
        // åˆ›å»ºç¼“å­˜
        ForwardCache* forward_cache = create_forward_cache(nn);
        BackwardCache* backward_cache = create_backward_cache(nn);
        
        // è®¡ç®—å†…å­˜ä½¿ç”¨
        size_t network_memory = nn->total_parameters * sizeof(float) * 2;  // å‚æ•° + æ¢¯åº¦
        size_t forward_cache_memory = forward_cache->max_layer_size * 3 * sizeof(float);
        size_t backward_cache_memory = backward_cache->max_layer_size * 4 * sizeof(float);
        size_t total_memory = network_memory + forward_cache_memory + backward_cache_memory;
        
        printf("ç½‘ç»œ %d:\n", i + 1);
        printf("  å‚æ•°æ•°é‡: %d\n", nn->total_parameters);
        printf("  ç½‘ç»œå†…å­˜: %.2f KB\n", network_memory / 1024.0f);
        printf("  å‰å‘ç¼“å­˜: %.2f KB\n", forward_cache_memory / 1024.0f);
        printf("  åå‘ç¼“å­˜: %.2f KB\n", backward_cache_memory / 1024.0f);
        printf("  æ€»å†…å­˜: %.2f KB\n", total_memory / 1024.0f);
        
        // æ¸…ç†
        free_forward_cache(forward_cache);
        free_backward_cache(backward_cache);
    }
    
    printf("å†…å­˜ä½¿ç”¨æµ‹è¯•å®Œæˆ\n\n");
}
```

**ç»ƒä¹ 5ï¼šå®Œæ•´ç®—æ³•æµ‹è¯•ç¨‹åº**

```c
// ä¸»æµ‹è¯•å‡½æ•°
int main() {
    printf("=== ç¬¬8å‘¨ï¼šæ ¸å¿ƒç®—æ³•å®ç°æµ‹è¯• ===\n\n");
    
    // è®¾ç½®éšæœºç§å­
    srand(time(NULL));
    
    // è¿è¡Œæ‰€æœ‰æµ‹è¯•
    test_forward_propagation();
    test_batch_forward_propagation();
    test_backward_propagation();
    test_gradient_checking();
    test_training_loop();
    test_performance_scaling();
    test_memory_usage();
    
    printf("=== æ‰€æœ‰æµ‹è¯•å®Œæˆ ===\n");
    return 0;
}
```

**åµŒå…¥å¼ä¼˜åŒ–ç‰ˆæœ¬**

```c
// åµŒå…¥å¼ç¯å¢ƒä¸‹çš„ä¼˜åŒ–ç‰ˆæœ¬
typedef struct {
    float* data;
    int size;
    int capacity;
} StaticBuffer;

// é™æ€ç¼“å†²åŒºåˆ›å»º
StaticBuffer* create_static_buffer(int capacity) {
    StaticBuffer* buffer = malloc(sizeof(StaticBuffer));
    buffer->data = malloc(capacity * sizeof(float));
    buffer->size = 0;
    buffer->capacity = capacity;
    return buffer;
}

// ä¼˜åŒ–çš„å‰å‘ä¼ æ’­ï¼ˆä½¿ç”¨é™æ€ç¼“å†²åŒºï¼‰
void nn_forward_static(NeuralNetwork* nn, float* input, float* output, 
                      StaticBuffer* buffer) {
    float* current_input = input;
    float* current_output = buffer->data;
    
    for (int i = 0; i < nn->num_layers; i++) {
        Layer* layer = &nn->layers[i];
        
        // è®¡ç®—å±‚è¾“å‡º
        for (int j = 0; j < layer->num_neurons; j++) {
            float sum = layer->neurons[j].bias;
            for (int k = 0; k < layer->input_size; k++) {
                sum += layer->neurons[j].weights[k] * current_input[k];
            }
            current_output[j] = layer->activation(sum);
        }
        
        // å‡†å¤‡ä¸‹ä¸€å±‚è¾“å…¥
        if (i == nn->num_layers - 1) {
            memcpy(output, current_output, layer->num_neurons * sizeof(float));
        } else {
            float* temp = current_input;
            current_input = current_output;
            current_output = temp;
        }
    }
}

// å†…å­˜ä½¿ç”¨ç›‘æ§
typedef struct {
    size_t peak_memory;
    size_t current_memory;
    int allocation_count;
    int deallocation_count;
} MemoryTracker;

MemoryTracker* create_memory_tracker() {
    MemoryTracker* tracker = malloc(sizeof(MemoryTracker));
    tracker->peak_memory = 0;
    tracker->current_memory = 0;
    tracker->allocation_count = 0;
    tracker->deallocation_count = 0;
    return tracker;
}

void* tracked_malloc(MemoryTracker* tracker, size_t size) {
    void* ptr = malloc(size);
    if (ptr) {
        tracker->current_memory += size;
        tracker->allocation_count++;
        if (tracker->current_memory > tracker->peak_memory) {
            tracker->peak_memory = tracker->current_memory;
        }
    }
    return ptr;
}

void tracked_free(MemoryTracker* tracker, void* ptr) {
    free(ptr);
    tracker->deallocation_count++;
    // æ³¨æ„ï¼šè¿™é‡Œç®€åŒ–äº†ï¼Œå®é™…åº”è¯¥è·Ÿè¸ªæ¯ä¸ªåˆ†é…çš„å¤§å°
}

void print_memory_tracker_stats(MemoryTracker* tracker) {
    printf("å†…å­˜è·Ÿè¸ªç»Ÿè®¡:\n");
    printf("å³°å€¼å†…å­˜ä½¿ç”¨: %.2f KB\n", tracker->peak_memory / 1024.0f);
    printf("åˆ†é…æ¬¡æ•°: %d\n", tracker->allocation_count);
    printf("é‡Šæ”¾æ¬¡æ•°: %d\n", tracker->deallocation_count);
    printf("å†…å­˜æ³„æ¼: %d\n", tracker->allocation_count - tracker->deallocation_count);
}
```

### ç¬¬9å‘¨ï¼šæ¨¡å‹ç®¡ç†å®ç°

#### å­¦ä¹ å†…å®¹
- **æ¨¡å‹åºåˆ—åŒ–**
- **æ¨¡å‹åŠ è½½**
- **æ¨¡å‹éªŒè¯**

#### ç†è®ºçŸ¥è¯†

**1. æ¨¡å‹åºåˆ—åŒ–æ ¼å¼**
- **äºŒè¿›åˆ¶æ ¼å¼**: ç´§å‡‘ã€å¿«é€Ÿ
- **æ–‡æœ¬æ ¼å¼**: å¯è¯»ã€å¯è°ƒè¯•
- **æ ‡å‡†æ ¼å¼**: ONNXã€TensorFlow Lite

**2. æ¨¡å‹ç‰ˆæœ¬ç®¡ç†**
- **ç‰ˆæœ¬å·**: æ ‡è¯†æ¨¡å‹ç‰ˆæœ¬
- **å…ƒæ•°æ®**: ç½‘ç»œç»“æ„ã€è®­ç»ƒå‚æ•°
- **æ ¡éªŒå’Œ**: éªŒè¯æ¨¡å‹å®Œæ•´æ€§

#### å®è·µç»ƒä¹ 
```c
// æ¨¡å‹åºåˆ—åŒ–
typedef struct {
    int version;
    int num_layers;
    int* layer_sizes;
    float learning_rate;
    char* optimizer_type;
} ModelMetadata;

bool save_model(NeuralNetwork* nn, const char* filename) {
    FILE* file = fopen(filename, "wb");
    if (!file) return false;
    
    // å†™å…¥å…ƒæ•°æ®
    ModelMetadata metadata;
    metadata.version = 1;
    metadata.num_layers = nn->num_layers;
    metadata.learning_rate = nn->learning_rate;
    
    fwrite(&metadata, sizeof(ModelMetadata), 1, file);
    
    // å†™å…¥ç½‘ç»œç»“æ„
    for (int i = 0; i < nn->num_layers; i++) {
        Layer* layer = &nn->layers[i];
        fwrite(&layer->input_size, sizeof(int), 1, file);
        fwrite(&layer->output_size, sizeof(int), 1, file);
    }
    
    // å†™å…¥æƒé‡å’Œåç½®
    for (int i = 0; i < nn->num_layers; i++) {
        Layer* layer = &nn->layers[i];
        int weight_size = layer->output_size * layer->input_size;
        int bias_size = layer->output_size;
        
        fwrite(layer->weights, sizeof(float), weight_size, file);
        fwrite(layer->biases, sizeof(float), bias_size, file);
    }
    
    fclose(file);
    return true;
}

NeuralNetwork* load_model(const char* filename) {
    FILE* file = fopen(filename, "rb");
    if (!file) return NULL;
    
    // è¯»å–å…ƒæ•°æ®
    ModelMetadata metadata;
    fread(&metadata, sizeof(ModelMetadata), 1, file);
    
    // åˆ›å»ºç½‘ç»œ
    NeuralNetwork* nn = malloc(sizeof(NeuralNetwork));
    nn->num_layers = metadata.num_layers;
    nn->learning_rate = metadata.learning_rate;
    nn->layers = malloc(nn->num_layers * sizeof(Layer));
    
    // è¯»å–ç½‘ç»œç»“æ„å¹¶åˆ†é…å†…å­˜
    for (int i = 0; i < nn->num_layers; i++) {
        Layer* layer = &nn->layers[i];
        fread(&layer->input_size, sizeof(int), 1, file);
        fread(&layer->output_size, sizeof(int), 1, file);
        
        int weight_size = layer->output_size * layer->input_size;
        int bias_size = layer->output_size;
        
        layer->weights = malloc(weight_size * sizeof(float));
        layer->biases = malloc(bias_size * sizeof(float));
        
        // è®¾ç½®æ¿€æ´»å‡½æ•°
        if (i == nn->num_layers - 1) {
            layer->activation = linear_activation;
            layer->activation_derivative = linear_derivative;
        } else {
            layer->activation = sigmoid;
            layer->activation_derivative = sigmoid_derivative;
        }
    }
    
    // è¯»å–æƒé‡å’Œåç½®
    for (int i = 0; i < nn->num_layers; i++) {
        Layer* layer = &nn->layers[i];
        int weight_size = layer->output_size * layer->input_size;
        int bias_size = layer->output_size;
        
        fread(layer->weights, sizeof(float), weight_size, file);
        fread(layer->biases, sizeof(float), bias_size, file);
    }
    
    fclose(file);
    return nn;
}
```

---

## ğŸ¯ é˜¶æ®µå››ï¼šæ¨¡å‹è®­ç»ƒä¸ä¼˜åŒ–ï¼ˆ2å‘¨ï¼‰

### ç¬¬10å‘¨ï¼šè®­ç»ƒç­–ç•¥ä¸æŠ€å·§

#### å­¦ä¹ å†…å®¹
- **æ•°æ®é¢„å¤„ç†**
- **è®­ç»ƒç­–ç•¥é€‰æ‹©**
- **è¶…å‚æ•°è°ƒä¼˜**

#### ç†è®ºçŸ¥è¯†

**1. æ•°æ®é¢„å¤„ç†**
- **å½’ä¸€åŒ–**: x = (x - Î¼) / Ïƒ
- **æ ‡å‡†åŒ–**: x = (x - min) / (max - min)
- **æ•°æ®å¢å¼º**: å¢åŠ è®­ç»ƒæ ·æœ¬å¤šæ ·æ€§

**2. è®­ç»ƒç­–ç•¥**
- **æ—©åœ**: é˜²æ­¢è¿‡æ‹Ÿåˆ
- **å­¦ä¹ ç‡è°ƒåº¦**: åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡
- **äº¤å‰éªŒè¯**: è¯„ä¼°æ¨¡å‹æ³›åŒ–èƒ½åŠ›

**3. è¶…å‚æ•°è°ƒä¼˜**
- **ç½‘æ ¼æœç´¢**: ç³»ç»Ÿæœç´¢æœ€ä¼˜å‚æ•°
- **éšæœºæœç´¢**: éšæœºé‡‡æ ·å‚æ•°ç»„åˆ
- **è´å¶æ–¯ä¼˜åŒ–**: æ™ºèƒ½å‚æ•°æœç´¢

#### å®è·µç»ƒä¹ 
```c
// æ•°æ®é¢„å¤„ç†
void normalize_data(float* data, int size, float* mean, float* std) {
    // è®¡ç®—å‡å€¼
    *mean = 0;
    for (int i = 0; i < size; i++) {
        *mean += data[i];
    }
    *mean /= size;
    
    // è®¡ç®—æ ‡å‡†å·®
    *std = 0;
    for (int i = 0; i < size; i++) {
        float diff = data[i] - *mean;
        *std += diff * diff;
    }
    *std = sqrtf(*std / size);
    
    // å½’ä¸€åŒ–
    for (int i = 0; i < size; i++) {
        data[i] = (data[i] - *mean) / *std;
    }
}

// æ—©åœæœºåˆ¶
typedef struct {
    float best_loss;
    int patience;
    int counter;
    bool should_stop;
} EarlyStopping;

bool check_early_stopping(EarlyStopping* es, float current_loss) {
    if (current_loss < es->best_loss) {
        es->best_loss = current_loss;
        es->counter = 0;
    } else {
        es->counter++;
        if (es->counter >= es->patience) {
            es->should_stop = true;
        }
    }
    return es->should_stop;
}
```

### ç¬¬11å‘¨ï¼šæ¨¡å‹è¯„ä¼°ä¸ä¼˜åŒ–

#### å­¦ä¹ å†…å®¹
- **æ¨¡å‹è¯„ä¼°æŒ‡æ ‡**
- **è¿‡æ‹Ÿåˆæ£€æµ‹**
- **æ¨¡å‹ä¼˜åŒ–æŠ€æœ¯**

#### ç†è®ºçŸ¥è¯†

**1. è¯„ä¼°æŒ‡æ ‡**
- **å‡†ç¡®ç‡**: æ­£ç¡®é¢„æµ‹çš„æ¯”ä¾‹
- **ç²¾ç¡®ç‡**: é¢„æµ‹ä¸ºæ­£ä¾‹ä¸­å®é™…ä¸ºæ­£ä¾‹çš„æ¯”ä¾‹
- **å¬å›ç‡**: å®é™…æ­£ä¾‹ä¸­è¢«æ­£ç¡®é¢„æµ‹çš„æ¯”ä¾‹
- **F1åˆ†æ•°**: ç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡

**2. è¿‡æ‹Ÿåˆæ£€æµ‹**
- **è®­ç»ƒæŸå¤± vs éªŒè¯æŸå¤±**: éªŒè¯æŸå¤±ä¸Šå‡è¡¨ç¤ºè¿‡æ‹Ÿåˆ
- **å­¦ä¹ æ›²çº¿**: è§‚å¯Ÿè®­ç»ƒå’ŒéªŒè¯æŸå¤±å˜åŒ–
- **æ­£åˆ™åŒ–**: L1ã€L2ã€Dropout

**3. æ¨¡å‹ä¼˜åŒ–**
- **æ¨¡å‹å‹ç¼©**: å‰ªæã€é‡åŒ–
- **é›†æˆå­¦ä¹ **: å¤šä¸ªæ¨¡å‹ç»„åˆ
- **çŸ¥è¯†è’¸é¦**: å¤§æ¨¡å‹æŒ‡å¯¼å°æ¨¡å‹

#### å®è·µç»ƒä¹ 
```c
// æ¨¡å‹è¯„ä¼°
typedef struct {
    float accuracy;
    float precision;
    float recall;
    float f1_score;
} ModelMetrics;

ModelMetrics evaluate_model(NeuralNetwork* nn, float* test_inputs, float* test_targets, int num_samples) {
    ModelMetrics metrics = {0};
    int true_positives = 0, false_positives = 0, false_negatives = 0, true_negatives = 0;
    
    for (int i = 0; i < num_samples; i++) {
        float* input = &test_inputs[i * nn->layers[0].input_size];
        float* target = &test_targets[i * nn->layers[nn->num_layers-1].output_size];
        
        float* prediction = malloc(nn->layers[nn->num_layers-1].output_size * sizeof(float));
        nn_forward(nn, input, prediction);
        
        // äºŒåˆ†ç±»è¯„ä¼°ï¼ˆå‡è®¾è¾“å‡ºå±‚åªæœ‰ä¸€ä¸ªç¥ç»å…ƒï¼‰
        int pred_class = prediction[0] > 0.5 ? 1 : 0;
        int true_class = target[0] > 0.5 ? 1 : 0;
        
        if (pred_class == 1 && true_class == 1) true_positives++;
        else if (pred_class == 1 && true_class == 0) false_positives++;
        else if (pred_class == 0 && true_class == 1) false_negatives++;
        else true_negatives++;
        
        free(prediction);
    }
    
    metrics.accuracy = (float)(true_positives + true_negatives) / num_samples;
    metrics.precision = (float)true_positives / (true_positives + false_positives);
    metrics.recall = (float)true_positives / (true_positives + false_negatives);
    metrics.f1_score = 2 * metrics.precision * metrics.recall / (metrics.precision + metrics.recall);
    
    return metrics;
}

// æ­£åˆ™åŒ–å®ç°
void add_l2_regularization(NeuralNetwork* nn, float lambda) {
    for (int l = 0; l < nn->num_layers; l++) {
        Layer* layer = &nn->layers[l];
        int weight_size = layer->output_size * layer->input_size;
        
        for (int i = 0; i < weight_size; i++) {
            layer->weights[i] -= lambda * layer->weights[i];
        }
    }
}
```

---

## ğŸš€ é˜¶æ®µäº”ï¼šæ¨¡å‹éƒ¨ç½²ä¸éªŒè¯ï¼ˆ1å‘¨ï¼‰

### ç¬¬12å‘¨ï¼šESP32éƒ¨ç½²éªŒè¯

#### å­¦ä¹ å†…å®¹
- **æ¨¡å‹ç§»æ¤**
- **æ€§èƒ½ä¼˜åŒ–**
- **å®é™…éªŒè¯**

#### ç†è®ºçŸ¥è¯†

**1. åµŒå…¥å¼éƒ¨ç½²è€ƒè™‘**
- **å†…å­˜é™åˆ¶**: æ¨¡å‹å¤§å°å’Œè¿è¡Œæ—¶å†…å­˜
- **è®¡ç®—èƒ½åŠ›**: æµ®ç‚¹è¿ç®—æ€§èƒ½
- **åŠŸè€—çº¦æŸ**: ç”µæ± ä¾›ç”µè®¾å¤‡çš„åŠŸè€—è¦æ±‚

**2. æ¨¡å‹ä¼˜åŒ–æŠ€æœ¯**
- **é‡åŒ–**: æµ®ç‚¹è½¬å®šç‚¹
- **å‰ªæ**: ç§»é™¤ä¸é‡è¦çš„æƒé‡
- **çŸ¥è¯†è’¸é¦**: å¤§æ¨¡å‹æŒ‡å¯¼å°æ¨¡å‹

#### å®è·µç»ƒä¹ 
```c
// ESP32ä¼˜åŒ–ç‰ˆæœ¬
#include "freertos/FreeRTOS.h"
#include "freertos/task.h"
#include "esp_log.h"

// å®šç‚¹æ•°å®ç°
typedef int16_t fixed_point_t;
#define FIXED_SCALE 1024
#define FLOAT_TO_FIXED(x) ((fixed_point_t)((x) * FIXED_SCALE))
#define FIXED_TO_FLOAT(x) ((float)(x) / FIXED_SCALE)

// é‡åŒ–ç¥ç»ç½‘ç»œ
typedef struct {
    int16_t* weights;
    int16_t* biases;
    int input_size;
    int output_size;
    float scale;
} QuantizedLayer;

typedef struct {
    QuantizedLayer* layers;
    int num_layers;
} QuantizedNeuralNetwork;

// é‡åŒ–å‰å‘ä¼ æ’­
void quantized_forward(QuantizedNeuralNetwork* qnn, int16_t* input, int16_t* output) {
    int16_t* current_input = input;
    
    for (int l = 0; l < qnn->num_layers; l++) {
        QuantizedLayer* layer = &qnn->layers[l];
        
        for (int i = 0; i < layer->output_size; i++) {
            int32_t sum = layer->biases[i];
            for (int j = 0; j < layer->input_size; j++) {
                sum += layer->weights[i * layer->input_size + j] * current_input[j];
            }
            output[i] = sum >> 10; // å³ç§»10ä½ç›¸å½“äºé™¤ä»¥1024
        }
        
        current_input = output;
    }
}

// ESP32ä»»åŠ¡
void nn_inference_task(void* pvParameters) {
    QuantizedNeuralNetwork* qnn = (QuantizedNeuralNetwork*)pvParameters;
    
    while (1) {
        // æ¨¡æ‹Ÿè¾“å…¥æ•°æ®
        int16_t input[2] = {FLOAT_TO_FIXED(0.5), FLOAT_TO_FIXED(0.3)};
        int16_t output[1];
        
        // æ‰§è¡Œæ¨ç†
        quantized_forward(qnn, input, output);
        
        // è½¬æ¢å›æµ®ç‚¹æ•°
        float result = FIXED_TO_FLOAT(output[0]);
        
        ESP_LOGI("NN", "Input: [%.2f, %.2f], Output: %.2f", 
                 FIXED_TO_FLOAT(input[0]), FIXED_TO_FLOAT(input[1]), result);
        
        vTaskDelay(pdMS_TO_TICKS(1000));
    }
}

void app_main(void) {
    ESP_LOGI("NN", "Starting Neural Network on ESP32");
    
    // åŠ è½½é‡åŒ–æ¨¡å‹
    QuantizedNeuralNetwork* qnn = load_quantized_model();
    
    // åˆ›å»ºæ¨ç†ä»»åŠ¡
    xTaskCreate(nn_inference_task, "nn_task", 4096, qnn, 5, NULL);
}
```

---

## ğŸ“š ç†è®ºçŸ¥è¯†ä½“ç³»

### 1. æ•°å­¦åŸºç¡€

#### çº¿æ€§ä»£æ•°
**å­¦ç§‘å½’å±**: æ•°å­¦ - çº¿æ€§ä»£æ•°
**æ ¸å¿ƒæ¦‚å¿µ**:
- å‘é‡è¿ç®—ï¼šåŠ æ³•ã€å‡æ³•ã€ç‚¹ç§¯ã€å‰ç§¯
- çŸ©é˜µè¿ç®—ï¼šä¹˜æ³•ã€è½¬ç½®ã€é€†çŸ©é˜µã€ç‰¹å¾å€¼
- çº¿æ€§å˜æ¢ï¼šç†è§£çŸ©é˜µä½œä¸ºå˜æ¢çš„ä½œç”¨
- å‘é‡ç©ºé—´ï¼šåŸºã€ç»´åº¦ã€çº¿æ€§æ— å…³

**æ¨èä¹¦ç±**:
1. **ã€Šçº¿æ€§ä»£æ•°åŠå…¶åº”ç”¨ã€‹** - Gilbert Strang
   - çº¿æ€§ä»£æ•°ç»å…¸æ•™æ
   - é€‚åˆç†è§£çŸ©é˜µè¿ç®—å’Œçº¿æ€§å˜æ¢
   - é…å¥—MITåœ¨çº¿è¯¾ç¨‹

2. **ã€Šçº¿æ€§ä»£æ•°ã€‹** - æå°šå¿—
   - ä¸­æ–‡çº¿æ€§ä»£æ•°æ•™æ
   - é€‚åˆä¸­æ–‡è¯»è€…ç†è§£

#### å¾®ç§¯åˆ†
**å­¦ç§‘å½’å±**: æ•°å­¦ - å¾®ç§¯åˆ†
**æ ¸å¿ƒæ¦‚å¿µ**:
- å¯¼æ•°æ¦‚å¿µï¼šç†è§£å˜åŒ–ç‡å’Œæ¢¯åº¦
- åå¯¼æ•°ï¼šå¤šå˜é‡å‡½æ•°çš„å¯¼æ•°
- é“¾å¼æ³•åˆ™ï¼šå¤åˆå‡½æ•°æ±‚å¯¼
- æ¢¯åº¦ä¸‹é™ï¼šä¼˜åŒ–ç®—æ³•åŸºç¡€
- æ³°å‹’å±•å¼€ï¼šå‡½æ•°è¿‘ä¼¼

**æ¨èä¹¦ç±**:
1. **ã€Šå¾®ç§¯åˆ†å­¦æ•™ç¨‹ã€‹** - è²èµ«é‡‘å“¥å°”èŒ¨
   - å¾®ç§¯åˆ†ç»å…¸æ•™æ
   - æ·±å…¥ç†è§£å¯¼æ•°å’Œç§¯åˆ†

2. **ã€Šé«˜ç­‰æ•°å­¦ã€‹** - åŒæµå¤§å­¦æ•°å­¦ç³»
   - ä¸­æ–‡å¾®ç§¯åˆ†æ•™æ
   - é€‚åˆå·¥ç§‘èƒŒæ™¯

#### æ¦‚ç‡ç»Ÿè®¡
**å­¦ç§‘å½’å±**: æ•°å­¦ - æ¦‚ç‡è®ºä¸æ•°ç†ç»Ÿè®¡
**æ ¸å¿ƒæ¦‚å¿µ**:
- æ¦‚ç‡åˆ†å¸ƒï¼šæ­£æ€åˆ†å¸ƒã€å‡åŒ€åˆ†å¸ƒ
- æœŸæœ›å’Œæ–¹å·®ï¼šéšæœºå˜é‡çš„ç»Ÿè®¡ç‰¹æ€§
- æœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼šå‚æ•°ä¼°è®¡æ–¹æ³•
- è´å¶æ–¯å®šç†ï¼šæ¡ä»¶æ¦‚ç‡

**æ¨èä¹¦ç±**:
1. **ã€Šæ¦‚ç‡è®ºä¸æ•°ç†ç»Ÿè®¡ã€‹** - é™ˆå¸Œå­º
   - æ¦‚ç‡ç»Ÿè®¡åŸºç¡€
   - ç†è§£éšæœºæ€§å’Œç»Ÿè®¡æ¨æ–­

2. **ã€Šæ¦‚ç‡è®ºåŸºç¡€æ•™ç¨‹ã€‹** - èŒ†è¯—æ¾
   - ä¸­æ–‡æ¦‚ç‡è®ºæ•™æ
   - é€‚åˆåˆå­¦è€…

### 2. æœºå™¨å­¦ä¹ ç†è®º

#### ç¥ç»ç½‘ç»œåŸºç¡€
**å­¦ç§‘å½’å±**: è®¡ç®—æœºç§‘å­¦ - äººå·¥æ™ºèƒ½ - æœºå™¨å­¦ä¹ 
**æ ¸å¿ƒæ¦‚å¿µ**:
- æ„ŸçŸ¥æœºæ¨¡å‹ï¼šMcCulloch-Pittsç¥ç»å…ƒ
- å¤šå±‚æ„ŸçŸ¥æœºï¼šå‰é¦ˆç¥ç»ç½‘ç»œ
- æ¿€æ´»å‡½æ•°ï¼šSigmoidã€ReLUã€Tanh
- ä¸‡èƒ½è¿‘ä¼¼å®šç†ï¼šç¥ç»ç½‘ç»œçš„è¡¨è¾¾èƒ½åŠ›

**æ¨èä¹¦ç±**:
1. **ã€Šç¥ç»ç½‘ç»œä¸æ·±åº¦å­¦ä¹ ã€‹** - é‚±é”¡é¹
   - ä¸­æ–‡æ·±åº¦å­¦ä¹ æ•™æ
   - é€‚åˆä¸­æ–‡è¯»è€…ç†è§£

2. **ã€Šæ·±åº¦å­¦ä¹ ã€‹** - Ian Goodfellow, Yoshua Bengio, Aaron Courville
   - æ·±åº¦å­¦ä¹ æƒå¨æ•™æ
   - å…¨é¢ä»‹ç»ç¥ç»ç½‘ç»œç†è®º

#### åå‘ä¼ æ’­ç®—æ³•
**å­¦ç§‘å½’å±**: è®¡ç®—æœºç§‘å­¦ - äººå·¥æ™ºèƒ½ - æœºå™¨å­¦ä¹ 
**æ ¸å¿ƒæ¦‚å¿µ**:
- é“¾å¼æ³•åˆ™ï¼šæ¢¯åº¦è®¡ç®—çš„æ ¸å¿ƒ
- è¯¯å·®åå‘ä¼ æ’­ï¼šä»è¾“å‡ºå±‚åˆ°è¾“å…¥å±‚
- æƒé‡æ›´æ–°ï¼šæ¢¯åº¦ä¸‹é™ä¼˜åŒ–
- æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸ï¼šè®­ç»ƒä¸­çš„å¸¸è§é—®é¢˜

**æ¨èä¹¦ç±**:
1. **ã€Šæ·±åº¦å­¦ä¹ ã€‹** - Ian Goodfellow
   - è¯¦ç»†è®²è§£åå‘ä¼ æ’­ç®—æ³•
   - åŒ…å«æ•°å­¦æ¨å¯¼å’Œå®ç°

2. **ã€Šæœºå™¨å­¦ä¹ ã€‹** - å‘¨å¿—å
   - æœºå™¨å­¦ä¹ åŸºç¡€ç†è®º
   - ç®—æ³•åŸç†å’Œå®ç°

#### ä¼˜åŒ–ç†è®º
**å­¦ç§‘å½’å±**: æ•°å­¦ - ä¼˜åŒ–ç†è®º
**æ ¸å¿ƒæ¦‚å¿µ**:
- æ¢¯åº¦ä¸‹é™ï¼šæ‰¹é‡ã€éšæœºã€å°æ‰¹é‡
- åŠ¨é‡ä¼˜åŒ–ï¼šåŠ é€Ÿæ”¶æ•›
- Adamä¼˜åŒ–å™¨ï¼šè‡ªé€‚åº”å­¦ä¹ ç‡
- å‡¸ä¼˜åŒ–ï¼šç†è®ºåŸºç¡€

**æ¨èä¹¦ç±**:
1. **ã€Šå‡¸ä¼˜åŒ–ã€‹** - Stephen Boyd
   - ä¼˜åŒ–ç†è®ºç»å…¸æ•™æ
   - æ·±å…¥ç†è§£ä¼˜åŒ–ç®—æ³•

2. **ã€Šæ•°å€¼ä¼˜åŒ–ã€‹** - Jorge Nocedal
   - æ•°å€¼ä¼˜åŒ–æ–¹æ³•
   - å®é™…ç®—æ³•å®ç°

### 3. åµŒå…¥å¼ç³»ç»Ÿç†è®º

#### å®æ—¶ç³»ç»Ÿ
**å­¦ç§‘å½’å±**: è®¡ç®—æœºç§‘å­¦ - å®æ—¶ç³»ç»Ÿ
**æ ¸å¿ƒæ¦‚å¿µ**:
- å®æ—¶æ€§æ¦‚å¿µï¼šç¡¬å®æ—¶ã€è½¯å®æ—¶
- ä»»åŠ¡è°ƒåº¦ï¼šä¼˜å…ˆçº§è°ƒåº¦ã€æ—¶é—´ç‰‡è½®è½¬
- ä¸­æ–­å¤„ç†ï¼šä¸­æ–­å‘é‡ã€ä¸­æ–­æœåŠ¡ç¨‹åº
- æœ€åæƒ…å†µæ‰§è¡Œæ—¶é—´ï¼ˆWCETï¼‰

**æ¨èä¹¦ç±**:
1. **ã€Šå®æ—¶ç³»ç»Ÿã€‹** - Jane W.S. Liu
   - å®æ—¶ç³»ç»Ÿç†è®º
   - ä»»åŠ¡è°ƒåº¦å’Œå®æ—¶æ€§ä¿è¯

2. **ã€ŠåµŒå…¥å¼å®æ—¶æ“ä½œç³»ç»Ÿã€‹** - ä½•å°åº†
   - ä¸­æ–‡å®æ—¶ç³»ç»Ÿæ•™æ
   - é€‚åˆåµŒå…¥å¼å·¥ç¨‹å¸ˆ

#### å†…å­˜ç®¡ç†
**å­¦ç§‘å½’å±**: è®¡ç®—æœºç§‘å­¦ - æ“ä½œç³»ç»Ÿ
**æ ¸å¿ƒæ¦‚å¿µ**:
- å†…å­˜å±‚æ¬¡ï¼šå¯„å­˜å™¨ã€ç¼“å­˜ã€ä¸»å­˜ã€å¤–å­˜
- å†…å­˜åˆ†é…ï¼šé™æ€åˆ†é…ã€åŠ¨æ€åˆ†é…
- å†…å­˜ä¼˜åŒ–ï¼šç¼“å­˜å‹å¥½ã€å†…å­˜å¯¹é½
- å†…å­˜ç¢ç‰‡ï¼šå†…éƒ¨ç¢ç‰‡ã€å¤–éƒ¨ç¢ç‰‡

**æ¨èä¹¦ç±**:
1. **ã€Šè®¡ç®—æœºç³»ç»Ÿï¼šç¨‹åºå‘˜çš„è§†è§’ã€‹** - Randal E. Bryant
   - è®¡ç®—æœºç³»ç»ŸåŸºç¡€
   - å†…å­˜ç®¡ç†å’Œç³»ç»Ÿä¼˜åŒ–

2. **ã€Šæ“ä½œç³»ç»Ÿæ¦‚å¿µã€‹** - Abraham Silberschatz
   - æ“ä½œç³»ç»ŸåŸºç¡€ç†è®º
   - å†…å­˜ç®¡ç†æœºåˆ¶

#### åŠŸè€—ä¼˜åŒ–
**å­¦ç§‘å½’å±**: ç”µå­å·¥ç¨‹ - ä½åŠŸè€—è®¾è®¡
**æ ¸å¿ƒæ¦‚å¿µ**:
- åŠ¨æ€åŠŸè€—ï¼šå¼€å…³åŠŸè€—ã€çŸ­è·¯åŠŸè€—
- é™æ€åŠŸè€—ï¼šæ¼ç”µæµåŠŸè€—
- åŠŸè€—ç®¡ç†ï¼šDVFSã€ç¡çœ æ¨¡å¼
- èƒ½é‡æ•ˆç‡ï¼šæ€§èƒ½ä¸åŠŸè€—çš„å¹³è¡¡

**æ¨èä¹¦ç±**:
1. **ã€Šä½åŠŸè€—è®¾è®¡ã€‹** - Jan M. Rabaey
   - ä½åŠŸè€—è®¾è®¡æŠ€æœ¯
   - é€‚åˆåµŒå…¥å¼ç³»ç»Ÿä¼˜åŒ–

2. **ã€Šæ•°å­—é›†æˆç”µè·¯è®¾è®¡ã€‹** - Jan M. Rabaey
   - æ•°å­—ç”µè·¯è®¾è®¡
   - åŠŸè€—åˆ†æåŸºç¡€

---

## ğŸ“– å‚è€ƒèµ„æº

### ä¹¦ç±æ¨è

#### æ•°å­¦åŸºç¡€
1. **ã€Šçº¿æ€§ä»£æ•°åŠå…¶åº”ç”¨ã€‹** - Gilbert Strang
   - çº¿æ€§ä»£æ•°ç»å…¸æ•™æ
   - é€‚åˆç†è§£çŸ©é˜µè¿ç®—å’Œçº¿æ€§å˜æ¢

2. **ã€Šå¾®ç§¯åˆ†å­¦æ•™ç¨‹ã€‹** - è²èµ«é‡‘å“¥å°”èŒ¨
   - å¾®ç§¯åˆ†ç»å…¸æ•™æ
   - æ·±å…¥ç†è§£å¯¼æ•°å’Œç§¯åˆ†

3. **ã€Šæ¦‚ç‡è®ºä¸æ•°ç†ç»Ÿè®¡ã€‹** - é™ˆå¸Œå­º
   - æ¦‚ç‡ç»Ÿè®¡åŸºç¡€
   - ç†è§£éšæœºæ€§å’Œç»Ÿè®¡æ¨æ–­

#### æœºå™¨å­¦ä¹ 
4. **ã€Šæ·±åº¦å­¦ä¹ ã€‹** - Ian Goodfellow, Yoshua Bengio, Aaron Courville
   - æ·±åº¦å­¦ä¹ æƒå¨æ•™æ
   - å…¨é¢ä»‹ç»ç¥ç»ç½‘ç»œç†è®º

5. **ã€Šç¥ç»ç½‘ç»œä¸æ·±åº¦å­¦ä¹ ã€‹** - é‚±é”¡é¹
   - ä¸­æ–‡æ·±åº¦å­¦ä¹ æ•™æ
   - é€‚åˆä¸­æ–‡è¯»è€…ç†è§£

6. **ã€Šæœºå™¨å­¦ä¹ ã€‹** - å‘¨å¿—å
   - æœºå™¨å­¦ä¹ åŸºç¡€ç†è®º
   - ç®—æ³•åŸç†å’Œå®ç°

#### åµŒå…¥å¼ç³»ç»Ÿ
7. **ã€Šå®æ—¶ç³»ç»Ÿã€‹** - Jane W.S. Liu
   - å®æ—¶ç³»ç»Ÿç†è®º
   - ä»»åŠ¡è°ƒåº¦å’Œå®æ—¶æ€§ä¿è¯

8. **ã€Šè®¡ç®—æœºç³»ç»Ÿï¼šç¨‹åºå‘˜çš„è§†è§’ã€‹** - Randal E. Bryant
   - è®¡ç®—æœºç³»ç»ŸåŸºç¡€
   - å†…å­˜ç®¡ç†å’Œç³»ç»Ÿä¼˜åŒ–

9. **ã€Šä½åŠŸè€—è®¾è®¡ã€‹** - Jan M. Rabaey
   - ä½åŠŸè€—è®¾è®¡æŠ€æœ¯
   - é€‚åˆåµŒå…¥å¼ç³»ç»Ÿä¼˜åŒ–

#### ESP32å’ŒFreeRTOS
10. **ã€ŠESP32æŠ€æœ¯å‚è€ƒæ‰‹å†Œã€‹** - Espressif Systems
    - ESP32ç¡¬ä»¶è¯¦ç»†è¯´æ˜
    - å¯„å­˜å™¨é…ç½®å’Œå¤–è®¾ä½¿ç”¨

11. **ã€Šä½¿ç”¨FreeRTOSè¿›è¡Œå®æ—¶ç¼–ç¨‹ã€‹** - Richard Barry
    - FreeRTOSä½¿ç”¨æŒ‡å—
    - å®æ—¶ç³»ç»Ÿç¼–ç¨‹å®è·µ

### åœ¨çº¿è¯¾ç¨‹

#### æ•°å­¦åŸºç¡€
1. **MITçº¿æ€§ä»£æ•°è¯¾ç¨‹** - Gilbert Strang
   - åœ¨çº¿è§†é¢‘è¯¾ç¨‹
   - é…å¥—ç»ƒä¹ å’Œä½œä¸š

2. **æ–¯å¦ç¦CS229æœºå™¨å­¦ä¹ ** - Andrew Ng
   - æœºå™¨å­¦ä¹ åŸºç¡€è¯¾ç¨‹
   - ç†è®ºä¸å®è·µç»“åˆ

#### æ·±åº¦å­¦ä¹ 
3. **CS231nè®¡ç®—æœºè§†è§‰** - Stanford
   - å·ç§¯ç¥ç»ç½‘ç»œ
   - è®¡ç®—æœºè§†è§‰åº”ç”¨

4. **CS224nè‡ªç„¶è¯­è¨€å¤„ç†** - Stanford
   - å¾ªç¯ç¥ç»ç½‘ç»œ
   - è‡ªç„¶è¯­è¨€å¤„ç†

#### åµŒå…¥å¼ç³»ç»Ÿ
5. **ESP32å®˜æ–¹æ•™ç¨‹** - Espressif
   - ESP32å¼€å‘å…¥é—¨
   - é¡¹ç›®å®è·µæŒ‡å¯¼

6. **FreeRTOSå®˜æ–¹æ•™ç¨‹** - Real Time Engineers Ltd
   - FreeRTOSä½¿ç”¨æŒ‡å—
   - å®æ—¶ç³»ç»Ÿç¼–ç¨‹

### åœ¨çº¿èµ„æº

#### ç†è®ºå­¦ä¹ 
1. **3Blue1Brownç¥ç»ç½‘ç»œç³»åˆ—**
   - ç›´è§‚çš„ç¥ç»ç½‘ç»œå¯è§†åŒ–
   - é€‚åˆç†è§£åŸºç¡€æ¦‚å¿µ

2. **Andrew Ngæ·±åº¦å­¦ä¹ è¯¾ç¨‹**
   - Courseraä¸Šçš„æ·±åº¦å­¦ä¹ è¯¾ç¨‹
   - ç†è®ºä¸å®è·µç»“åˆ

#### å®è·µé¡¹ç›®
3. **GitHubç¥ç»ç½‘ç»œå®ç°**
   - å¼€æºç¥ç»ç½‘ç»œå®ç°
   - å­¦ä¹ ä¼˜ç§€ä»£ç 

4. **Kaggleç«èµ›å¹³å°**
   - å®é™…æœºå™¨å­¦ä¹ é¡¹ç›®
   - æå‡å®æˆ˜èƒ½åŠ›

---

## ğŸ’¡ å­¦ä¹ å»ºè®®

### 1. **ç†è®ºå…ˆè¡Œ**
- å…ˆç†è§£æ•°å­¦åŸç†ï¼Œå†å®ç°ä»£ç 
- æ¯ä¸ªç®—æ³•éƒ½è¦èƒ½æ‰‹åŠ¨æ¨å¯¼
- ç”»å›¾ç†è§£ç®—æ³•æµç¨‹

### 2. **å¾ªåºæ¸è¿›**
- ä»ç®€å•ä¾‹å­å¼€å§‹
- é€æ­¥å¢åŠ å¤æ‚åº¦
- æ¯ä¸ªé˜¶æ®µéƒ½è¦æœ‰å®Œæ•´ç†è§£

### 3. **å®è·µéªŒè¯**
- æ¯ä¸ªç†è®ºéƒ½è¦ç”¨ä»£ç éªŒè¯
- å¯¹æ¯”ä¸åŒå®ç°æ–¹æ³•
- åˆ†ææ€§èƒ½å·®å¼‚

### 4. **è®°å½•å­¦ä¹ **
- å»ºç«‹å­¦ä¹ ç¬”è®°
- è®°å½•é‡åˆ°çš„é—®é¢˜
- æ€»ç»“å­¦ä¹ å¿ƒå¾—

### 5. **é¡¹ç›®é©±åŠ¨**
- æ¯ä¸ªé˜¶æ®µéƒ½è¦æœ‰å®Œæ•´é¡¹ç›®
- å°†ç†è®ºåº”ç”¨åˆ°å®é™…é—®é¢˜
- å»ºç«‹ä¸ªäººé¡¹ç›®ä½œå“é›†

---

**è¿™ä¸ªå­¦ä¹ è®¡åˆ’ä¸“é—¨é’ˆå¯¹åµŒå…¥å¼å·¥ç¨‹å¸ˆï¼Œé‡ç‚¹åœ¨äºç†è§£ç¥ç»ç½‘ç»œç®—æ³•åŸç†ï¼ŒæŒæ¡è®­ç»ƒå’Œéƒ¨ç½²æµç¨‹ï¼Œæœ€ç»ˆé€šè¿‡ESP32éªŒè¯å­¦ä¹ æˆæœã€‚ç¥æ‚¨å­¦ä¹ é¡ºåˆ©ï¼** ğŸš€ 
